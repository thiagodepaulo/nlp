{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxe2Z2ow5V7XBDKIZNLDCP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiagodepaulo/nlp/blob/enap/topic_models_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUFn11_bfS0n",
        "outputId": "e895fe1b-fbc4-4a3c-e107-2a619eb3db9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.8.0\n",
            "    Uninstalling gensim-3.8.0:\n",
            "      Successfully uninstalled gensim-3.8.0\n",
            "Successfully installed gensim-4.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim.corpora import Dictionary\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEgpIP4rvK37",
        "outputId": "a5ba6081-2757-4929-9e47-13bc8142d9b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups = fetch_20newsgroups(subset='train')"
      ],
      "metadata": {
        "id": "NfMelICkvXXT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [word_tokenize(doc.lower()) for doc in newsgroups.data]\n",
        "\n",
        "corpus = [[token for token in doc if not token.isnumeric()] for doc in corpus]\n",
        "\n",
        "# Remove words that are only one character.\n",
        "docs = [[token for token in doc if len(token) > 1] for doc in corpus]"
      ],
      "metadata": {
        "id": "wzXpE9lvvb-K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
      ],
      "metadata": {
        "id": "m2qLHLVhwAyA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary representation of the documents.\n",
        "dictionary = Dictionary(docs)"
      ],
      "metadata": {
        "id": "QfNId3aNx1qR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of documents: %d' % len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l586HaS4yRJg",
        "outputId": "6a8eaa6e-7ba9-461a-d973-d98acb1b30f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens: 171377\n",
            "Number of documents: 11314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LDA model.\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Set training parameters.\n",
        "num_topics = 10\n",
        "chunksize = 1000\n",
        "passes = 20\n",
        "iterations = 100\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make an index to word dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        "    eval_every=eval_every\n",
        ")\n"
      ],
      "metadata": {
        "id": "CtXLHY2ryYdS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_topics = model.top_topics(corpus)\n",
        "\n",
        "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
        "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
        "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(top_topics)"
      ],
      "metadata": {
        "id": "M3Ps-k17yiKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swo_IRSA06Sr",
        "outputId": "893166a8-ef81-4eea-a7da-68c569e24be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==3.8 in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8) (1.11.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8) (1.16.0)\n",
            "Requirement already satisfied: smart-open>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os       #importing os to set environment variable\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxsedwuv07aG",
        "outputId": "810f05f2-b85d-4601-9d8a-ea582ed6680c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.22\" 2024-01-16\n",
            "OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip\n",
        "!pip install little_mallet_wrapper"
      ],
      "metadata": {
        "id": "GCNyMgqx116u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_mallet = '/content/mallet-2.0.8/bin/mallet'  # CHANGE THIS TO YOUR MALLET PATH\n"
      ],
      "metadata": {
        "id": "aafacVxy14z2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import little_mallet_wrapper as lmw\n",
        "\n",
        "output_directory_path = '.'\n",
        "\n",
        "num_topics = 20\n",
        "\n",
        "topic_keys, topic_distributions = lmw.quick_train_topic_model(path_to_mallet,\n",
        "                                                              output_directory_path,\n",
        "                                                              num_topics,\n",
        "                                                              newsgroups.data)"
      ],
      "metadata": {
        "id": "w5Qc5Oe92Apl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87760f15-9773-4f67-abaa-53302faaf759"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing data...\n",
            "Complete\n",
            "Training topic model...\n",
            "Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [word_tokenize(doc) for doc in newsgroups.data]"
      ],
      "metadata": {
        "id": "z7m10-dB7QWy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, t in enumerate(topic_keys):\n",
        "    print(i, '\\t', ' '.join(t[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyRacDpR8AOF",
        "outputId": "890c8675-78f1-46be-e0d7-4b9d3742a7d4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 \t the and for this from are list please your send\n",
            "1 \t the and that for key are with can The will\n",
            "2 \t the for and The sale will new year Public price\n",
            "3 \t the and with are that for has The other not\n",
            "4 \t more than the and any would for are much that\n",
            "5 \t the that and not are you for this have with\n",
            "6 \t the and with that for are The from you was\n",
            "7 \t the and for was game with team will that year\n",
            "8 \t you the that and this don't not what are have\n",
            "9 \t the and was The that from for with Armenian Turkish\n",
            "10 \t the and was that they were his had out for\n",
            "11 \t the and are that were people they who their not\n",
            "12 \t University Computer Inc Science USA Research Center and State Technology\n",
            "13 \t the and for with drive have that card SCSI use\n",
            "14 \t the and for with file that version can from files\n",
            "15 \t From writes Apr article David THE wrote John Michael Mark\n",
            "16 \t the and that for not will have this would their\n",
            "17 \t the and The From New from April American for was\n",
            "18 \t the that have this was been not but and has\n",
            "19 \t From the and writes Angeles Los Clipper Chip was Big\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kgrFJROF_awL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}