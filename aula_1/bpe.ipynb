{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thyarles/unb-fmc-nlp/blob/main/aula_1/bpe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV1x2AARpgTB"
      },
      "source": [
        "# Atividade: Aplicação do Algoritmo BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iWHmcK2UX6K",
        "outputId": "0f9c9cf8-a3b1-4984-bca7-eea121b71195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo corpus lido com 20000 linhas!\n"
          ]
        }
      ],
      "source": [
        "# Vamos baixar o corpus previamente preparado e descompactar\n",
        "import os\n",
        "\n",
        "file_path = './corpus.txt'\n",
        "corpus = ''\n",
        "lines = 0\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      corpus = file.read()\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      lines = sum(1 for line in file)\n",
        "    print(f'Arquivo corpus lido com {lines} linhas!')\n",
        "else:\n",
        "    url = 'https://raw.githubusercontent.com/thyarles/unb-fmc-nlp/refs/heads/main/aula_1/corpus.tar.xz'\n",
        "    cmd = ! wget {url} && tar -xvJf corpus.tar.xz\n",
        "    print('Ok, arquivo corpus recuperado! Rode a célula novamente.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ty1YCjtzpnE9"
      },
      "outputs": [],
      "source": [
        "# Vamos baixar os arquivos necessários com a biblioteca request\n",
        "\n",
        "module = \"bpe.py\"\n",
        "\n",
        "if not os.path.exists(module):\n",
        "  # Está rodando no Google Colab, vamos baixar o módulo bpe.py...\n",
        "  url = \"https://raw.githubusercontent.com/thyarles/unb-fmc-nlp/refs/heads/main/aula_1/bpe.py\"\n",
        "  cmd = ! wget {url}\n",
        "\n",
        "  # ... e o pickles, se existir\n",
        "  url = 'https://raw.githubusercontent.com/thyarles/unb-fmc-nlp/refs/heads/main/aula_1/my_bpe.pkl'\n",
        "  cmd = ! wget {url}\n",
        "\n",
        "# Vamos instanciar o modelo\n",
        "from bpe import MyBPE\n",
        "my_bpe = MyBPE()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiVnteCuqNPw",
        "outputId": "694a48d9-06f0-4f89-af11-9c157813caac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo será lido de 'my_bpe.pkl'.\n"
          ]
        }
      ],
      "source": [
        "# Vamos treinar (ou carregar, se o pickle existir)\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "# Modelo\n",
        "model_file = 'my_bpe.pkl'\n",
        "\n",
        "if os.path.exists(model_file):\n",
        "    # Se tiver modelo treinado, leia\n",
        "    print(\"Modelo será lido de 'my_bpe.pkl'.\")\n",
        "    with open(model_file, 'rb') as file:\n",
        "        my_bpe = pickle.load(file)\n",
        "else:\n",
        "    # Se não, treine\n",
        "    print(\"Modelo será treinado.\")\n",
        "\n",
        "    # Inicio\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Treina\n",
        "    my_bpe.train(corpus, 500)\n",
        "\n",
        "    # Tempo de execução\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Treinamento em {elapsed_time:.2f} segundos.\")\n",
        "\n",
        "    # Salva para evitar novo treinamento\n",
        "    with open(model_file, 'wb') as file:\n",
        "        pickle.dump(my_bpe, file)\n",
        "    print(\"Modelo salvo em 'my_bpe.pkl'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul01g_x7s5Pl",
        "outputId": "3c5f839a-865d-4172-993b-bca43af91ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[65, 291, 76, 387, 289, 375, 258, 110, 294, 99, 101, 300, 293, 49, 48, 391, 68, 101, 122, 310, 382, 323, 465, 49, 53, 46]\n"
          ]
        }
      ],
      "source": [
        "# Vamos codificar algo\n",
        "test = my_bpe.encode('Ada Lovelace nasceu em 10 de Dezembro de 1815.')\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qryc1gqItr_X",
        "outputId": "50b49320-d49b-444a-a3fb-f1a203b9342c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ada Lovelace nasceu em 10 de Dezembro de 1815.\n"
          ]
        }
      ],
      "source": [
        "# Vamos descodificar algo\n",
        "print(my_bpe.decode(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIKdJefbt0tF",
        "outputId": "cebf348d-b488-44d3-fedc-f6593bf63d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary 257 with tokens (32, 261) decodes to \" de \"\n",
            "Vocabulary 258 with tokens (32, 328) decodes to \" || \"\n",
            "Vocabulary 259 with tokens (41, 32) decodes to \") \"\n",
            "Vocabulary 260 with tokens (41, 260) decodes to \"), \"\n",
            "Vocabulary 261 with tokens (42, 32) decodes to \"* \"\n",
            "Vocabulary 262 with tokens (44, 32) decodes to \", \"\n",
            "Vocabulary 263 with tokens (45, 32) decodes to \"- \"\n",
            "Vocabulary 264 with tokens (45, 341) decodes to \"-se \"\n",
            "Vocabulary 265 with tokens (46, 32) decodes to \". \"\n",
            "Vocabulary 266 with tokens (48, 32) decodes to \"0 \"\n",
            "Vocabulary 267 with tokens (49, 32) decodes to \"1 \"\n",
            "Vocabulary 268 with tokens (49, 56) decodes to \"18\"\n",
            "Vocabulary 269 with tokens (49, 57) decodes to \"19\"\n",
            "Vocabulary 270 with tokens (50, 32) decodes to \"2 \"\n",
            "Vocabulary 271 with tokens (50, 48) decodes to \"20\"\n",
            "Vocabulary 272 with tokens (51, 32) decodes to \"3 \"\n",
            "Vocabulary 273 with tokens (52, 32) decodes to \"4 \"\n",
            "Vocabulary 274 with tokens (53, 32) decodes to \"5 \"\n",
            "Vocabulary 275 with tokens (58, 32) decodes to \": \"\n",
            "Vocabulary 276 with tokens (59, 32) decodes to \"; \"\n",
            "Vocabulary 277 with tokens (61, 32) decodes to \"= \"\n",
            "Vocabulary 278 with tokens (61, 61) decodes to \"==\"\n",
            "Vocabulary 279 with tokens (65, 32) decodes to \"A \"\n",
            "Vocabulary 280 with tokens (67, 286) decodes to \"Cat\"\n",
            "Vocabulary 281 with tokens (79, 32) decodes to \"O \"\n",
            "Vocabulary 282 with tokens (97, 32) decodes to \"a \"\n",
            "Vocabulary 283 with tokens (97, 98) decodes to \"ab\"\n",
            "Vocabulary 284 with tokens (97, 99) decodes to \"ac\"\n",
            "Vocabulary 285 with tokens (97, 100) decodes to \"ad\"\n",
            "Vocabulary 286 with tokens (97, 103) decodes to \"ag\"\n",
            "Vocabulary 287 with tokens (97, 105) decodes to \"ai\"\n",
            "Vocabulary 288 with tokens (97, 108) decodes to \"al\"\n",
            "Vocabulary 289 with tokens (97, 109) decodes to \"am\"\n",
            "Vocabulary 290 with tokens (97, 110) decodes to \"an\"\n",
            "Vocabulary 291 with tokens (97, 112) decodes to \"ap\"\n",
            "Vocabulary 292 with tokens (97, 114) decodes to \"ar\"\n",
            "Vocabulary 293 with tokens (97, 115) decodes to \"as\"\n",
            "Vocabulary 294 with tokens (97, 116) decodes to \"at\"\n",
            "Vocabulary 295 with tokens (97, 118) decodes to \"av\"\n",
            "Vocabulary 296 with tokens (97, 256) decodes to \"ao \"\n",
            "Vocabulary 297 with tokens (97, 259) decodes to \"as \"\n",
            "Vocabulary 298 with tokens (97, 260) decodes to \"a, \"\n",
            "Vocabulary 299 with tokens (97, 261) decodes to \"ade \"\n",
            "Vocabulary 300 with tokens (97, 263) decodes to \"am \"\n",
            "Vocabulary 301 with tokens (97, 270) decodes to \"a. \"\n",
            "Vocabulary 302 with tokens (97, 271) decodes to \"ado \"\n",
            "Vocabulary 303 with tokens (97, 280) decodes to \"aç\"\n",
            "Vocabulary 304 with tokens (98, 108) decodes to \"bl\"\n",
            "Vocabulary 305 with tokens (98, 114) decodes to \"br\"\n",
            "Vocabulary 306 with tokens (99, 104) decodes to \"ch\"\n",
            "Vocabulary 307 with tokens (99, 105) decodes to \"ci\"\n",
            "Vocabulary 308 with tokens (99, 108) decodes to \"cl\"\n",
            "Vocabulary 309 with tokens (99, 111) decodes to \"co\"\n",
            "Vocabulary 310 with tokens (99, 314) decodes to \"cul\"\n",
            "Vocabulary 311 with tokens (100, 101) decodes to \"de\"\n",
            "Vocabulary 312 with tokens (100, 105) decodes to \"di\"\n",
            "Vocabulary 313 with tokens (100, 117) decodes to \"du\"\n",
            "Vocabulary 314 with tokens (100, 256) decodes to \"do \"\n",
            "Vocabulary 315 with tokens (100, 257) decodes to \"da \"\n",
            "Vocabulary 316 with tokens (100, 258) decodes to \"de \"\n",
            "Vocabulary 317 with tokens (100, 268) decodes to \"des\"\n",
            "Vocabulary 318 with tokens (100, 272) decodes to \"dos \"\n",
            "Vocabulary 319 with tokens (100, 275) decodes to \"das \"\n",
            "Vocabulary 320 with tokens (101, 32) decodes to \"e \"\n",
            "Vocabulary 321 with tokens (101, 99) decodes to \"ec\"\n",
            "Vocabulary 322 with tokens (101, 100) decodes to \"ed\"\n",
            "Vocabulary 323 with tokens (101, 103) decodes to \"eg\"\n",
            "Vocabulary 324 with tokens (101, 105) decodes to \"ei\"\n",
            "Vocabulary 325 with tokens (101, 108) decodes to \"el\"\n",
            "Vocabulary 326 with tokens (101, 109) decodes to \"em\"\n",
            "Vocabulary 327 with tokens (101, 110) decodes to \"en\"\n",
            "Vocabulary 328 with tokens (101, 114) decodes to \"er\"\n",
            "Vocabulary 329 with tokens (101, 115) decodes to \"es\"\n",
            "Vocabulary 330 with tokens (101, 116) decodes to \"et\"\n",
            "Vocabulary 331 with tokens (101, 118) decodes to \"ev\"\n",
            "Vocabulary 332 with tokens (101, 120) decodes to \"ex\"\n",
            "Vocabulary 333 with tokens (101, 259) decodes to \"es \"\n",
            "Vocabulary 334 with tokens (101, 260) decodes to \"e, \"\n",
            "Vocabulary 335 with tokens (101, 263) decodes to \"em \"\n",
            "Vocabulary 336 with tokens (102, 111) decodes to \"fo\"\n",
            "Vocabulary 337 with tokens (102, 264) decodes to \"for\"\n",
            "Vocabulary 338 with tokens (103, 117) decodes to \"gu\"\n",
            "Vocabulary 339 with tokens (105, 32) decodes to \"i \"\n",
            "Vocabulary 340 with tokens (105, 97) decodes to \"ia\"\n",
            "Vocabulary 341 with tokens (105, 99) decodes to \"ic\"\n",
            "Vocabulary 342 with tokens (105, 100) decodes to \"id\"\n",
            "Vocabulary 343 with tokens (105, 103) decodes to \"ig\"\n",
            "Vocabulary 344 with tokens (105, 108) decodes to \"il\"\n",
            "Vocabulary 345 with tokens (105, 109) decodes to \"im\"\n",
            "Vocabulary 346 with tokens (105, 110) decodes to \"in\"\n",
            "Vocabulary 347 with tokens (105, 114) decodes to \"ir\"\n",
            "Vocabulary 348 with tokens (105, 115) decodes to \"is\"\n",
            "Vocabulary 349 with tokens (105, 116) decodes to \"it\"\n",
            "Vocabulary 350 with tokens (105, 118) decodes to \"iv\"\n",
            "Vocabulary 351 with tokens (105, 122) decodes to \"iz\"\n",
            "Vocabulary 352 with tokens (105, 256) decodes to \"io \"\n",
            "Vocabulary 353 with tokens (105, 257) decodes to \"ia \"\n",
            "Vocabulary 354 with tokens (105, 259) decodes to \"is \"\n",
            "Vocabulary 355 with tokens (105, 269) decodes to \"ico\"\n",
            "Vocabulary 356 with tokens (105, 282) decodes to \"ici\"\n",
            "Vocabulary 357 with tokens (105, 285) decodes to \"ist\"\n",
            "Vocabulary 358 with tokens (109, 32) decodes to \"m \"\n",
            "Vocabulary 359 with tokens (109, 357) decodes to \"mais \"\n",
            "Vocabulary 360 with tokens (110, 256) decodes to \"no \"\n",
            "Vocabulary 361 with tokens (110, 257) decodes to \"na \"\n",
            "Vocabulary 362 with tokens (110, 282) decodes to \"nci\"\n",
            "Vocabulary 363 with tokens (111, 32) decodes to \"o \"\n",
            "Vocabulary 364 with tokens (111, 99) decodes to \"oc\"\n",
            "Vocabulary 365 with tokens (111, 103) decodes to \"og\"\n",
            "Vocabulary 366 with tokens (111, 108) decodes to \"ol\"\n",
            "Vocabulary 367 with tokens (111, 109) decodes to \"om\"\n",
            "Vocabulary 368 with tokens (111, 110) decodes to \"on\"\n",
            "Vocabulary 369 with tokens (111, 114) decodes to \"or\"\n",
            "Vocabulary 370 with tokens (111, 115) decodes to \"os\"\n",
            "Vocabulary 371 with tokens (111, 116) decodes to \"ot\"\n",
            "Vocabulary 372 with tokens (111, 118) decodes to \"ov\"\n",
            "Vocabulary 373 with tokens (111, 259) decodes to \"os \"\n",
            "Vocabulary 374 with tokens (111, 260) decodes to \"o, \"\n",
            "Vocabulary 375 with tokens (111, 270) decodes to \"o. \"\n",
            "Vocabulary 376 with tokens (111, 300) decodes to \"ou \"\n",
            "Vocabulary 377 with tokens (111, 335) decodes to \"out\"\n",
            "Vocabulary 378 with tokens (112, 101) decodes to \"pe\"\n",
            "Vocabulary 379 with tokens (112, 108) decodes to \"pl\"\n",
            "Vocabulary 380 with tokens (112, 111) decodes to \"po\"\n",
            "Vocabulary 381 with tokens (112, 114) decodes to \"pr\"\n",
            "Vocabulary 382 with tokens (112, 265) decodes to \"per\"\n",
            "Vocabulary 383 with tokens (112, 267) decodes to \"par\"\n",
            "Vocabulary 384 with tokens (112, 281) decodes to \"pri\"\n",
            "Vocabulary 385 with tokens (112, 289) decodes to \"pel\"\n",
            "Vocabulary 386 with tokens (112, 298) decodes to \"pro\"\n",
            "Vocabulary 387 with tokens (112, 318) decodes to \"por \"\n",
            "Vocabulary 388 with tokens (112, 368) decodes to \"pres\"\n",
            "Vocabulary 389 with tokens (112, 370) decodes to \"port\"\n",
            "Vocabulary 390 with tokens (113, 117) decodes to \"qu\"\n",
            "Vocabulary 391 with tokens (114, 97) decodes to \"ra\"\n",
            "Vocabulary 392 with tokens (114, 101) decodes to \"re\"\n",
            "Vocabulary 393 with tokens (114, 105) decodes to \"ri\"\n",
            "Vocabulary 394 with tokens (114, 111) decodes to \"ro\"\n",
            "Vocabulary 395 with tokens (114, 117) decodes to \"ru\"\n",
            "Vocabulary 396 with tokens (114, 256) decodes to \"ro \"\n",
            "Vocabulary 397 with tokens (114, 258) decodes to \"re \"\n",
            "Vocabulary 398 with tokens (114, 266) decodes to \"ran\"\n",
            "Vocabulary 399 with tokens (114, 268) decodes to \"res\"\n",
            "Vocabulary 400 with tokens (115, 32) decodes to \"s \"\n",
            "Vocabulary 401 with tokens (115, 101) decodes to \"se\"\n",
            "Vocabulary 402 with tokens (115, 111) decodes to \"so\"\n",
            "Vocabulary 403 with tokens (115, 116) decodes to \"st\"\n",
            "Vocabulary 404 with tokens (115, 117) decodes to \"su\"\n",
            "Vocabulary 405 with tokens (115, 258) decodes to \"se \"\n",
            "Vocabulary 406 with tokens (115, 260) decodes to \"s, \"\n",
            "Vocabulary 407 with tokens (115, 279) decodes to \"são \"\n",
            "Vocabulary 408 with tokens (116, 256) decodes to \"to \"\n",
            "Vocabulary 409 with tokens (116, 265) decodes to \"ter\"\n",
            "Vocabulary 410 with tokens (117, 32) decodes to \"u \"\n",
            "Vocabulary 411 with tokens (117, 108) decodes to \"ul\"\n",
            "Vocabulary 412 with tokens (117, 109) decodes to \"um\"\n",
            "Vocabulary 413 with tokens (117, 110) decodes to \"un\"\n",
            "Vocabulary 414 with tokens (117, 114) decodes to \"ur\"\n",
            "Vocabulary 415 with tokens (117, 115) decodes to \"us\"\n",
            "Vocabulary 416 with tokens (117, 116) decodes to \"ut\"\n",
            "Vocabulary 417 with tokens (117, 263) decodes to \"um \"\n",
            "Vocabulary 418 with tokens (118, 105) decodes to \"vi\"\n",
            "Vocabulary 419 with tokens (121, 32) decodes to \"y \"\n",
            "Vocabulary 420 with tokens (122, 32) decodes to \"z \"\n",
            "Vocabulary 421 with tokens (124, 32) decodes to \"| \"\n",
            "Vocabulary 422 with tokens (124, 301) decodes to \"|| \"\n",
            "Vocabulary 423 with tokens (195, 160) decodes to \"à\"\n",
            "Vocabulary 424 with tokens (195, 161) decodes to \"á\"\n",
            "Vocabulary 425 with tokens (195, 162) decodes to \"â\"\n",
            "Vocabulary 426 with tokens (195, 163) decodes to \"ã\"\n",
            "Vocabulary 427 with tokens (195, 167) decodes to \"ç\"\n",
            "Vocabulary 428 with tokens (195, 169) decodes to \"é\"\n",
            "Vocabulary 429 with tokens (195, 170) decodes to \"ê\"\n",
            "Vocabulary 430 with tokens (195, 173) decodes to \"í\"\n",
            "Vocabulary 431 with tokens (195, 179) decodes to \"ó\"\n",
            "Vocabulary 432 with tokens (195, 181) decodes to \"õ\"\n",
            "Vocabulary 433 with tokens (195, 186) decodes to \"ú\"\n",
            "Vocabulary 434 with tokens (226, 128) decodes to \"�\"\n",
            "Vocabulary 435 with tokens (256, 261) decodes to \"o de \"\n",
            "Vocabulary 436 with tokens (257, 109) decodes to \"a m\"\n",
            "Vocabulary 437 with tokens (257, 258) decodes to \"a e \"\n",
            "Vocabulary 438 with tokens (257, 261) decodes to \"a de \"\n",
            "Vocabulary 439 with tokens (257, 271) decodes to \"a do \"\n",
            "Vocabulary 440 with tokens (262, 116) decodes to \"ent\"\n",
            "Vocabulary 441 with tokens (264, 32) decodes to \"or \"\n",
            "Vocabulary 442 with tokens (264, 116) decodes to \"ort\"\n",
            "Vocabulary 443 with tokens (265, 32) decodes to \"er \"\n",
            "Vocabulary 444 with tokens (266, 100) decodes to \"and\"\n",
            "Vocabulary 445 with tokens (266, 116) decodes to \"ant\"\n",
            "Vocabulary 446 with tokens (266, 271) decodes to \"ando \"\n",
            "Vocabulary 447 with tokens (267, 32) decodes to \"ar \"\n",
            "Vocabulary 448 with tokens (268, 115) decodes to \"ess\"\n",
            "Vocabulary 449 with tokens (268, 116) decodes to \"est\"\n",
            "Vocabulary 450 with tokens (268, 260) decodes to \"es, \"\n",
            "Vocabulary 451 with tokens (269, 108) decodes to \"col\"\n",
            "Vocabulary 452 with tokens (269, 109) decodes to \"com\"\n",
            "Vocabulary 453 with tokens (269, 110) decodes to \"con\"\n",
            "Vocabulary 454 with tokens (269, 263) decodes to \"com \"\n",
            "Vocabulary 455 with tokens (270, 65) decodes to \". A\"\n",
            "Vocabulary 456 with tokens (270, 69) decodes to \". E\"\n",
            "Vocabulary 457 with tokens (272, 261) decodes to \"os de \"\n",
            "Vocabulary 458 with tokens (273, 104) decodes to \"inh\"\n",
            "Vocabulary 459 with tokens (274, 32) decodes to \"al \"\n",
            "Vocabulary 460 with tokens (275, 261) decodes to \"as de \"\n",
            "Vocabulary 461 with tokens (276, 256) decodes to \"ão \"\n",
            "Vocabulary 462 with tokens (277, 257) decodes to \"ada \"\n",
            "Vocabulary 463 with tokens (277, 272) decodes to \"ados \"\n",
            "Vocabulary 464 with tokens (278, 256) decodes to \"ento \"\n",
            "Vocabulary 465 with tokens (278, 258) decodes to \"ente \"\n",
            "Vocabulary 466 with tokens (280, 279) decodes to \"ção \"\n",
            "Vocabulary 467 with tokens (282, 288) decodes to \"cion\"\n",
            "Vocabulary 468 with tokens (283, 103) decodes to \"reg\"\n",
            "Vocabulary 469 with tokens (284, 258) decodes to \"que \"\n",
            "Vocabulary 470 with tokens (287, 32) decodes to \"é \"\n",
            "Vocabulary 471 with tokens (287, 263) decodes to \"ém \"\n",
            "Vocabulary 472 with tokens (288, 116) decodes to \"ont\"\n",
            "Vocabulary 473 with tokens (292, 257) decodes to \"ica \"\n",
            "Vocabulary 474 with tokens (294, 115) decodes to \"ass\"\n",
            "Vocabulary 475 with tokens (294, 260) decodes to \"as, \"\n",
            "Vocabulary 476 with tokens (296, 98) decodes to \"amb\"\n",
            "Vocabulary 477 with tokens (299, 32) decodes to \"á \"\n",
            "Vocabulary 478 with tokens (299, 281) decodes to \"ári\"\n",
            "Vocabulary 479 with tokens (302, 32) decodes to \"== \"\n",
            "Vocabulary 480 with tokens (302, 361) decodes to \"=== \"\n",
            "Vocabulary 481 with tokens (304, 259) decodes to \"ais \"\n",
            "Vocabulary 482 with tokens (305, 114) decodes to \"eir\"\n",
            "Vocabulary 483 with tokens (306, 279) decodes to \"ação \"\n",
            "Vocabulary 484 with tokens (307, 395) decodes to \"idade \"\n",
            "Vocabulary 485 with tokens (309, 260) decodes to \"os, \"\n",
            "Vocabulary 486 with tokens (315, 116) decodes to \"cont\"\n",
            "Vocabulary 487 with tokens (316, 57) decodes to \"199\"\n",
            "Vocabulary 488 with tokens (317, 116) decodes to \"part\"\n",
            "Vocabulary 489 with tokens (317, 257) decodes to \"para \"\n",
            "Vocabulary 490 with tokens (319, 257) decodes to \"uma \"\n",
            "Vocabulary 491 with tokens (329, 281) decodes to \"óri\"\n",
            "Vocabulary 492 with tokens (330, 256) decodes to \"como \"\n",
            "Vocabulary 493 with tokens (334, 48) decodes to \"200\"\n",
            "Vocabulary 494 with tokens (334, 49) decodes to \"201\"\n",
            "Vocabulary 495 with tokens (365, 417) decodes to \"ênci\"\n",
            "Vocabulary 496 with tokens (371, 290) decodes to \"ões \"\n",
            "Vocabulary 497 with tokens (380, 148) decodes to \"—\"\n",
            "Vocabulary 498 with tokens (383, 32) decodes to \". A \"\n",
            "Vocabulary 499 with tokens (400, 354) decodes to \"foi \"\n",
            "Vocabulary 500 with tokens (401, 109) decodes to \"form\"\n"
          ]
        }
      ],
      "source": [
        "# Vamos imprimir as subpalavras\n",
        "my_bpe.print_subwords()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}