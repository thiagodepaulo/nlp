{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thyarles/unb-fmc-nlp/blob/main/aula_1/notes_lets_build_the_gpt_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's build the GPT Tokenizer\n",
        "Notes from the video https://www.youtube.com/watch?v=zduSFxRajkE."
      ],
      "metadata": {
        "id": "siTnhXnKCCGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Most of the problem we see on the LLM are from Tokenizers (like do simple math wrong).\n",
        "* The unicode has three types, UTF-8, UTF-16, and UTF-32. The UTF-8 is the standard because it the only one that has variable length. For latin characters, the UTF-16 add zero word on every letter, and UTF-32 add two zero words.\n",
        "* We can't use the Unicode to tokenizer because it has a huge code space (about 150 thousand).  "
      ],
      "metadata": {
        "id": "PA5OEDpzKKzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To check the UTF-8 value\n",
        "[ord(x) for x in \"Charles.\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8bAFEWtDG4V",
        "outputId": "5ec31f5f-5a95-43cb-ecb0-331e22b81d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[67, 104, 97, 114, 108, 101, 115, 46]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To check the UTFs 8, 16 and 32.\n",
        "print(\"%s\\n%s\\n%s\" %\n",
        "(\n",
        "  list(\"Charles.\".encode(\"utf-8\")),\n",
        "  list(\"Charles.\".encode(\"utf-16\")),\n",
        "  list(\"Charles.\".encode(\"utf-32\")))\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4--l6YylDdGy",
        "outputId": "250d158c-91bf-4386-f5c1-92c539b77960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[67, 104, 97, 114, 108, 101, 115, 46]\n",
            "[255, 254, 67, 0, 104, 0, 97, 0, 114, 0, 108, 0, 101, 0, 115, 0, 46, 0]\n",
            "[255, 254, 0, 0, 67, 0, 0, 0, 104, 0, 0, 0, 97, 0, 0, 0, 114, 0, 0, 0, 108, 0, 0, 0, 101, 0, 0, 0, 115, 0, 0, 0, 46, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte pair encoding"
      ],
      "metadata": {
        "id": "3JlXUqeCJ_26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding"
      ],
      "metadata": {
        "id": "vZx6mDI0XBSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: aaabdaaabac (vocabulary size = eleven, four tokens)\n",
        "#          -> find the pair that occurs more frequently and replace it with a single token\n",
        "#          Z = aa, Y = ab, X = zy -> XdXac (vocabulary size = seven, five tokens)\n",
        "\n",
        "# Fancy chars consume more bytes, that's why the code points is less than tokens\n",
        "text = \"\"\"\n",
        "  Alan Turing foi um matemático e criptógrafo inglês considerado atualmente como o\n",
        "  pai da computação, uma vez que, por meio de suas ideias, foi possível desenvolver\n",
        "  o que chamamos hoje de computador. Turing também ficou muito conhecido como um dos\n",
        "  responsáveis por decifrar o código utilizado pelas comunicações nazistas durante\n",
        "  a Segunda Guerra Mundial.\n",
        "\n",
        "  Por meio do seu trabalho, foi desenvolvida uma máquina conhecida como “bomba\n",
        "  eletromecânica” (The Bombe, em inglês), que decifrou o código da máquina Enigma\n",
        "  utilizado pelos alemães, e permitiu que os Aliados tivessem acesso a informações\n",
        "  privilegiadas ao longo da guerra. Turing morreu em 1954, provavelmente tendo\n",
        "  cometido suicídio.\n",
        "\"\"\"\n",
        "tokens = text.encode(\"utf-8\")     # raw bytes\n",
        "tokens = list(map(int, tokens))   # integers from 0 to 255\n",
        "print(\"The text has %d code points and %d tokens.\" % (len(text), len(tokens)))"
      ],
      "metadata": {
        "id": "42y0W-yxDvSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f20a4712-ab6a-4af0-c35d-326b769431d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text has 717 code points and 741 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's find the most frequent value\n",
        "\n",
        "# Using the pythonic way\n",
        "def get_stats_pythonic(ids):\n",
        "  counts = {}\n",
        "  for pair in zip(ids, ids[1:]):\n",
        "    counts[pair] = counts.get(pair, 0) + 1\n",
        "  return counts\n",
        "\n",
        "# Using human way\n",
        "def get_stats_human(ids):\n",
        "    counts = {}\n",
        "    for i in range(len(ids) - 1):\n",
        "        pair = (ids[i], ids[i + 1])\n",
        "        if pair in counts:\n",
        "            counts[pair] += 1\n",
        "        else:\n",
        "            counts[pair] = 1\n",
        "    return counts\n",
        "\n",
        "stats = get_stats_pythonic(tokens)\n",
        "print(sorted(((v, k) for k,v in stats.items()), reverse=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJE8nzsPJ68I",
        "outputId": "cb30428d-2314-47d5-bb01-6f375646eb89"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(24, (111, 32)), (15, (97, 32)), (14, (32, 99)), (13, (115, 32)), (12, (101, 32)), (12, (99, 111)), (12, (32, 100)), (10, (111, 109)), (10, (100, 111)), (10, (32, 32)), (10, (10, 32)), (9, (32, 112)), (8, (105, 110)), (8, (101, 115)), (8, (100, 101)), (8, (44, 32)), (8, (32, 10)), (7, (114, 97)), (7, (100, 97)), (7, (32, 109)), (6, (118, 101)), (6, (117, 101)), (6, (116, 105)), (6, (113, 117)), (6, (111, 115)), (6, (111, 114)), (6, (110, 103)), (6, (109, 101)), (6, (109, 97)), (6, (109, 32)), (6, (105, 100)), (6, (97, 100)), (6, (32, 117)), (6, (32, 97)), (5, (117, 32)), (5, (116, 101)), (5, (114, 105)), (5, (114, 32)), (5, (111, 110)), (5, (109, 111)), (5, (105, 99)), (5, (102, 111)), (5, (101, 114)), (5, (101, 110)), (5, (101, 109)), (5, (101, 108)), (5, (101, 99)), (5, (97, 115)), (5, (32, 111)), (5, (32, 101)), (4, (195, 161)), (4, (117, 116)), (4, (117, 114)), (4, (117, 109)), (4, (117, 105)), (4, (116, 97)), (4, (115, 101)), (4, (112, 111)), (4, (109, 195)), (4, (105, 97)), (4, (105, 32)), (4, (101, 105)), (4, (100, 105)), (4, (99, 195)), (4, (99, 105)), (4, (97, 108)), (4, (32, 116)), (4, (32, 113)), (4, (32, 105)), (4, (32, 102)), (3, (195, 179)), (3, (195, 167)), (3, (167, 195)), (3, (117, 110)), (3, (115, 115)), (3, (114, 114)), (3, (114, 111)), (3, (112, 101)), (3, (111, 105)), (3, (110, 116)), (3, (110, 105)), (3, (110, 100)), (3, (110, 97)), (3, (109, 98)), (3, (108, 105)), (3, (108, 101)), (3, (105, 111)), (3, (105, 108)), (3, (105, 103)), (3, (104, 101)), (3, (103, 111)), (3, (103, 32)), (3, (97, 195)), (3, (97, 109)), (3, (84, 117)), (3, (32, 115)), (3, (32, 84)), (2, (226, 128)), (2, (195, 181)), (2, (195, 173)), (2, (195, 170)), (2, (195, 163)), (2, (181, 101)), (2, (179, 100)), (2, (170, 115)), (2, (161, 113)), (2, (122, 97)), (2, (118, 111)), (2, (118, 105)), (2, (117, 97)), (2, (116, 114)), (2, (115, 195)), (2, (115, 117)), (2, (115, 44)), (2, (114, 109)), (2, (114, 101)), (2, (112, 117)), (2, (112, 114)), (2, (111, 117)), (2, (111, 108)), (2, (111, 44)), (2, (110, 118)), (2, (110, 115)), (2, (110, 104)), (2, (109, 117)), (2, (109, 112)), (2, (108, 195)), (2, (108, 118)), (2, (108, 111)), (2, (108, 109)), (2, (108, 97)), (2, (105, 122)), (2, (105, 118)), (2, (105, 116)), (2, (105, 115)), (2, (105, 102)), (2, (104, 111)), (2, (103, 117)), (2, (103, 108)), (2, (102, 114)), (2, (101, 117)), (2, (101, 116)), (2, (101, 103)), (2, (101, 44)), (2, (99, 97)), (2, (98, 97)), (2, (97, 116)), (2, (97, 110)), (2, (65, 108)), (2, (46, 32)), (2, (46, 10)), (2, (32, 65)), (1, (195, 169)), (1, (195, 162)), (1, (179, 103)), (1, (173, 118)), (1, (173, 100)), (1, (169, 109)), (1, (163, 111)), (1, (163, 101)), (1, (162, 110)), (1, (161, 118)), (1, (161, 116)), (1, (157, 32)), (1, (156, 98)), (1, (128, 157)), (1, (128, 156)), (1, (122, 105)), (1, (122, 32)), (1, (118, 97)), (1, (116, 195)), (1, (116, 117)), (1, (116, 111)), (1, (115, 116)), (1, (115, 112)), (1, (115, 111)), (1, (115, 105)), (1, (115, 41)), (1, (114, 46)), (1, (112, 116)), (1, (112, 97)), (1, (111, 118)), (1, (111, 106)), (1, (111, 46)), (1, (110, 102)), (1, (110, 32)), (1, (109, 105)), (1, (108, 104)), (1, (108, 46)), (1, (108, 32)), (1, (106, 101)), (1, (105, 117)), (1, (105, 112)), (1, (104, 97)), (1, (103, 114)), (1, (103, 109)), (1, (103, 105)), (1, (102, 105)), (1, (101, 122)), (1, (100, 117)), (1, (99, 114)), (1, (99, 104)), (1, (99, 101)), (1, (98, 195)), (1, (98, 111)), (1, (98, 101)), (1, (97, 226)), (1, (97, 122)), (1, (97, 118)), (1, (97, 114)), (1, (97, 111)), (1, (97, 105)), (1, (97, 102)), (1, (97, 99)), (1, (97, 98)), (1, (97, 46)), (1, (84, 104)), (1, (83, 101)), (1, (80, 111)), (1, (77, 117)), (1, (71, 117)), (1, (69, 110)), (1, (66, 111)), (1, (57, 53)), (1, (53, 52)), (1, (52, 44)), (1, (49, 57)), (1, (41, 44)), (1, (40, 84)), (1, (32, 226)), (1, (32, 118)), (1, (32, 114)), (1, (32, 110)), (1, (32, 108)), (1, (32, 104)), (1, (32, 103)), (1, (32, 83)), (1, (32, 80)), (1, (32, 77)), (1, (32, 71)), (1, (32, 69)), (1, (32, 66)), (1, (32, 49)), (1, (32, 40)), (1, (10, 10))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see what are the most printed values\n",
        "chr(111), chr(32) # this is the opposite of ord(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THRRwH7INSEU",
        "outputId": "f822cb0d-fd20-4191-d8bc-43bb0c5f5cb2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('o', ' ')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create news tokens starting from 256\n",
        "top_pair = max(stats, key=stats.get)\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "  # replace ids of the pair with idx\n",
        "  new_ids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    # if not on the last position and finds, replace it\n",
        "    if i < len(ids) -1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      new_ids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      new_ids.append(ids[i])\n",
        "      i += 1\n",
        "  return new_ids\n",
        "\n",
        "# To check\n",
        "# print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
        "new_tokens = merge(tokens, top_pair, 256)\n",
        "# The result should change from 741 to 717 as we had 24 pairs of ('o', ' ')\n",
        "print(\"The text has %d code points and %d tokens.\" % (len(text), len(new_tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKbvoRUCOesc",
        "outputId": "83ae8d6e-2c26-4aa8-e214-0b9bd3b0e243"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text has 717 code points and 717 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The more changes you make, bigger will be your vocabulary and shorter will be\n",
        "# your text. You need to find the best balance.\n",
        "\n",
        "vocabulary_size = 276 # desired vocab size\n",
        "number_merges = vocabulary_size - 256 # minus what we have already\n",
        "ids = list(tokens) # let's keep the original list intact (use list to copy)\n",
        "\n",
        "merges = {}\n",
        "for i in range(number_merges):\n",
        "  stats = get_stats_pythonic(ids)\n",
        "  pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i\n",
        "  print(f\"Merging in a token {idx} the pair {pair}...\")\n",
        "  ids = merge(ids, pair, idx)\n",
        "  merges[pair] = idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAII8Wa7SZmj",
        "outputId": "18ed046a-e956-4078-f742-21a469d24308"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging in a token 256 the pair (111, 32)...\n",
            "Merging in a token 257 the pair (97, 32)...\n",
            "Merging in a token 258 the pair (115, 32)...\n",
            "Merging in a token 259 the pair (101, 32)...\n",
            "Merging in a token 260 the pair (99, 111)...\n",
            "Merging in a token 261 the pair (10, 32)...\n",
            "Merging in a token 262 the pair (261, 32)...\n",
            "Merging in a token 263 the pair (105, 110)...\n",
            "Merging in a token 264 the pair (44, 32)...\n",
            "Merging in a token 265 the pair (100, 256)...\n",
            "Merging in a token 266 the pair (260, 109)...\n",
            "Merging in a token 267 the pair (109, 32)...\n",
            "Merging in a token 268 the pair (116, 105)...\n",
            "Merging in a token 269 the pair (114, 97)...\n",
            "Merging in a token 270 the pair (100, 101)...\n",
            "Merging in a token 271 the pair (100, 257)...\n",
            "Merging in a token 272 the pair (118, 101)...\n",
            "Merging in a token 273 the pair (113, 117)...\n",
            "Merging in a token 274 the pair (111, 114)...\n",
            "Merging in a token 275 the pair (263, 103)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the compression we got\n",
        "print(f'Tokens before merge: {len(tokens)}')\n",
        "print(f'Tokens after merge: {len(ids)}')\n",
        "print(f'Ratio: {len(tokens)/len(ids):.2f}x')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC3bgNjMR79a",
        "outputId": "2ee48ecd-1333-4fd5-e7bb-42d20553133c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens before merge: 741\n",
            "Tokens after merge: 563\n",
            "Ratio: 1.32x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "  # Given the string, retir the ids (list of integers, aka tokens)\n",
        "  tokens = list(text.encode('utf-8'))\n",
        "  while len(tokens) > 1:\n",
        "    stats = get_stats_pythonic(tokens)\n",
        "    # float('inf') is a fallback, a big number if something goes wrong on the index\n",
        "    pair = min(stats, key=lambda p: merges.get(p, float('inf')))\n",
        "    if pair not in merges:\n",
        "      break # nothing else can be merged\n",
        "    idx = merges[pair]\n",
        "    tokens = merge(tokens, pair, idx)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "bftQPQW7nlAv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding"
      ],
      "metadata": {
        "id": "pv3HUmHGW8wY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Populate the vocab with map from 0 to 255\n",
        "vocabulary = {idx: bytes([idx]) for idx in range(256)}\n",
        "\n",
        "# Now let's get the remaining items from merges\n",
        "for (p0, p1), idx in merges.items():\n",
        "  # vocabulary is just a byte object, so the plus is a concatenation\n",
        "  vocabulary[idx] = vocabulary[p0] + vocabulary[p1]\n",
        "\n",
        "def decode(idx):\n",
        "  # Given the ids (list of integers), find the string related\n",
        "  text_bytes = b''.join(vocabulary[idx] for idx in ids)\n",
        "  # The errors can be strict, ignore, replace, backslashreplace or surrogatescape\n",
        "  return text_bytes.decode(encoding='utf-8', errors='replace')"
      ],
      "metadata": {
        "id": "L7UT4frFW5RX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python code"
      ],
      "metadata": {
        "id": "HxLYhOcFqOC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BPE (Byte-Pair Encoding)\n",
        "\n",
        "# Quanto maiores forem as modificações, maior será o vocabulário e menor será\n",
        "# o texto. Precisamos encontrar o equilíbrio.\n",
        "\n",
        "# __init__      : chama o criador do vocabulario e um dicionário vazio\n",
        "# _build_vocab  : cria o vocabulário\n",
        "# _get_stats    : conta a ocorrência dos pares em determinada string\n",
        "# _merge        : troca as ocorrências de maior frequência por novo vocabulário\n",
        "# train         : procura os caracteres frequentes e une, criando novo vocabulário\n",
        "# encoding      : converte texto em ids\n",
        "# decoding      : converte ids em texto\n",
        "# print_subwords: imprime a representação criada no treinamento\n",
        "\n",
        "class MyBPE():\n",
        "\n",
        "    # Inicializa dicionário para merges e vocabulário, tornando-os disponíveis\n",
        "    # durante toda existência da classe\n",
        "    def __init__(self):\n",
        "        self.merges = {}\n",
        "        self.vocab = self._build_vocab()\n",
        "\n",
        "    # Cria o vocabulário, é chamada imediatamente após a instância da classe\n",
        "    def _build_vocab(self):\n",
        "        # Cria o vocabulário inicial, com valores de 0 a 255\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        # Estende o vocabulário com base nos valores que tiveram merge\n",
        "        # Observe que a soma é uma concatenação, pois vocab é byte\n",
        "        for (p0, p1), idx in self.merges.items():\n",
        "            vocab[idx] = vocab[p0] + vocab[p1]\n",
        "        return vocab\n",
        "\n",
        "\n",
        "    # Conta as ocorrências de cada par em determinado ids\n",
        "    def _get_stats(self, ids):\n",
        "        counts = {}\n",
        "        for pair in zip(ids, ids[1:]):\n",
        "            # Incrementa a cada par\n",
        "            counts[pair] = counts.get(pair, 0) + 1\n",
        "        return counts\n",
        "    # Outra forma de fazer (versão mais humana e menos pytônica)\n",
        "      # def _get_stats(ids):\n",
        "      # counts = {}\n",
        "      # for i in range(len(ids) - 1):\n",
        "      #     pair = (ids[i], ids[i + 1])\n",
        "      #     if pair in counts:\n",
        "      #         counts[pair] += 1\n",
        "      #     else:\n",
        "      #         counts[pair] = 1\n",
        "      # return counts\n",
        "\n",
        "\n",
        "    # Troca determinado par por um novo vocabulário\n",
        "    def _merge(self, ids, pair, idx):\n",
        "        newids = []\n",
        "        i = 0\n",
        "        while i < len(ids):\n",
        "            # Verifica o par na sequência e pula se houver merge (+2)\n",
        "            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
        "                newids.append(idx)\n",
        "                i += 2\n",
        "            # Não teve merge, mantenha o valor e vai para o próximo\n",
        "            else:\n",
        "                newids.append(ids[i])\n",
        "                i += 1\n",
        "        return newids\n",
        "\n",
        "\n",
        "    # Treina o modelo adicionando cada par unido no dicionário\n",
        "    def train(self, text, vocab_size):\n",
        "        # Se o vocabulário for maior que 255, sinalize o erro\n",
        "        assert vocab_size >= 256\n",
        "        # O vocabulário já tem 256, calcule a diferença\n",
        "        num_merges = vocab_size - 256\n",
        "        # Converta o texto de entrada para bytes, usando unicode UTF-8\n",
        "        text_bytes = text.encode(\"utf-8\")\n",
        "        # Cada byte tem que ser um elemento independente\n",
        "        ids = list(text_bytes)\n",
        "        # Dicionário para salvar o processamento local\n",
        "        merges = {}\n",
        "        # Vocabulário com os valores de 0 a 255\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for i in range(num_merges):\n",
        "            # Pega a frequênica dos pares\n",
        "            stats = self._get_stats(ids)\n",
        "            # Procura pelo de maior frequência\n",
        "            pair = max(stats, key=stats.get)\n",
        "            # Incrementa identificador do vocabulário\n",
        "            idx = 256 + i\n",
        "            # Faz a união do par no identificador idx\n",
        "            ids = self._merge(ids, pair, idx)\n",
        "            # Registra como uma união para operações de [de]codificação\n",
        "            merges[pair] = idx\n",
        "            # Atualiza vocabulário\n",
        "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
        "        # Salva os merges na instância\n",
        "        self.merges = merges\n",
        "        # Salva o vocabulário na instância\n",
        "        self.vocab = vocab\n",
        "\n",
        "\n",
        "    # Decodifica determinados ids em texto\n",
        "    def decode(self, ids):\n",
        "        # Cria a cadeia de bytes\n",
        "        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
        "        # Decodifica usando UTF-8: se tiver erro de conversão, troque\n",
        "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text\n",
        "\n",
        "\n",
        "    # Codifica determinado texto em ids\n",
        "    def encode(self, text):\n",
        "        # Converte o texto para byte usando UTF-8\n",
        "        text_bytes = text.encode(\"utf-8\")\n",
        "        # Converte em lista para que cada byte seja independente\n",
        "        ids = list(text_bytes)\n",
        "        # A lista tem que ter pelo menos 2 elementos, do contrário\n",
        "        # não faz qualquer sentido o merge\n",
        "        while len(ids) >= 2:\n",
        "            stats = self._get_stats(ids)\n",
        "            # float('inf') é um fallback, um número gigante que garantirá um\n",
        "            # valor mínimo caso tenha algo errado no index\n",
        "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
        "            if pair not in self.merges:\n",
        "                # Se chegou aqui, não há nada mais para unir, saia\n",
        "                break\n",
        "            # Recupere o idx da instância\n",
        "            idx = self.merges[pair]\n",
        "            # Recuere o ids da instância e faça a união do par\n",
        "            ids = self._merge(ids, pair, idx)\n",
        "        return ids\n",
        "\n",
        "\n",
        "    # Imprime a lista de subpalavras criadas no treinamento\n",
        "    def print_subwords(self):\n",
        "      merges = sorted(self.merges)\n",
        "      # Como o vocabulário base tem 256 itens, vamos começar do 257\n",
        "      i = 257\n",
        "      for tokens in merges:\n",
        "        print(f'Vocabulary {i} with tokens {tokens} decodes to \"{self.decode(tokens)}\"')\n",
        "        i += 1\n"
      ],
      "metadata": {
        "id": "0FnCCbJhs6bR"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teste = MyBPE()\n",
        "teste.train(\"hello world\", 260)"
      ],
      "metadata": {
        "id": "qAGY9wO1vHx6"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teste.print_subwords()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAH6x0CPxK17",
        "outputId": "1923a19d-4160-47d9-bbfa-7aac3bafcc44"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary 257 with tokens (104, 101) decodes to \"he\"\n",
            "Vocabulary 258 with tokens (256, 108) decodes to \"hel\"\n",
            "Vocabulary 259 with tokens (257, 108) decodes to \"hell\"\n",
            "Vocabulary 260 with tokens (258, 111) decodes to \"hello\"\n"
          ]
        }
      ]
    }
  ]
}