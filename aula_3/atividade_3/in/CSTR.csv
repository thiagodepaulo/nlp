file_name,text,class
126.txt,"Rhetorical (Rhet) is a programming / knowledge representation system that offers a set of tools for building an automated reasoning system. Its emphasis is on flexibility of representation. This document extends TR 326 with more information about the internals of the Rhet system. In addition it provides the information needed for users to write their own builtin functions, or better lispfns (that use internally provided library functions).",ArtificiallIntelligence
5.txt,"Reduction is the operation of transforming a production in a Linear Context-Free Rewriting System (LCFRS) into two simpler productions by factoring out a subset of the nonterminals on the production's righthand side. Reduction lowers the rank of a production but may increase its fan-out. We show how to apply reduction in order to minimize the parsing complexity of the resulting grammar, and study the relationship between rank, fan-out, and parsing complexity. We show that it is always possible to obtain optimum parsing complexity with rank two. However, among transformed grammars of rank two, minimum parsing complexity is not always possible with minimum fan-out.",ArtificiallIntelligence
48.txt,"For years, researchers have used knowledge-intensive techniques for disambiguating during parsing. These techniques required a lot of hand-coded information, thus they would not scale to large domains. In addition, they often required the invention of pseudo-probabilities, which also do not scale, and provide ill-founded quantitative measures. The data-driven techniques, which have become popular over the past few years, seem appealing in light of this: once you have an annotated corpus, there is no need to code up knowledge bases or invent ""magic numbers."" However, these methods also have extensive failings, which we will detail. We present a framework for corpus-based syntactic disambiguation which pulls together the well-foundedness of the traditional approaches and the scalability of the corpus-based approaches. We also present a model of language production that places greater emphasis on lexical statistics.",ArtificiallIntelligence
81.txt,"Proceedings of a workshop held in conjunction with the 12th International International Conference on Machine Learning in July 1995 in Tahoe City, CA.",ArtificiallIntelligence
25.txt,"The Medication Advisor is the latest project of the Conversational Interaction and Spoken Dialogue research group at the University of Rochester. The goal of the project is an intelligent assistant that interacts with its users via conversational natural language, and provides them with information and advice regarding their prescription medications. Managing prescription drug regimens is a major problem, particularly for older people living at home who tend to have both complex medication schedules and, often, somewhat reduced faculties for keeping track of them. Patient compliance with prescribed regimens is notoriously low, leading to incorrect and sometimes harmful usage of both prescribed and over-the-counter medications. The Medication Advisor builds on our prior experience constructing conversational assistants in other domains. In addition to providing new challenges, the project allows us to validate previous efforts in areas such as portability. This brief report details our initial efforts and outlines our future direction.",ArtificiallIntelligence
46.txt,"An investigation of the referring behavior of personal and demonstrative pronouns in two corpora: a collection of problem-solving dialogs from the TRAINS93 corpus and prepared news stories from the Boston University Radio Corpus. Unlike most studies of pronominal reference, which limit themselves to pronouns that co-specify the meaning of another noun phrase (called coreference annotation), this study has a wider scope and includes all pronouns in the discourse. As a result, a broader characterization is possible for the pronouns in question. This study shows that current models of pronoun resolution that assume each pronoun to have a nominal antecedent are of limited utility when applied to spontaneous language.",ArtificiallIntelligence
76.txt,"Based on the observation that the unpredictable nature of conversational speech makes it almost impossible to reliably model sequential word constraints, the notion of {\em word set error criteria} is proposed for improved recognition of spontaneous dialogues. The single pass Adaptive Boosting (AB) algorithm enables the language model weights to be tuned using the {\em word set error} criteria. In the two pass version of the algorithm, the basic idea is to predict a {\em set} of words based on some {\em a priori} information, and perform a re-scoring pass wherein the probabilities of the words in the predicted word set are amplified or {\em boosted} in some manner. An adaptive gradient descent procedure for tuning the {\em word boosting} factor has been formulated which enables the boost factors to be incrementally adjusted to maximize accuracy of the speech recognition system outputs on held-out training data using the word set error criteria. Two novel models which predict the required word sets have been presented: {\em utterance triggers} which capture within-utterance long-distance word inter-dependencies, and {\em dialogue triggers} which capture local temporal dialogue-oriented word relations. The proposed Trigger and Adaptive Boosting (TAB) algorithm, and the single pass Adaptive Boosting (AB) algorithm have been experimentally tested on a subset of the TRAINS-93 spontaneous dialogues and the TRAINS-95 semi-spontaneous corpus, and have resulted in improved performances.",ArtificiallIntelligence
9.txt,"Research in Information Extraction has been overly focused on the extraction of facts concerning individuals as compared to general knowledge pertaining to classes of entities and events. In addition, preference has been given to simple techniques in order to enable high volume throughput.  In what follows we give examples of existing work in the field of knowledge acquisition, then follow with ideas on areas for exploration beyond the current state of the art, specifically with respect to the extraction of conditional knowledge, making use of deeper linguistic analysis than is currently the norm.",ArtificiallIntelligence
30.txt,"State of the art plan recognition for use in natural language dialogue systems has progressed in coverage of discourse phenomena and plan navigation strategies. Most systems, however, suffer from several deficiencies, namely, they do not have a specific strategy for the (inevitable) case where they make an incorrect hypothesis inference and they cannot handle interleaved plan navigation, where a user jumps back and forth between several plans. In addition, most plan recognition systems cannot handle the rich variety of possible natural language utterances a dialogue system may receive as input, especially the case where a language utterance corresponds to several actions that the system considers to be atomic. We discuss previous work in plan recognition, especially in the area of dialogues systems. We then describe a plan recognition system that can recover from incorrect inferences, handles interleaved plan navigation, and handles several linguistic phenomena, including support for natural language multi-action paraphrase.",ArtificiallIntelligence
41.txt,"Image-based object recognition systems developed recently don't require the construction of a 3D geometric model, allowing recognition of objects for which current geometric recognition technologies do not apply. Such systems are typically trained with labeled, clean views that cover the whole viewing sphere and can sometimes handle generic, visually similar classes with moderate variation. It has been little explored whether such systems can be trained from imagery that is unlabeled, and whether they can be trained from imagery that is not trivially segmentable.  In this report we investigate how an object recognition system developed previously can be trained from clean images of objects with minimal supervision. After training this system on a single or a small number of views of each object, a simple learning algorithm is able to attract additional views to the object representation, building clusters of views belonging to the same object. We explore how the learning performance improves by extending the set of views, introducing a small amount of supervision, or using more complicated learning algorithms.",ArtificiallIntelligence
56.txt,"Neurons in the visual cortex are known to possess localized, oriented receptive fields. It has previously been suggested that these distinctive properties may reflect an efficient image encoding strategy based on maximizing the sparseness of the distribution of output neuronal activities or alternately, extracting the independent components of natural image ensembles. Here, we show that a relatively simple neural solution to the problem of transformation-invariant visual recognition also causes localized, oriented receptive fields to be learned from natural images. These receptive fields, which code for various transformations in the image plane, allow a pair of cooperating neural networks, one estimating object identity (``what'') and the other estimating object transformations (``where''), to simultaneously recognize an object and estimate its pose by jointly maximizing the a posteriori probability of generating the observed visual data. We provide experimental results demonstrating the ability of these networks to factor retinal stimuli into object-centered features and object-invariant transformations. The resulting neuronal architecture suggests concrete computational roles for the neuroanatomical connections known to exist between the dorsal and ventral visual pathways.",ArtificiallIntelligence
4.txt,"Many of the previous efforts in generalizing over knowledge extracted from text have relied on the use of manually created word sense hierarchies, such as WordNet. We present initial results on generalizing over textually derived knowledge, through the use of the LDA topic model framework, as the ï¬rst step towards automatically building corpus speciï¬c ontologies.",ArtificiallIntelligence
96.txt,"Temporal projection is a crucial task in planning. In order to achieve its goals, the planner must be able to reason about the consequences of its actions. In the real world, the planner does not have complete information about the environment or even the consequences of its actions. The planner thus must be able to reason about the probabilistic nature of the world and the probabilistic effects of its action.  Interactions between actions and events in the world are not only probabilistic; they also are temporally complex. Simultaneous events can interact in many different ways depending on their temporal properties. An action's temporal relation to its preconditions and effects can also become quite complex.  Existing projection systems are weak in their representations of temporally complex actions and events in a probabilistic world. Those that can handle probabilistic situations have a limited representation of temporal relations, while those that can handle complex temporal relations generally assume the world to be completely deterministic. Moreover, the speed of existing probabilistic projection systems are low. These systems are impractical to scale up to larger problems. In this thesis, we propose a practical projection system that can handle both the probabilistic nature of the world and the temporally complex nature of actions. The projection is based on simulation methods. Projection is done by simulating possible courses of events, one at a time. The simulation traces are then collected and processed by a projection module front-end, which provides the planner and the execution monitor with probabilistic estimates of propositions' values. The representation allowed by the system is rich in both the temporal and the probabilistic aspects.",ArtificiallIntelligence
7.txt,"Existing work in the extraction of commonsense knowledge from text has been restricted to factoids that serve as statements about what may possibly obtain in the world. We present an approach to deriving stronger general claims from large sets of factoids. The idea is to coalesce the observed nominals for a given predicate argument into a few predominant types, obtained as WordNet synsets. The results can be construed as generically quantiï¬ed sentences restricting the semantic type of an argument position of a predicate.",ArtificiallIntelligence
47.txt,"Natural language generation is a knowledge-intensive, goal-directed process involving many interacting choices. Some questions that a generation system must answer include: (1) What information needs to be included in the output to satisfy the speaker's or writer's communicative goals? (2) How should a discourse contribution be structured to ensure its coherence? (3) Which modalities should be used to maximize the information exchange? (4) How can output be tailored to specific users? In this paper, we examine some aspects of natural language generation that constrain the planning process, including theories of discourse structure, models of discourse context and of users, and multimodal generation.",ArtificiallIntelligence
72.txt,"Currently, the TRAINS dialog system uses a more or less standard chart parser as the interface between the text of the dialog and the rest of the dialog processing system. However, traditional chart parsers are not well equipped to handle dialogs because dialog constituents can be discontinuous, with interspersed acknowledgments, editing terms, repairs, etc. This paper proposes some modifications of the current TRAINS parser enabling it to handle discontinuous dialog structure. The representation of a dialog is still superficially hierarchical (rather than consisting of interleaved structures). This is made possible by two devices: one is to accommodate repairs (e.g., to uh .. to Corning) through explicit grammar rules; the other is to accommodate mid-sentence acknowledgments (e.g., okay), editing terms (e.g., uh), etc. as ``trailers"" attached to lexical items. We show how this works on a simple sample dialog. Because allowing for repairs and interruptions introduces much ambiguity, we also discuss some initial disambiguation techniques.",ArtificiallIntelligence
77.txt,"Recent neurophysiological experiments appear to indicate that the responses of visual cortical neurons in a monkey freely viewing a natural scene can sometimes differ substantially from those obtained when the same image subregions are flashed during a conventional fixation task. These new findings attain significance from the fact that neurophysiological research in the past has been based predominantly on cell recordings obtained during fixation tasks, under the assumption that these data would be useful in predicting responses in more general situations. We describe a hierarchical model of visual memory that reconciles the two differing experimental results mentioned above by predicting neural responses in both fixating and free-viewing conditions. The model dynamically combines input-driven bottom-up signals with expectation-driven top-down signals to achieve optimal estimation of current state using a Kalman filter based framework. The architecture of the model posits a role for the reciprocal connections between adjoining visual cortical areas in determining neural response properties.",ArtificiallIntelligence
80.txt,"The focus of this thesis proposal is to improve the ability of a computational system to understand spoken utterances in a dialogue with a human. Available computational methods for word recognition do not perform as well on spontaneous speech as we would hope. Even a state of the art recognizer achieves slightly worse than 70\% word accuracy on (nearly) spontaneous speech in a conversation about a specific problem.  To address this problem, I will explore novel methods for post-processing the output of a speech recognizer in order to correct errors. I adopt statistical techniques for modeling the noisy channel from the speaker to the listener in order to correct some of the errors introduced there. The statistical model accounts for frequent errors such as simple word/word confusions and short phrasal problems (one-to-many word substitutions and many-to-one word concatenations). To use the model, a search algorithm is required to find the most likely correction of a given word sequence from the speech recognizer. The post-processor output should contain fewer errors, thus making interpretation by higher levels, such as parsing, more reliable.  Spontaneous speech is also challenging to process because it is more incremental than written language. Utterances frequently form brief phrases and fragments rather than full sentences; they tend to come in installments and refinements. Known methods for parsing do not perform as well as we would like in the face of these linguistic ambiguities and idiosyncrasies. Even state of the art algorithms for parsing spontaneous language sustain high error rates.  To address the incrementality of spontaneously spoken utterances, I will develop methods for segmenting a given utterance into ``chunks'' representing individual thoughts. Given an utterance of spontaneous speech, a tool for automatic prosodic feature extraction will analyze the output of the error-correcting post-processor and the acoustic waveform to generate prosodic cues. These cues will aid a robust parser using a prosody-wise grammar to identify the incremental phrases in the utterance and to provide a syntactic analysis.  These components will augment the {\sc Trains-95} conversational planning assistant.",ArtificiallIntelligence
34.txt,"Statistical techniques have revolutionized all areas of natural language processing, and syntactic parsing is no exception. The availability of large syntactically annotated corpora (principally through the Penn Treebank project) has precipitated parsing's shift from the task of constructing interpretations to the task of constructing a labeled bracketing.  These corpus-based techniques are robust and scalable, two desiderata lacking in early, knowledge-based approaches to parsing. The early approaches are typified by parsers that could operate only in a narrow domain, but that produced semantically interpretable parses. In contrast, the corpus-based approaches produce underspecified labeled bracketings that are not sufficiently detailed for applications in natural language understanding.  In this dissertation we describe a parser that uses hand-written linguistically informed knowledge sources (grammar, lexicon, ontology) to enrich the labeled bracketing in the Penn Treebank. The enriched corpus is then used as the data source for statistical parsing in our well-founded framework. Furthermore, parsing in this framework supports a fully-lexicalized parsing model, and allows for the natural integration of word sense disambiguation with syntactic disambiguation. We show that jointly modeling word sense ambiguity and syntactic ambiguity results in improved syntactic disambiguation. We also describe our treatment of coordinated structures (a topic generally ignored in statistical parsing), and our novel method for using an ontology to settle on backed-off estimators via hypothesis testing.",ArtificiallIntelligence
120.txt,"Recognition of motion sequences is a crucial ability for biological and robot vision systems. We present an architecture for the higher-level processes involved in recognition of complex structured motion. The work is focused on modeling human recognition of Moving Light Displays. MLDs are image sequences that contain only motion information at a small number of locations. Despite the extreme paucity of information in these displays, humans can recognize MLDs generated from a variety of common human movements. This dissertation explores the high-level representations and computational processes required for the recognition task. The structures and algorithms are articulated in the language of structured connectionist models. The implemented network can discriminate three human gaits from data generated by several actors.  Recognition of any motion involves indexing into stored models of movement. We present a representation for such models, called scenarios, based on coordinated sequences of discrete motion events. A method for indexing into this representation is described. We develop a parallel model of spatial and conceptual attention that is essential for disambiguating the spatially and temporally diffuse MLD data. The major computational problems addressed are: (1) representation of time-varying visual models; (2) integration of visual stimuli over time; (3) gestalt formation in and between spatially-localized feature maps and central movement representations; (4) contextual feedback to lower levels; and (5) the use of attention to focus processing on particular spatial locations and particular high-level representations. Several novel connectionist mechanisms are developed and used in the implementation.  In particular, we present advances in connectionist representation of temporal sequences and in using high-level knowledge to control an attentional mechanism. We show that recognition of gait can be achieved directly from motion features, without complex shape information, and that the motion information need not be finely quantized. We show how the ""what"" and ""where"" processes in vision can be tightly coupled in a synergistic fashion. These results indicate the value of the structured connectionist paradigm in modeling perceptual processes: no previous computational model has accounted for MLD recognition and we do not know how it would be approached in any other paradigm.",ArtificiallIntelligence
24.txt,"Language is about symbols and those symbols must be grounded in the physical environment during human development. Most recently, there has been an increased awareness of the essential role of inferences of speakersU referential intentions in grounding those symbols. Experiments have shown that these inferences as revealed in eye, head and hand movements serve as an important driving force in language learning at a relatively early age. The challenge ahead is to develop formal models of language acquisition that can shed light on the leverage provided by embodiment. We present an implemented computational model of embodied language acquisition that learns words from natural interactions with users. The system can be trained in unsupervised mode in which users perform everyday tasks while providing natural language descriptions of their behaviors. We collect acoustic signals in concert with user-centric multisensory information from nonspeech modalities, such as userUs perspective video, gaze positions, head directions and hand movements. A multimodal learning algorithm is developed that firstly spots words from continuous speech and then associates action verbs and object names with their grounded meanings. The central idea is to make use of non-speech contextual information to facilitate word spotting, and utilize userUs attention as deictic reference to discover temporal correlations of data from different modalities to build lexical items. We report the results of a series of experiments that demonstrate the effectiveness of our approach.",ArtificiallIntelligence
87.txt,"In ordinary first-order logic, a valid inference in a language {\bf L} is one in which the conclusion is true in every model of the language in which the premises are true.  To accommodate inductive/uncertain/probabilistic/non-monotonic inference, we weaken that demand to the demand that the conclusion be true in a large proportion of the models in which the relevant premises are true. More generally, we say that an inference is [p,q] valid if its conclusion is true in a proportion lying between p and q of those models in which the relevant premises are true. If we include a statistical variable binding operator ``%'' in our language, there are many quite general (and useful) things we can say about uncertain validity. A surprising result is that some of these things may conflict with Bayesian conditionalization.",ArtificiallIntelligence
115.txt,"Explanation closure (EC) axioms were previously introduced as a means of solving the frame problem. This paper provides a thorough demonstration of the power of EC combined with action closure (AC) for reasoning about dynamic worlds, by way of Sandewall's test suite of 12-or-so problems [Sandewall 1991; 1992]. Sandewall's problems range from the ""Yale turkey shoot"" (and variants) to the ""stuffy room"" problem, and were intended as a test and challenge for nonmonotonic logics of action. The EC/AC-based solutions for the most part do not resort to nonmonotonic reasoning at all, yet yield the intuitively warranted inferences in a direct, transparent fashion. While there are good reasons for ultimately employing nonmonotonic or probabilistic logics---e.g., pervasive uncertainty and the qualification problem---this does show that the scope of monotonic methods has been underestimated. Subsidiary purposes of the paper are to clarify the intuitive status of EC axioms in relation to action effect axioms; and to show how EC, previously formulated within the situation calculus, can be applied within the framework of a temporal logic similar to Sandewall's ""discrete fluent logic,"" with some gains in clarity.",ArtificiallIntelligence
82.txt,"This report describes a corpus of task-oriented dialogues set in the TRAINS domain. A user collaborates with a planning assistant to accomplish some task involving manufacturing and shipping goods in a railroad freight system. We include a description of the task, collection situation, and transcriptions conventions. The audio files, along with time-aligned word and phoneme transcriptions are available on CD-ROM from the Linguistic Data Consortium. Altogether, there are 98 dialogs included, collected using 20 different tasks and 34 different speakers. This amounts to six and a half hours of speech, about 5900 speaker turns, and 55000 transcribed words.",ArtificiallIntelligence
112.txt,"This paper describes the performance evaluation of six temporal reasoning systems. We show that if you are working with large temporal datasets where information is added incrementally throughout the execution of the program, systems using incompletely connected graphs (i.e., TMM, TimeGraph and TimeGraph-II) seem the best option. While they do not offer the constant query time of systems using fully connected graphs (i.e., the systems based on constraint satisfaction), the savings at assertion time are so substantial that the relatively small performance penalty for queries is a reasonable tradeoff. Of course, these systems do not offer the expressivity of the interval-based systems as they only handle point-based relations. Of the three, TimeGraph-II offers a wider range of qualitative relations as it handles point inequality. It does not currently handle metric information, however, as do TMM and TimeGraph. Thus decisions between these three may be more determined by the reasoning capabilities required rather than raw performance.",ArtificiallIntelligence
100.txt,"The problem of ambiguity is central to any theory of language interpretation, whether our interest is in language processing in humans or in developing a usable natural language processing system. Psycholinguistic evidence suggests that human subjects are able to choose an interpretation when necessary, and that competing factors are involved in this choice; however, no theory of language interpretation deals satisfactorily with the combinatorial explosion paradox---the fact that no matter how ambiguous natural language sentences are, they are usually interpreted without significant effort.  The main idea presented in this dissertation is that the scope preferences observed in the literature are not obtained by an independent `scope disambiguation' module, but are the result of independent interpretation processes such as definite description interpretation or the interpretation of modals. None of these interpretive procedures is especially concerned with `scope disambiguation,' but the result of these inferences is that relations of contextual dependency such as anaphoric reference or presuppositionality become part of the common ground; the scope preferences observed in the literature reflect these relations of dependency. The dissertation includes a formal proposal concerning the representation of contextual dependency and its impact on the semantics of sentence constituents.  The theory of ambiguity here presented is based on a distinction between semantic ambiguity, that can be captured implicitly, by means of underspecified representations, and perceived ambiguity, that results from the process of discourse interpretation. A new model of the common ground is introduced, that can be used to characterize both situations characterized by the presence of semantic ambiguity, and situations characterized by the existence of perceived ambiguity.  The reasoning that leads to the establishment of scoping preferences makes use, I argue, of information that is pragmatic in nature; this calls for a model of discourse interpretation in which the common ground contains such information. In the case of spoken language conversations, the common ground must be a model of the discourse situation of the conversational participants.",ArtificiallIntelligence
84.txt,"We describe some simple domain-independent improvements to plan-refinement strategies for well-founded partial order planning that promise to bring this style of planning closer to practicality. One suggestion concerns the strategy for selecting plans for refinement among the current (incomplete) candidate plans. We propose an A* heuristic that counts only steps and open conditions, while ignoring ""unsafe conditions"" (threats). A second suggestion concerns the strategy for selecting open conditions (goals) to be established next in a selected incomplete plan. Here we propose a variant of a strategy suggested by Peot and Smith and studied by Joslin and Pollack; the variant gives top priority to unmatchable open conditions (enabling the elimination of the plan), second-highest priority to goals that can only be achieved uniquely, and otherwise uses LIFO prioritization. The preference for uniquely achievable goals is a ""zero-commitment"" strategy in the sense that the corresponding plan refinements are a matter of deductive certainty, involving no guesswork. In experiments based on modifications of UCPOP, we have obtained improvements by factors ranging from 5 to several hundred for a variety of problems that are nontrivial for the unmodified version. Crucially, the hardest problems give the greatest improvements.",ArtificiallIntelligence
38.txt,"In this report we discuss the creation and initial annotation of the Monroe corpus, a collection of video and audio data of 20 human-human, mixed-initiative, task-oriented dialogs about disaster-handling tasks. We describe how the dialogs were collected, what tasks were used, and how the data was transcribed and aligned.",ArtificiallIntelligence
128.txt,"RPRS is a hierarchical plan recognition system built within the RHET knowledge representation system. It provides a powerful system for plan recognition based on the algorithms of Kautz, with the general reasoning capabilities of RHET. RPRS takes special advantage of Rhet's type relations, constraints, equality, and contextual reasoning abilities.  RPRS is also intended as a demonstration of the Rhet programming and knowledge representation system's hybrid reasoning capabilities. Utilizing the lisp interface to Rhet, RPRS allows the user to use the Rhet structured type system to build plan types, and given some observation or set of observations have Rhet derive the set of plans that are consistent with these observations. Since RPRS includes the TEMPOS specialized reasoner for Rhet, steps and observations can have reference to time-intervals, and/or be temporally constrained with respect to one another.",ArtificiallIntelligence
11.txt,"We address the problem of domain-dependence in semantic role labeling systems by attempting to bootstrap from unlabeled data in new domains. We explore a variety of methods for bootstrapping, and compare two machine learning techniques, decision lists and maximum entropy.",ArtificiallIntelligence
36.txt,"One of the less appreciated obstacles to scaling multi-agent systems is understanding the impact of the role(s) that people will play in those systems. As we try to adapt existing software tools and agent-based applications to play supportive roles in larger multi-agent systems, we must develop strategies for coordinating not only the problem-solving behavior of these agent communities, but also their information sharing and interactive behavior. Our research interest is in mixed-initiative control of intelligent systems [Burstein and McDermott, 1996; Burstein et al., 1998; Ferguson et al., 1996a] and, in particular, of interactive planning systems comprised of a heterogeneous collection of software agents. In this paper, we describe our experience constructing a prototype tool combining elements of TRIPS [Ferguson and Allen, 1998], an interactive, mixed-initiative agent-based planning architecture using spoken natural language dialogue, with the CAMPS Mission Planner, an interactive airlift scheduling tool developed for the Air Force [Emerson and Burstein, 1999], together with some related resource management agents representing other parts of the airlift planning organization. The latter scheduling tools were not originally designed to participate as part of a mixed-initiative, interactive agent community, but rather were designed for direct user interaction through their own GUIs. We describe some requirements revealed by this effort for effective mixed-initiative interaction in such an environment, including the role of explanation, the need for contextual information sharing among the agents, and our approach to intelligent invocation and integration of available agent capabilities.",ArtificiallIntelligence
51.txt,"Discourse markers, also known as clue words, are used extensively in human-human task-oriented dialogs to signal the structure of the discourse. Previous work showed their importance in monologs and social conversations for marking discourse structure, but little attention has been paid to their importance in spoken dialog systems. This paper investigates what discourse markers signal about the upcoming speech, and when they tend to be used in task-oriented dialog. We demonstrate that there is a high correlation between specific discourse markers and specific conversational moves, between discourse marker use and adjacency pairs, and between discourse markers and the speaker's orientation to information presented in the prior turn. We limit our analysis to turn-initial discourse markers and discover new patterns in their interaction with phenomena specific to dialog.",ArtificiallIntelligence
33.txt,"This manual describes a method for annotating rhetorical relations, adjacency pairs, and other argumentation acts found in task-oriented spoken dialog [Traum, 1993; Traum and Hinkelman, 1992]. It is largely aimed at the novice annotator rather than the computational linguist, and therefore in choosing terminology we have valued intuitiveness over precision. This work came out of an exploration of how to mark structure above the speech act in the Monroe corpus [Stent, 2000 (TN 99-2)]. For more information about the development of this manual, see [Stent, 2000 (INLG)]. This tool is designed for use with ArgumentationTool, a tool for marking argumentation acts in dialog that is available from http://www.cs.rochester.edu/research/cisd/resources/aad/.",ArtificiallIntelligence
49.txt,"The centering framework explains local discourse coherence by relating a speaker's focus of attention and the forms of referring expressions. Although this framework has proven useful in single-speaker discourse, its utility for multi-party discourse has not been shown. It is unclear how to adapt it to handle discourse phenomena such as turn-taking, acknowledgments, first and second person pronouns, and disfluencies. This paper reports our experiments applying three naive models of centering theory for dialog. These results will be used as a baseline for future, more sophisticated models.",ArtificiallIntelligence
109.txt,"When one works with a system that utilizes inheritance hierarchies the following problem often arises. A new object is introduced and it must be integrated into a hierarchy; under which classes in the hierarchy should the new object be positioned? In this paper, I formalize this problem for feature-based default inheritance hierarchies. Since it turns out to be NP-complete, I present an approximation algorithm for it. I show that this algorithm is efficient and look at some of the possible problematic situations for the algorithm. Although more analysis and experimentation are needed, these preliminary results show that the algorithm warrants such efforts.",ArtificiallIntelligence
43.txt,"Existing algorithms for pronoun resolution typically cast the problem into a coreference task, which means they simply identify an antecedent noun phrase for each pronoun. Selection of the antecedent is usually based on a calculation of salience or focus. This simplified approach is unable to account for pronouns without noun-phrase antecedents. Examples are abstract referents such as events, propositions, and speech acts that might appear in the linguistic surface form as sentential complements, verbal constructions or entire sentences, as well as consequences or outcomes that don't appear in the surface form at all. This paper contains a survey of current methods of pronoun resolution for natural language understanding. It then proposes a strategy for resolving pronominal reference to abstract entities that incorporates semantic information in addition to salience calculations. Preliminary experiments are described that show the strategy to perform well above baseline on a collection of spoken task-oriented dialogs.",ArtificiallIntelligence
58.txt,"In this report we describe an experiment designed to: evaluate the performance of the TRAINS-96 system as a whole; examine the utility of a new robust post-parser module, recently added to the TRAINS system; and explore the benefit to the user of receiving system feedback on speech input. The evaluation uses the same task-based methodology as was used for the TRAINS-95 evaluation [Sikorski and Allen 96], in which the user and computer cooperatively solve a given problem. Success is measured in terms of task performance measures such as time to completion of a task, and the quality of the final plan produced.",ArtificiallIntelligence
102.txt,"Speech of multiple speakers is transformed to speech produced by a single speaker (speech normalization) using cross-coding networks. Internal representations for classification are acquired by feeding back the internal speech (i-speech) produced. Training proceeds by unfolding the network through time, and combining the classification error with the intermediate speaker-normalization errors. Experimental results on multi-speaker syllable recognition tasks with trained and new speakers are discussed.",ArtificiallIntelligence
29.txt,"Reinforcement learning is a promising technique for learning agents to adapt their own strategies in multi-agent systems. Most existing reinforcement learning algorithms are designed from a single-agent's perspective and for simplicity assume the environment is stationary, i.e., the distribution of the utility of each state-action pair does not change. However, in a more realistic model of multi-agent systems, the agents are continually adapting their own strategies owing to different utilities at different times. Because of the non-stationarity, multi-agent systems are more sensitive to the trade-off between exploitation, which uses the best strategy so far, and exploration, which tries to find better strategies. Exploration is especially important to these changing circumstances. In this paper, we assume that the utility of each state-action pair is a stochastic process. This allows us to describe the trade-off dilemma as a Brownian bandit problem to formalize Sutton's recency-based exploration bonus in non-stationary environments. To demonstrate the performance of the exploration bonus, we build agents using Q-learning algorithm with a smoothed best response dynamics. The simulations show that the agents can efficiently adapt to changes in their peers' behaviors whereas the same algorithm, using Boltzmann exploration, cannot adapt.",ArtificiallIntelligence
88.txt,"This dissertation describes the formal foundations and implementation of a commonsense, mixed-initiative plan reasoning system. By ``plan reasoning'' I mean the complete range of cognitive tasks that people perform with plans including, for example, plan construction (planning), plan recognition, plan evaluation and comparison, and plan repair (replanning), among other things. ``Mixed-initiative'' means that several participants can each make contributions to the plan under development through some form of communication. ``Commonsense'' means that the system represents plans and their constituents at a level that is ``natural'' to us in the sense that they can be described and discussed in language. In addition, the reasoning that the system performs includes those conclusions that we would take to be sanctioned by common sense, including especially those conclusions that are defeasible given additional knowledge or time spent reasoning.  The main theses of this dissertation are the following: (1) Any representation of plans sufficient for commonsense plan  reasoning must be based on an expressive and natural representation  of such underlying phenomena as time, properties, events, and  actions. (2) For mixed-initiative planning, plans should be viewed as  arguments that a certain course of action under certain conditions  will achieve certain goals. These theses are defended by presenting, first, a representation of events and actions based on interval temporal logic and, second, a representation of plans as arguments in a formal system of defeasible reasoning that explicitly constructs arguments. These two aspects of commonsense plan reasoning are combined and implemented in the TRAINS domain plan reasoner, which is also described in detail.  The emphasis in this dissertation is on breadth, taking as its data human communicative and plan reasoning abilities and developing formalisms that characterize these abilities and systems that approximate them. I therefore draw on literature from a broad range of disciplines in the development of these ideas, including: philosophy of language, linguistics and AI work on knowledge representation for the representation of events and actions, philosophical logic and AI work on nonmonotonic reasoning for representing defeasible knowledge and reasoning about it, and, of course, AI work on planning and plan recognition itself.",ArtificiallIntelligence
67.txt,"We describe the goals, architecture, and functioning of the TRAINS-93 system, with emphasis on the representational issues involved in putting together a complex language processing and reasoning agent. The system is intended as an experimental prototype of an intelligent, conversationally proficient planning advisor in a dynamic domain of cargo trains and factories. For this team effort, our strategy at the outset was to let the designers of the various language processing, discourse processing, plan reasoning, execution and monitoring modules choose whatever representations seemed best suited for their tasks, but with the constraint that all should strive for principled, general approaches.  Disparities between modules were bridged by careful design of the interfaces, based on regular in-depth discussion of issues encountered by the participants. Because of the goal of generality and principled representation, the multiple representations ended up with a good deal in common (for instance, the use of explicit event variables and the ability to refer to complex abstract objects such as plans); and future unifications seem quite possible. We explain some of the goals and particulars of the KRs used, evaluate the extent to which they served their purposes, and point out some of the tensions between representations that needed to be resolved. On the whole, we found that using very expressive representations minimized the tensions, since it is easier to extract what one needs from an elaborate representation retaining all semantic nuances, than to make up for lost information.",ArtificiallIntelligence
111.txt,"Integrated, flexible, surviving, autonomous AI systems are on the horizon, raising new issues in systems support. These systems typically embody hard real-time constraints (for servoing) and soft real-time constraints (solving problems to some level of effectiveness within some time constraints). We assume an adequate hard real-time control substrate, and are concerned here with resource allocation for high-level decision-making in Soft PArallel Real-Time ApplicationS (SPARTAS). Such applications only need to respond to their environment quicker than their environment can dramatically change on them. SPARTAS often generate behavior by running high-level algorithms based on a model of the world and on information from the environment. Designing an executive for SPARTAS is challenging, since in its full generality it calls for dynamic decision-making about resource allocation, scheduling, choice of methods, and handling reflexive or reactive behavior smoothly within a context of planned or intended actions, and a host of other issues not typically encountered either in off-line or hard real-time applications. An important aspect of the environment for a SPARTA is its own state; what resources are being used for what purposes, and which are available. Modern SPARTAS are written on parallel computers, further complicating matters. We are designing Ephor, a run-time envirobnment for parallel machines to alleviate some of the difficulties faced by a SPARTA programmer. In this paper we briefly describe Ephor, show how it allows simpler application code, and demonstrate that Ephor improves problem-solving performance in the presence of varying internal system state by dynamically choosing between different planners.",ArtificiallIntelligence
113.txt,In uncertain reasoning one often needs to combine conflicting pieces of evidence. We show how the need for evidence combination arises in Kyburg's Evidential Probability system and investigate various methods of dealing with it.,ArtificiallIntelligence
63.txt,"When we work with information from multiple sources, the formats of the knowledge bases may not be uniform. It would be desirable to be able to combine a knowledge base of default rules with one containing autoepistemic formulas. Previous works on relating default logic and autoepistemic logic mostly impose some constraints on autoepistemic logic, and thus are not suitable for combining the two logics. We first present a fixed point formulation of autoepistemic logic analogous to that of default logic. Then we introduce a possible world framework with a partition structure, which corresponds to our intuitive notion of accessibility as linking alternate ``possible'' worlds. We show that both default logic and autoepistemic logic can be characterized using this framework, and the constraints imposed on the possible world structures correspond to the requirements in the fixed point formulations. Casting both default logic and autoepistemic logic in a common framework is important for developing a semantics applicable to the two logics, both separately and combined.",ArtificiallIntelligence
6.txt,"Previous work by Talbot and Osborne (2007a) explored the use of randomized storage mechanisms in language modeling. These structures %trade a small amount of error for significant space savings, enabling the use of larger language models on relatively modest hardware.  Going beyond space efficient count storage, here we present the Transition Counter, an extended model for performing space efficient counting over streams of ï¬nite length. Theoretical and initial experimental results show the promise of approximate counting in the context of limited space.",ArtificiallIntelligence
101.txt,"Supervised, neural network, learning algorithms have proven very successful at solving a variety of learning problems. However, they suffer from a common problem of requiring explicit output labels. This requirement makes such algorithms implausible as biological models. In this paper, it is shown that pattern classification can be achieved in a multi-layered, feed-forward neural network, without requiring explicit output labels, by a process of supervised self-organization. The class projection is achieved by optimizing appropriate within-class uniformity and between-class discernibility criteria. The mapping function and the class labels are developed together iteratively using the derived self-organizing back-propagation algorithm. The ability of the self-organizing network to generalize on unseen data is also experimentally evaluated on real data sets, and compares favorably with the traditional labeled supervision with neural networks. However, interesting features emerge out of the proposed self-organizing supervision that are absent in conventional approaches. The further implications of self-organizing supervision with neural networks are also discussed.",ArtificiallIntelligence
39.txt,"We propose a compaction of WordNet senses for natural language understanding (NLU) applications, where only those distinctions that are not predictable from other knowledge sources are retained. Further, we propose that word sense disambiguation programs that use WordNet as their dictionary may be evaluated with respect to this compaction, for a better indicator of performance. WordNet is attractive for studies of word sense disambiguation because of its quite comprehensive lexical coverage. However, for NLU applications, its very fine-grained distinctions among word senses may be superfluous, in that these distinctions often reflect a regular polysemy which is productive across many words. The knowledge that this regular polysemy exploits is knowledge that a natural language understanding system must already have in order to handle other phases of the understanding process, such as reference resolution. Thus a program which is able to disambiguate WordNet senses modulo, e.g. metonymy, will have essentially ""done its job"" as a word sense disambiguator. We conclude with an evaluation of different disambiguators with respect to the compaction.",ArtificiallIntelligence
92.txt,"The usefulness of accurate sequence information is re-evaluated in this paper. A novel idea, called phonetic set hashing, of transforming phone sequences to words is then suggested. Phone sequences are mapped onto the corresponding phone sets, and the latter used as keys for indexing appropriate words. By using data-driven training strategies, the problem of word segmentation has been alleviated. The robustness of phone set hashing towards insertion, deletion, and substitution errors has also been studied. Experiments with subsets of the TIMIT database indicate that phone set hashing is a simple, fast scheme for word pre-selection.",ArtificiallIntelligence
123.txt,"A linguistic form's compositional, timeless meaning can be surrounded or even contradicted by various social, aesthetic, or analogistic companion meanings. This paper addresses a series of problems in the structure of spoken language discourse, including turn-taking and grounding. It views these processes as composed of fine-grained actions, which resemble speech acts both in resulting from a computational mechanism of planning and in having a rich relationship to the specific linguistic features which serve to indicate their presence.  The resulting notion of Conversation Acts is more general than speech act theory, encompassing not only the traditional speech acts but turn-taking, grounding, and higher-level argumentation acts as well. Furthermore, the traditional speech acts in this scheme become fully joint actions, whose successful performance requires full listener participation.  This paper presents a detailed analysis of spoken language dialogue. It shows the role of each class of conversation acts in discourse structure, and discusses how members of each class can be recognized in conversation. Conversation acts, it will be seen, better account for the success of conversation than speech act theory alone.",ArtificiallIntelligence
64.txt,"This document describes the design and implementation of TRAINS-96, a prototype mixed-initiative planning assistant system. The TRAINS-96 system helps a human manager solve routing problems in a simple transportation domain. It interacts with the human using spoken, typed, and graphical input and generates spoken output and graphical map displays. The key to TRAINS-96 is that it treats the interaction with the user as a dialogue in which each participant can do what they do best. The TRAINS-96 system is intended as both a demonstration of the feasibility of realistic mixed-initiative planning and as a platform for future research. This document describes both the design of the system and such features of its use as might be useful for further experimentation. Further references and a comprehensive set of manual pages are also provided.",ArtificiallIntelligence
26.txt,"In order to understand natural language, it is necessary to understand the intentions behind it, (i.e., why an utterance was spoken). We model dialogue as collaboration between agents. Communicative intentions can then be seen as how an agent is trying to affect the collaboration. Most previous work on intention-recognition approaches to dialogue has focused on only a small subset of agent collaboration paradigms (i.e., master-slave), and thus is unable to account for dialogues in other paradigms, such as mixed-initiative collaboration. Previous work has also either modeled dialogues where the agents are only planning or dialogues where agents are only acting. This restricts dialogue-model coverage to only those cases and does not model dialogues where collaboration about acting and planning occurs.  In this paper, we present a collaborative problem-solving model of dialogue. This model is able to account for a much wider array of dialogues than previous models have covered. It covers the spectrum of collaboration paradigms (from master-slave to mixed-initiative) as well as dialogues where interleaved acting and planning are taking place.  We propose, for future research, to complete this model and to build a domain-independent intention-recognition system based on it for use within the TRIPS dialogue system.",ArtificiallIntelligence
68.txt,"In this paper we describe a recent experiment designed to evaluate the performance of the TRAINS-95 system. The evaluation uses a task-based evaluation methodology appropriate for dialogue systems such as TRAINS-95, where a human and a computer interact and collaborate to solve a given problem. In task-based evaluations, techniques are measured in terms of their affect on task performance measures such as how long it takes to develop a solution using the system, and the quality of the final plan produced. The evaluation explores the robustness of the TRAINS-95 system in the presence of word recognition errors, the amount of training required to effectively use the system, and user preferences.",ArtificiallIntelligence
93.txt,"This report compares two formalisms for uncertain inference, combinatorial semantics and Dempster-Shafer belief function theory, on the basis of an example from the domain of medical diagnosis. We review Shafer's example about the imaginary disease ploxoma and show how it would be represented in combinatorial semantics. We conclude that belief function theory has a qualitative advantage because it offers greater flexibility of expression and provides results about more specific classes of patients. Nevertheless, a quantitative comparison reveals that the inferences sanctioned by combinatorial semantics are more reliable than those of belief function theory.",ArtificiallIntelligence
83.txt,"Training a learning machine from examples is accomplished by minimizing a quantitative error measure, the training error defined over a training set. A low error on the training set does not, however, guarantee a low expected error on any future example presented to the learning machine---that is, a low generalization error.  The main goal of the dissertation is to merge theory and practice: to develop theoretically based but experimentally adapted tools that allow an accurate prediction of the generalization error of an arbitrarily arbitrarily complex classifier. This goal is reached through experimental and theoretical studies of the relationship between the training and generalization error for a variety of learning machines. The result is the introduction of a practical and principled method for predicting the generalization error. The power and accuracy of the predictive procedure is illustrated from application to real-life problems. Theoretical inspiration for the model arises from calculations of of the expected difference between the training and generalization error for some simple learning machines. Novel computations of this character are included in the dissertation. Experimental studies yield experience with the performance ability of real-life classifiers, and result in new capacity measures for a set of classifiers.  The dissertation also presents a new classification algorithm, the Soft Margin Classifier algorithm, for learning with errors on the training set. The algorithm is an extension of the Optimal Margin Classifier algorithm, and is consistently found to outperform its predecessor because it absorbs out-lying and erroneous patterns in flexible margins.",ArtificiallIntelligence
71.txt,"Most natural language processing tasks require lexical semantic information such as verbal argument structure and selectional restrictions, corresponding nominal semantic class, verbal aspectual class, synonym and antonym relationships between words, and various verbal semantic features such as causation and manner. This dissertation addresses two primary questions related to such information: how should one represent it and how can one acquire it.  It is argued that, in order to support inferencing, a representation with well-understood semantics should be used. Standard first order logic has well-understood semantics and a multitude of inferencing systems have been implemented for it. However, standard first order logic, although a good starting point, needs to be extended before it can efficiently and concisely support all the lexically-based inferences needed. Using data primarily from the TRAINS dialogues, the following extensions are argued for: modal operators, predicate modification, restricted quantification, and non-standard quantifiers. These representational tools are present in many systems for sentence-level semantics but have not been discussed in the context of lexical semantics.  A number of approaches to automatic acquisition are considered and it is argued that a ``surface cueing'' approach is currently the most promising. Morphological cueing, a type of surface cueing, is introduced. It makes use of fixed correspondences between derivational affixes and lexical semantic information. The semantics of a number of affixes are discussed and data resulting from the application of the method to the Brown corpus is presented.  Finally, even if lexical semantics could be acquired on a large scale, natural language processing systems would continue to encounter unknown words. Derivational morphology can also be used at run-time to help natural language understanding systems deal with unknown words. A system is presented that provides lexical semantic information for such derived unknown words.",ArtificiallIntelligence
106.txt,"We describe two domain-independent temporal reasoning systems called TimeGraph I and II, which can be used in AI-applications as tools for efficiently managing large sets of relations in the Point Algebra, in the Interval Algebra, and metric information such as absolute times and durations. Our representation of time is based on timegraphs, graphs partitioned into a set of chains on which the search is supported by a metagraph data structure. TimeGraph I was originally developed by Taugher, Schubert, and Miller in the context of story comprehension. TimeGraph II provides useful extensions, including efficient algorithms for handing inequations, and relations expressing point-interval exclusion and interval disjointness. These extensions make the system much more expressive in the representation of qualitative information and suitable for a large class of applications.",ArtificiallIntelligence
19.txt,"Human beings have the innate ability to educe meaning from a mass of data by discovering and exploiting regularities in it. Patterns in the world seem to ""jump out"" at us; they seem obvious. In this paper, we present a system to discover laws in richly structured worlds that is inspired by this form of human reasoning. Much previous work in rule discovery has worked with impoverished domains describable as a list of (object, value) pairs. Such representations admit of relatively efficient algorithms, but are too poor to describe interesting features of the real world and of many logical systems.  We survey more recent work in the field of relational data mining that seeks to extend these algorithms to richer domains. Previous approaches to this problem have worked by searching the space of syntactically correct rule-statements for those that satisfy certain criteria. Their search is guided by linguistic and declarative bias; they hypothesize the possible rules in some order and then test each one.  We argue that the space of possible rules is too large to be searched effectively in this manner. We propose an alternative, data-driven search paradigm, in which the search is guided not by relationships between the forms of the hypothesized rules, but by correlations in the data they represent. We argue that such pattern-driven search enables the detection of richer and more powerful hypotheses, including those involving equality and nested quantification.  We present a prototype system that incorporates our ideas, and the results obtained when it is applied to the problem of detecting invariants in arbitrary planning worlds. Finally, we discuss ways of extending the approach to more realistic domains, and of extending the discovery process by enabling it to create new concepts as necessary to better describe the data.",ArtificiallIntelligence
90.txt,"A central difficulty with automatic speech recognition is the temporally inaccurate nature of the speech signal. Despite this, speech has been traditionally modeled as a purely sequential (albeit probabilistic) process. The usefulness of accurate sequence information is re-evaluated in this paper, both at the acoustic and lexical levels for the task of speech recognition. At the acoustic level, speech segments are quantized into discrete vectors, and converted into set representations as opposed to accurate sequences. Recognition of the quantized vector sets dramatically improved performance as contrasted with the corresponding vector sequence representations. At the lexical level, our study suggests that accurate sequence information is, again, not crucial. In fact locally discarding phoneme sequence information may be useful for coping with errors (such as insertion, substitution). Based on the idea of phone set indexing, a lexical access algorithm is developed. Thus, this work questions the traditional approach of modeling speech as a purely sequential process, and suggests that discarding local sequential information may be a good idea. As an alternative to a purely sequential representation, a set representation seems to be a viable option.",ArtificiallIntelligence
13.txt,"This report proposes a generalization of Dynamic Predicate Logic that allows a straightforward treatment of functional anaphora in texts such as ""Most men had a gun, but only a few used it,"" or ""If all of the graduates received a job offer, then all of them accepted their offer."" The approach dynamically assigns (partial) functions as values of variables that are existentially quantified within the scopes of quantifiers like ""all"" and ""most."" The proposed method is also applicable to bridging anaphora and functionally dependent entities in frames, scripts, and generic sentences.",ArtificiallIntelligence
54.txt,"The receptive fields of neurons in the mammalian primary visual cortex are oriented not only in the domain of space, but in most cases, also in the domain of space-time. While the orientation of a receptive field in space determines the selectivity of the neuron to image structures at a particular orientation, a receptive fieldUs orientation in space-time characterizes important additional properties such as velocity and direction selectivity. Previous studies have focused on explaining the spatial receptive field properties of visual neurons by relating them to the statistical structure of static natural images. In this report, we examine the possibility that the distinctive spatiotemporal properties of visual cortical neurons can be understood in terms of a statistically efficient strategy for encoding natural time varying images. We describe an artificial neural network that attempts to accurately reconstruct its spatiotemporal input data while simultaneously reducing the statistical dependencies between its outputs. The network utilizes spatiotemporally summating neurons and learns efficient sparse distributed representations of its spatiotemporal input stream by using recurrent lateral inhibition and a simple threshold nonlinearity for rectification of neural responses. When exposed to natural time varying images, neurons in a simulated network developed localized receptive fields oriented in both space and space-time, similar to the receptive fields of neurons in the primary visual cortex.",ArtificiallIntelligence
118.txt,"The implementation of genetic algorithms raises many important issues. These issues can be divided into two main classes: genetic search quality and execution performance. In the context of parallel genetic algorithms on distributed-memory computers, performance considerations have always driven the design of implementations. Thus, centralized implementations have not previously been seriously considered for distributed-memory architectures.  The work we present here defines a set of genetic algorithm implementation alternatives for distributed-memory computers, in which strategies with some centralization are included. Each of our implementation alternatives uses a different level of distribution of the population, from the single logically centralized population to a totally distributed set of subpopulations.  The design alternatives we define can be applied to the implementation of any parallel genetic algorithm. As an example of such an implementation, we study the quality of the search and the execution performance of our strategies on the 0-1 Integer Linear Programming problem, on a Transputer network. Our results show that implementations incurring higher overheads can produce as good or better solutions faster than than very ""efficient"" implementations, depending on the characteristics of the problem at hand. More specifically, in some cases, utilizing more centralized parallel genetic search strategies results in the fastest convergence towards the optimal solution, therefore reducing the number of generations needed by the algorithm.",ArtificiallIntelligence
20.txt,"This study of the Fall 2002 Computer Programming (CSC 171) course provides a detailed analysis of the relationship between variables such as workshop attendance, gender, ethnicity and prior student ability and student performance. The results, detailed in the subsequent sections below, suggest the following:  * Workshop attendance has a significantly positive impact on student performance even after controlling for variations in gender and prior student ability.  * Due to the small sample size of the female and minority groups, the magnitude of the role gender and ethnicity plays in affecting student performance cannot be conclusively determined based on statistical analyses.  * Withdrawing female students performed significantly below their male counterparts even though they attended more workshops on average, while female students who completed the course did not perform significantly differently from their male counterparts.  * Prior student ability (as measured by SAT scores) is significant in affecting student performance.  * Controlling for prior student ability alters the effect of workshop attendance on performance only slightly. OLS regression results suggest an overestimation, logistic regression results suggest an underestimation of the effect prior to adding SAT scores.",ArtificiallIntelligence
66.txt,"The responses of visual cortical neurons during fixation tasks can be significantly modulated by stimuli from beyond the classical receptive field. Modulatory effects in neural responses have also been recently reported in a task where a monkey freely views a natural scene. In this paper, we describe a hierarchical network model of visual recognition that explains these experimental observations by using a form of the extended Kalman filter as given by the Minimum Description Length (MDL) principle. The model dynamically combines input-driven bottom-up signals with expectation-driven top-down signals to predict current recognition state. Synaptic weights in the model are adapted in a Hebbian manner according to a learning rule also derived from the MDL principle. The resulting prediction/learning scheme can be viewed as implementing a form of the Expectation-Maximization (EM) algorithm. The architecture of the model posits an active computational role for the reciprocal connections between adjoining visual cortical areas in determining neural response properties. In particular, the model demonstrates the possible role of feedback from higher cortical areas in mediating neurophysiological effects due to stimuli from beyond the classical receptive field. Simulations of the model are provided that help explain the experimental observations regarding neural responses in both free viewing and fixating conditions.",ArtificiallIntelligence
40.txt,"Appearance-based object recognition systems are currently the most successful approach for dealing with 3D recognition of arbitrary objects in the presence of clutter and occlusion. However, no current system seems directly scalable to human performance levels in this domain. In this report we describe a series of experiments on a previously described object recognition system that try to see which, if any, design axes of such systems hold the greatest potential for improving performance. We look at the potential effect of different design modifications and we conclude that the greatest leverage lies at the level of intermediate feature construction.",ArtificiallIntelligence
55.txt,"This paper examines the nature of visual representations that direct ongoing performance in sensorimotor tasks. Performance of such natural tasks requires relating visual information from different gaze positions. To explore this we used the technique of making task relevant display changes during saccadic eye movements. Subjects copied a pattern of colored blocks on a computer monitor, using the mouse to drag the blocks across the screen. Eye position was monitored using a dual-purkinje eye tracker, and the color of blocks in the pattern was changed at different points in task performance. When the target of the saccade changed color during the saccade, the duration of fixations on the model pattern increased, depending on the point in the task that the change was made. Thus different fixations on the same visual stimulus served a different purpose. The results also indicated that the visual information that is retained across successive fixations depends on moment by moment task demands. This is consistent with previous suggestions that visual representations are limited and task dependent. Changes in blocks in addition to the saccade target led to greater increases in fixation duration. This indicated that some global aspect of the pattern was retained across different fixations. Fixation durations revealed effects of the display changes that were not revealed in perceptual report. This can be understood by distinguishing between processes that operate at different levels of description and different time scales. Our conscious experience of the world may reflect events over a longer time scale than those underlying the substructure of the perceptuo-motor machinery.",ArtificiallIntelligence
107.txt,"It seems to be a common feeling that animals learn to see, and this feeling, together with the re-emergence of computer learning paradigms that mimic many forms of human learning, has raised hopes that learning is the key to the computer vision problem. Indeed, it seems clear that Nature does not ""program"" all our visual capabilities into the genome, and we certainly know that programming a computer with a closed-form solution to the vision problem is a daunting task.  The aim of this informal and elementary report (basically a term paper) is to cast doubt on the idea that biological systems learn to see. The complex process of development, beginning at fertilization and ending with a mature individual, could be considered to have genetic (""nature"") and learning (""nurture"") processes as logical endpoints or opposite poles. This report mostly considers what goes on between those endpoints, and is meant to raise the possibility that some of the least understood processes in biology are responsible for visual capabilities.",ArtificiallIntelligence
53.txt,"We propose a semantics for belief in which the derivation of new beliefs from old ones is modeled as a computational process. Using this model, we characterize conditions under which it is appropriate to reason about other agents by simulating their inference processes with one's own.",ArtificiallIntelligence
86.txt,"We describe a method of 3-D object recognition based on two stage use of a general purpose associative memory and a principal views representation. The basic idea is to make use of semi-invariant objects called keys. A key is any robustly extractable feature that has sufficient information content to specify a 2-D configuration of an associated object (location, scale, orientation) plus sufficient additional parameters to provide efficient indexing and meaningful verification. The recognition system utilizes an associative memory organized so that access via a key feature evokes associated hypotheses for the identity and configuration of all objects that could have produced it. These hypothesis are fed into a second stage associative memory, which maintains a probabilistic estimate of the likelihood of each hypothesis based on statistics about the occurrence of the keys in the primary database. Because it is based on a merged percept of local features rather than global properties, the method is robust to occlusion and background clutter, and does not require prior segmentation. Entry of objects into the memory is an active, automatic procedure. We have implemented a version of the system that allows arbitrary definitions for key features. Experiments using keys based on perceptual groups of line segments are reported. Good results were obtained on a database derived from of approximately 150 images representing different views of 7 polyhedral objects.",ArtificiallIntelligence
61.txt,"Using results from the field of robust statistics, we derive a class of Kalman filters that are robust to structured and unstructured noise in the input data stream. Each filter from this class maintains robust optimal estimates of the input process's hidden state by allowing the measurement covariance matrix to be a non-linear function of the prediction errors. This endows the filter with the ability to reject outliers in the input stream. Simultaneously, the filter also learns an internal model of input dynamics by adapting its measurement and state transition matrices using two additional Kalman filter-based adaptation rules. We present experimental results demonstrating the efficacy of such filters in mediating appearance-based segmentation and recognition of objects and image sequences in the presence of varying degrees of occlusion, clutter, and noise.",ArtificiallIntelligence
62.txt,"This document describes a new pattern matching engine used as part of the discourse reasoning components in the TRAINS-96 system. Its dominant characteristics are simplicity, efficiency, and an economical model for driving the search engine.",ArtificiallIntelligence
44.txt,In this report we describe a method for extracting curves from an image using directional pixel variances instead of gradient measures as low-level boundary evidence. The advantage of the variance over the image gradient is that we can accurately compute the direction of a local edge even if a sudden contrast change occurs in the background. This allows curves belonging to object contours to be followed more easily. We compared our method to a similar method based on the image gradient and we found that it obtains better results when run on synthetic and natural images. Our method also improved the performance of a contour-based 3D object recognition system in cluttered images.,ArtificiallIntelligence
103.txt,"We address the problems of determining consistency and of finding a solution for sets of 3-point relations expressing exclusion of a point from an interval, and for sets of 4-point relations expressing interval disjointness. Availability of these relations is an important requirement for dealing with the sorts of temporal constraints encountered in many AI applications such as plan reasoning. We prove that consistency testing is NP-complete and finding a solution is NP-hard.",ArtificiallIntelligence
31.txt,"WordNet is a lexical database that, among other things, arranges English nouns into a hierarchy ranked by specificity, providing links between a more general word and words that are specializations of it. For example, the word ""mammal"" is linked (transitively via some intervening words) to ""dog"" and to ""cat."" This hierarchy bears some resemblance to the hierarchies of types (or properties, or predicates) often used in artificial intelligence systems. However, WordNet was not designed for such uses, and is organized in a way that makes it far from ideal for them. This report describes our attempts to arrive at a quantitative measure of the quality of the information that can be extracted from WordNet by interpreting it as a formal taxonomy, and to design automatic techniques for improving the quality by filtering out dubious assertions.",ArtificiallIntelligence
104.txt,"Reasoning about temporal information is an important task in many areas of Artificial Intelligence. In this paper we address the problem of scalability in temporal reasoning by providing a collection of new algorithms for efficiently managing large sets of qualitative temporal relations. We focus on the class of relations forming the Point Algebra (PA-relations) and on a major extension to include binary disjunctions of PA-relations (PA-disjunctions). Such disjunctions add a great deal of expressive power, including the ability to stipulate disjointness of temporal intervals, which is important in planning applications.  Our representation of time is based on timegraphs, graphs partitioned into a set of chains on which the search is supported by a metagraph data structure. The approach is an extension of the time representation proposed by Schubert, Taugher and Miller in the context of story comprehension. The algorithms herein enable construction of a timegraph from a given set of PA-relations, querying a timegraph, and efficiently checking the consistency of a timegraph augmented by a set of PA-disjunctions. Experimental results illustrate the efficiency of the proposed approach.",ArtificiallIntelligence
124.txt,Problems for strict and convex Bayesianism are discussed. A set-based Bayesianism generalizing convex Bayesianism and intervalism is proposed. This approach abandons not only the strict Bayesian requirement of a unique real-valued probability function in any decision-making context but also the requirement of convexity for a set-based representation of uncertainty. Levi's E-admissibility decision criterion is retained and is shown to be applicable in the non-convex case.,ArtificiallIntelligence
14.txt,"Self-awareness is an aspect of consciousness that is highly developed in humans in comparison with other animals. A human being unaware of his or her personal characteristics, of what he or she knows and doesn't know, can do and cannot do, wants and doesn't want, has experienced and is experiencing, etc., would surely be difficult to communicate with naturally. Therefore we believe that consciousness plays a crucial role in building artificial dialog agents with human-level abilities.  We will provide an overview of consciousness as viewed in philosophy, biology and artificial intelligence, and we will present relevant work on agents that show abilities related to consciousness. Moreover we will present our initial attempt to extend the architecture of a simple EPILOG-based agent originally built by A.N. Kaplan in the direction of our goal of a human-like conscious dialog agent.",ArtificiallIntelligence
45.txt,"Domain-independent planning is a notoriously hard search problem. Several systematic search techniques have been proposed in the context of various formalisms. However, despite their theoretical completeness, in practice these algorithms are incomplete because for many problems the search space is too large to be (even partially) explored, and a plan cannot be found in reasonable time (if one exists). In this paper we propose a new search method in the context of Blum and Furst's planning graph approach, which is based on local search. Local search techniques are incomplete, but in practice they can efficiently solve problems that are unsolvable for current systematic search methods. We introduce three particular heuristics to guide the local search (Walkplan, Tabuplan and T-Walkplan), and we propose two methods for combining local and systematic search. Our techniques are implemented in a system called GPG, which can be used for both plan-generation and plan-adaptation tasks. Experimental results show that GPG can efficiently solve problems that are very hard for the systematic search of IPP and Graphplan, including Kautz and Selman's Logistics-d.",ArtificiallIntelligence
23.txt,"This paper describes a method for computing the domain of quantification of an adverbially quantified sentence. This method relies on the accommodation of presuppositions in the scope of a quantificational adverb and on the resolution of the domain in context. Situations form the link between adverbial quantifiers and presuppositions, as adverbial quantifiers are taken to quantify over situations and presuppositions are taken to be constraints on resource situations. This paper also briefly describes a computational system for processing such sentences based on this method.",ArtificiallIntelligence
21.txt,"This thesis describes an implemented technique for resolving anaphoric pronouns referring to both individual and abstract entities. The model defines rules for evoking high-order entities from discourse and also a pronoun resolution method that is appropriate for both demonstrative and personal pronouns. It correctly interprets 72% of the pronouns, while a previous leading technique is correct on only 37%, when tested on a corpus of task-oriented spoken dialog.",ArtificiallIntelligence
59.txt,"Visual cognition depends critically on the moment-to-moment orientation of gaze. Gaze is changed by saccades, rapid eye movements that orient the fovea over targets of interest in a visual scene. Saccades are ballistic; a prespecified target location is computed prior to the movement and visual feedback is precluded. Once a target is fixated, gaze is typically held for about 300 milliseconds, although it can be held for both longer and shorter intervals. Despite these distinctive properties, there has been no specific computational model of the gaze targeting strategy employed by the human visual system during visual cognitive tasks. This paper proposes such a model that uses iconic scene representations derived from oriented spatiochromatic filters at multiple scales. Visual search for a target object proceeds in a coarse-to-fine fashion with the target's largest scale filter responses being compared first. Task-relevant target locations are represented as saliency maps which are used to program eye movements. Once fixated, targets are remembered by using spatial memory in the form of object-centered maps. The model was empirically tested by comparing its performance with actual eye movement data from human subjects in natural visual search tasks. Experimental results indicate excellent agreement between eye movements predicted by the model and those recorded from human subjects.",ArtificiallIntelligence
78.txt,"The TRAINS world is a transportation domain in which commodoties are moved from site to site by various forms of transportation. It includes factories, warehouses, trains, planes, ships and trucks and the agents that operate these facilities. The TRAINS world simulator is a general purpose graphical simulator that includes modules for simulating the TRAINS world. The simulator allows extensions to the TRAINS world by providing a language for describing causal models.",ArtificiallIntelligence
114.txt,"This report contains a small corpus of transcriptions of task oriented spoken conversations in the TRAINS domain. Included are 16 conversations, amounting to over 80 minutes of speech. Also included are a description of the task and collection situation and the conventions used in transcription and utterance segmentation.",ArtificiallIntelligence
121.txt,"We propose a wide-ranging knowledge representation formalism designed expressly to support many different forms of reasoning about plans. We begin with an event-based language based on the interval temporal logic. The language supports reasoning about action attempts and composite actions, both of which are given axiomatic definitions. We then define a representation for plans viewed as arguments that a certain course of action under certain explicit conditions will achieve certain goals. We can represent both correct and incorrect plans, and reason about why they might or might not fail. An important aspect of this work is the formal characterization of plan reasoning as assumption-based reasoning, to make the non-deductive aspects of plan reasoning explicit. A preliminary implementation of these ideas has already been built as the plan reasoning component of the TRAINS system.",ArtificiallIntelligence
42.txt,"There are four major dialog-specific challenges in processing natural language: 1) determining an utteranceUs speech act, 2) finding utterance boundaries, 3) allowing for the possibility that speakers may continue each other's utterances and interrupt each other, and 4) handling speech repairs and editing terms (uh, I mean). We worked with the Multiparty Discourse Group to develop the Backward- and Forward-Looking annotation scheme that unlike many current speech act taxonomies allows utterance multi-functionality to be captured. To help with challenge 2, we use a statistical utterance boundary detector. To handle challenges 3 and 4, we developed a unique parsing framework in which metarules specify allowable forms of phrase breakage and interleaving. A stream of words tagged with their speakers are given to the parser. Second speaker continuations are naturally allowed and metarules allow phrase structure to be formed around second speaker interruptions. Similarly, metarules allow phrase structure to be formed around speech repairs and editing terms. The parser can thus include repairs and editing terms in its output, allowing higher-level reasoning processes to make inferences about hesitations and false starts in the input. We have also shown that the parser can use its knowledge of grammar and the syntactic structure of the input to improve pre-parser speech repair identification.",ArtificiallIntelligence
105.txt,"We analyze the problem of computing the minimal labels for a network of temporal relations in the Point Algebra. van Beek proposes an algorithm for accomplishing this task which takes $O(max(n^3,n^2\cdot m))$ time (for $n$ points and $m$ $\neq$-relations). We show that the proof of the correctness of this algorithm given by van Beek and Cohen is faulty, and we provide a new proof showing that the algorithm is indeed correct.",ArtificiallIntelligence
52.txt,"Interactive spoken dialog provides many new challenges for natural language understanding systems. One of the most critical challenges is simply determining the speaker's intended utterances: both segmenting a speaker's turn into utterances and determining the intended words in each utterance. Even assuming perfect word recognition, the latter problem is complicated by the occurrence of speech repairs, which occur when the speaker goes back and changes (or repeats) something she just said. The words that are replaced or repeated are no longer part of the intended utterance, and so need to be identified. The two problems of segmenting the turn into utterances and resolving speech repairs are strongly intertwined with a third problem: identifying discourse markers. Lexical items that can function as discourse markers, such as ""well"" and ""okay,"" are ambiguous as to whether they are introducing an utterance unit, signaling a speech repair, or are simply part of the context of an utterance, as in ""that's okay."" Spoken dialog systems need to address these three issues together and early on in the processing stream. In fact, just as these three issues are closely intertwined with each other, they are also intertwined with identifying the syntactic role or part-of-speech (POS) of each word and the speech recognition problem of predicting the next word given the previous words.  In this thesis, we present a statistical language model for resolving these issues. Rather than finding the best word interpretation for an acoustic signal, we redefine the speech recognition problem so that it also identifies the POS tags, discourse markers, speech repairs and intonational phrase endings (a major cue in determining utterance units). Adding these extra elements to the speech recognition problem actually allows it to better predict the words involved, since we are able to make use of the predictions of boundary tones, discourse markers and speech repairs to better account for what word will occur next. Furthermore, we can take advantage of acoustic information, such as silence information, which tends to co-occur with speech repairs and intonational phrase endings, that current language models can only regard as noise in the acoustic signal. The output of this language model is a much fuller account of the speaker's turn, with part-of-speech assigned to each word, intonation phrase endings and discourse markers identified, and speech repairs detected and corrected. In fact, the identification of the intonational phrase endings, discourse markers, and resolution of the speech repairs allows the speech recognizer to model the speaker's utterances, rather than simply the words involved, and thus it can return a more meaningful analysis of the speaker's turn for later processing.",ArtificiallIntelligence
95.txt,"The TRAINS project is an effort to build a conversationally proficient planning assistant. A key part of the project is the construction of the TRAINS system, which provides the research platform for a wide range of issues in natural language understanding, mixed-initiative planning systems, and representing and reasoning about time, actions and events. Four years have now passed since the beginning of the project. Each year we have produced a demonstration system that focused on a dialog that illustrates particular aspects of our research. The commitment to building complete integrated systems is a significant overhead on the research, but we feel it is essential to guarantee that the results constitute real progress in the field. This paper describes the goals of the project, and our experience with the effort so far.  This paper is to appear in the Journal of Experimental and Theoretical AI, 1995.",ArtificiallIntelligence
79.txt,"This report is a user's manual for the TRAINS-95 parsing system. An accompanying report describes the grammar used in TRAINS-95, and the robust speech act interpretation system, which takes the chart and produces a series of speech acts that best characterize it. The parser is based on the bottom-up parser described in Natural Language Understanding, Second Ed. (Allen, 1994, Chapters 3, 4, and 5). It uses the same formats for the grammar and the lexical entries, and the same basic bottom-up algorithm. There are a number of extensions beyond the basic system described in the book, each of which will be discussed in this report, including: (1) support for parsing word lattices; (2) best-first parsing using context-free probabilistic rules; (3) incremental (word by word) parser with backup for corrections; (4) hierarchical feature values and extended unification options; (5) a hierarchical lexicon entry format that simplifies defining large lexicons; and (6) procedural attachment to chart actions.",ArtificiallIntelligence
18.txt,"This paper presents an integrated approach to build an affect lexicon for emotion tagging of free text. The primary linguistic resource for this lexicon includes electronic dictionaries, on-line word association norms and a large scale commonsense corpus. Our main goal is to automatically collect frequently used affect words and phrases and also assess their emotion intensity. Multiple natural language processing techniques, like POS tagging, parsing, phrase chunking, constituent identification, are employed. We show examples of affect assessment by using this lexicon as well as syntactic processing. Our system gives out plausible emotion analysis for test text. Potential applications includes building emotional virtual agents, estimating user's attitude, and assisting information retrieval.",ArtificiallIntelligence
27.txt,"We describe a parser that draws from both extant corpora and linguistic knowledge sources, and thus is suitable as a front end for applications requiring both broad coverage and rich syntactic analysis. We detail many of the difficulties and assumptions involved in combining these data and knowledge sources. We also describe the novel language model that we use for disambiguation and show that it outperforms a comparable model without the same knowledge sources.",ArtificiallIntelligence
65.txt,"A characteristic feature of the mammalian visual cortex is the reciprocity of connections between cortical areas. While corticocortical feedforward connections have been well studied, the computational function of the corresponding feedback projections has remained relatively unclear. We have modelled the visual cortex as a hierarchical predictor wherein feedback projections carry predictions for lower areas and feedforward projections carry the difference between the predictions and the actual internal state. The activities of model neurons and their synaptic strength are continually adapted using a hierarchical Kalman filter that minimizes errors in prediction. The model generalizes several previously proposed encoding schemes and allows functional interpretations of a number of well-known psychophysical and neurophysiological phenomena. Here, we present simulation results suggesting that the classical phenomenon of endstopping in cortical neurons may be viewed as an emergent property of the cortex implementing a hierarchical Kalman filter-like prediction mechanism for efficient encoding and recognition.",ArtificiallIntelligence
22.txt,"DISCOPLAN is an implemented set of efficient preplanning algorithms intended to enable faster domain-independent planning. It includes algorithms that use a hypothesize-and-test paradigm to discover and inductively verify state constraints (invariants) implicit in the structure of a given set of planning operators and initial state. Such state constraints have been shown to be very useful, for example, for speeding up SAT-based planning, regression planning, and heuristic decomposition of planning problems. DISCOPLAN handles operators with conditional effects, and efficiently discovers constraints of the following types: (1) type constraints; (2) predicate domain constraints; (3) simple implicative constraints involving up to two fluent literals and any number of static literals, where one of the fluent literals contains all of the variables occurring in the other literals; (4) single-valuedness (sv-) and n-valuedness constraints; (5) implicative and sv-constraints, relaxing the restrictions on variable subsumption and requiring simultaneous induction; (6) antisymmetry constraints; (7) XOR-constraints; and (8) some additional constraints obtainable by an iterative version of the hypothesize-and-test paradigm. The methods for (6) and (8) involve ""expanding"" operators so as to include preconditions and effects implied by constraints discovered earlier. We also provide provably correct (and provisionally implemented) methods for discovering additional types of constraints, including constraints involving arbitrarily many fluent literals.",ArtificiallIntelligence
116.txt,"This report presents a method by which a reinforcement learning agent can solve the incomplete perception problem using memory. The agent uses a Hidden Markov Model (HMM) to represent its internal state space and creates memory capacity by splitting states of the HMM. The key idea is a test to determine when and how a state should be split: the agent only splits a state when the split will help the agent predict utility. Thus the agent can build an internal state space proportionate to the task at hand, not as large as would be required to represent all of its perceivable world. I call the technique UDM, for Utile Distinction Memory.",ArtificiallIntelligence
97.txt,"This document describes a toolkit and guidelines for the transcription of dialogues. The premise of these tools is that a dialogue between two people can be broken down into a series of utterance files, each spoken by one participant. This allows the transcription tools and standards already designed for single speaker speech to be used.",ArtificiallIntelligence
35.txt,"Reasoning about semantic classes and determining compatibility of the words in a given context is an important procedure used in many modules of natural language understanding systems. However, most existing systems do not devote much attention to their ontological knowledge representations, resulting in implementations that are not portable to other domains. At the same time, statistical methods are more robust and less labor-intensive to develop, but typically result in models that are not easily interpretable by humans. We propose a semantic feature representation for use in practical dialogue systems and argue that it can offer advantages in terms of lexicon development and portability---in particular for defining selectional restrictions---and can also be useful for other system modules that do logical inference. We then propose to develop statistical methods allowing us to learn parts of our representation from corpus data.",ArtificiallIntelligence
17.txt,"In this paper, we present the results for semantic labeling, extending the work of [Gildea and Jurafsky, 2002], [Fleischman et al., 2003], [Pradhan et al., 2004], and others. The main labeling approach is based on Maximum Entroopy. We show the performance of the baseline system as well as those by applying coreference resolution, stemming and feature combinations to the feature files.",ArtificiallIntelligence
12.txt,"Factoring a Synchronous Context-Free Grammar into an equivalent grammar with a smaller number of nonterminals in each rule enables more efficient strategies for synchronous parsing. We present an algorithm for factoring an n-ary SCFG into a k-ary grammar in time O(kn). We also show how to efficiently compute the exact number of k-ary parsable permutations of length n, and discuss asymptotic behavior as n grows. The number of length n permutations that are k-ary parsable approaches a fixed ratio between successive terms as n grows for fixed k. As k grows, the difference between successive ratios approaches 1/e.",ArtificiallIntelligence
10.txt,"Recent systems for semantic role labeling are very dependent on the specific predicates and corpora on which they are trained, but labeling new data is expensive. We study which features and classifiers are best able to generalize to unseen predicates from new semantic frames. We find that automatically derived cluster information is especially helpful in this setting, and that a relatively simple a posteriori classifier outperforms Maximum Entropy.",ArtificiallIntelligence
91.txt,"The process of adding to the common ground between conversational participants (called grounding) has previously been either oversimplified or studied in an off-line manner. This dissertation presents a computational theory, in which a protocol is presented which can be used to determine, for any given state of the conversation, whether material has been grounded or what it would take to ground the material. This protocol is related to the mental states of participating agents, showing the motivations for performing particular grounding acts and what their effects will be.  We extend speech act theory to account for levels of action both above and below the sentence level, including the level of grounding acts described above. Traditional illocutionary acts are now seen to be multi-agent acts which must be grounded to have their usual effects.  A conversational agent model is provided, showing how grounding fits in naturally with the other functions that an agent must perform in engaging in conversation. These ideas are implemented within the TRAINS conversation system.  Also presented is a situation-theoretic model of plan execution relations, giving definitions of what it means for an action to begin, continue, complete, or repair the execution of a plan. This framework is then used to provide precise definitions of the grounding acts in terms of agents executing a general communication plan in which one agent must present the content and another acknowledge it.",ArtificiallIntelligence
94.txt,"This dissertation addresses the problem of unsupervised learning for pattern classification or category learning. A model that is based on gross cortical anatomy and implements biologically plausible computations is developed and shown to have classification power approaching that of a supervised discriminant algorithm.  The advantage of supervised learning is that the final error metric is available during training. Unfortunately, when modeling human category learning, or in constructing classifiers for autonomous robots, one must deal with not having an omniscient entity labeling all incoming sensory patterns. We show that we can substitute for the labels by making use of structure between the pattern distributions to different sensory modalities. For example the co-occurrence of a visual image of a cow with a ``moo'' sound can be used to simultaneously develop appropriate visual features for distinguishing the cow image and appropriate auditory features for recognizing the moo.  We model human category learning as a process of minimizing the disagreement between outputs of sensory modalities processing temporally coincident patterns. We relate this mathematically to the optimal goal of minimizing the number of misclassifications in each modality and apply the idea to derive an algorithm for piecewise linear classifiers in which each network uses the output of the other networks as a supervisory signal.  Using the Peterson-Barney vowel dataset we show that the algorithm finds appropriate placement for the classification boundaries. The algorithm is then demonstrated on the task of learning to recognize acoustic and visual speech from images of lips and their emanating sounds Performance on these tasks is within 1-7\% of the related supervised algorithm (LVQ2.1).  Finally we compare the algorithm to Becker's IMAX algorithm and give suggestions as to how the algorithm may be implemented in the brain using physiological results concerning the relationship between two types of neural plasticity, LTP and LTD, observed in visual cortical cells. We also show how the algorithm can be used as an efficient method for dealing with learning from data with missing values.",ArtificiallIntelligence
16.txt,"Planning invariants are formulae that are true in every reachable state of a planning world. We describe a novel approach to the problem of discovering such invariants in propositional form---by analyzing only a set of reachable states of the planning domain, and not its operators. Our system works by exploiting perceived patterns of propositional covariance across the set of states: It hypothesizes that strongly-defined patterns represent features of the planning world.  We demonstrate that, in practice, our system overwhelmingly produces correct invariants. Moreover, we compare it with a well-known system from the literature that uses complete operator descriptions, and show that it discovers a comparable number of invariants, and moreover, does so hundreds or thousands of times faster.  We also show how an existing operator-based invariant finder can be used to verify the correctness of the invariants we find, should operator information be available. We show that such hybrid systems can efficiently produce verifiably true invariants.",ArtificiallIntelligence
28.txt,"The clustering problem has been widely studied since it arises in many application domains in engineering, business and social science. It aims at identifying the distribution of patterns and intrinsic correlations in large data sets by partitioning the data points into similarity clusters. Traditional clustering algorithms use distance functions to measure similarity and are not suitable for high dimensional spaces. In this paper, we propose a non-distance based clustering algorithm for high dimensional spaces. Based on the maximum likelihood principle, the algorithm is to optimize parameters to maximize the likelihood between data points and the model generated by the parameters. Experimental results on both synthetic data sets and a real data set show the efficiency and effectiveness of the algorithm.",ArtificiallIntelligence
99.txt,"We present a representation of events and action based on interval temporal logic that is significantly more expressive and more natural than most previous AI approaches. The representation is motivated by work in natural language semantics and discourse, temporal logic, and AI planning and plan recognition. The formal basis of the representation is presented in detail, from the axiomatization of time periods to the relationship between actions and events and their effects. The power of the representation is illustrated by applying it to the axiomatization and solution of several standard problems from the AI literature on action and change. An approach to the frame problem based on explanation closure is shown to be both powerful and natural when combined with our representational framework. We also discuss features of the logic that are beyond the scope of many traditional representations, and describe our approach to difficult problems such as external events and simultaneous actions.",ArtificiallIntelligence
57.txt,"One serious problem of standard Genetic Programming (GP) is that evolved expressions appear to drift towards large and slow forms on average. This report presents a novel analysis of the role played by variable complexity in the selection and survival of GP expressions. It defines a particular property of GP representations, called rooted tree-schema, that sheds light on the role of variable complexity of evolved representations. A tree-schema is a relation on the space of tree-shaped structures which provides a quantifiable partitioning of the search space. The present analysis answers questions such as: What role does variable complexity play in the selection and survival of evolved expressions? What is the influence of a parsimony penalty? How heavy should parsimony penalty be weighted or how should it be adapted in order to preserve the underlying optimization process? Are there alternative approaches to simulating a parsimony penalty that do not result in a change of the fitness landscape? The present report provides theoretical answers to these questions, interpretation of these results, and an experimental perspective.",ArtificiallIntelligence
127.txt,"Rhetorical (Rhet) is a programming / knowledge representation system that offers a set of tools for building automated reasoning systems. Its emphasis is on flexibility of representation, allowing the user to decide if the system will basically operate as a theorem prover, a frame-like system, or an associative network. Rhet may be used as the back-end to a user's programming system and handle the knowledge represen- tation chores, or it may be used as a full-blown programming language.  Rhet offers two major modes of inference: a horn clause theorem prover (backwards chaining mechanism) and a forward chaining mechanism. Both modes use a common representation of facts, namely horn clauses with universally quantified, potentially type restricted, variables, and use the unification algorithm. Additionally, they both share the following additional specialized reasoning capabilities: (1) variables may be typed with a fairly general type theory that allows a limited calculus of types including intersection and subtraction; (2) full reasoning about equality between ground terms; (3) reasoning within a context space, with access to axioms and terms in parent contexts; (4) escapes into Lisp for use as necessary.",ArtificiallIntelligence
50.txt,"A Personalized System of Instruction (PSI) is a student-paced method of teaching in which students progress by displaying mastery of written material. Cooperative Learning is a method of instruction in which students work in groups to help each other study. In the Fall of 1996, a computer literacy course in which half of the students followed a PSI curriculum and the other half followed a Cooperative Learning curriculum was offered. Data from this experiment showed several statistically significant differences between the two curricula in student satisfaction as measured by end-of-the-semester course evaluation forms. These questionnaires indicated that students felt that the PSI classes increased their knowledge at the 99\% confidence level. They also indicated that students felt that the PSI course procedures better supported course objectives, that the PSI course required more work, and that it was easier to get answers from the TAs in the PSI classes at the 95\% confidence level. The data also showed statistically significant evidence that students learned more from the PSI curriculum as measured by exams. Analysis of rosters from the programming class offered the following semester showed no statistically significant difference between the proportion of the PSI students who took the programming class and the proportion of the cooperative learning students who took the programming class.",ArtificiallIntelligence
73.txt,"We show that by inferring parameter domains of planning operators, given the definitions of the operators and the initial and goal conditions, we can often speed up the planning process by an order of magnitude or more. We infer parameter domains by a polynomial-time algorithm that uses forward propagation of sets of constants occurring in the initial conditions and in operator postconditions. During planning, parameter domains can be used to prune operator instances whose parameter domains are inconsistent with binding constraints, and to eliminate spurious ""clobbering threats"" that cannot, in fact, be realized without violating domain constraints. We illustrate these applications with examples from the UCPOP test suite and from the Rochester TRAINS transportation planning domain.",ArtificiallIntelligence
117.txt,"There is a fundamental division between two approaches to cognition and inference in the real world. These approaches may be found in relatively pure form among ""probabilists"" and ""logicists"" in artificial intelligence. Given evidence and background knowledge, the justifiable inference, on the first view, is that the probability of the conclusion is p (or that its degree of certainty is c, etc.). Given evidence and background knowledge, the inference to the conclusion, on the second view, is justified just in case it conforms to an acceptable (often non-monotonic) principle of inference.  This is such a fundamental difference that it may well be that abstract arguments are not really going to prove much. It is surely the case that both approaches should be explored and tested. This article will explore the dimensions of a research program based on a particular version of the second approach: that in which the conclusion of an inference from data and background knowledge is justified if that data and background knowledge renders the conclusion probable enough. The corresponding conclusion on the first view would be: the conclusion is highly probable. But our conclusion is categorical; it is not qualified or hedged; it is accepted. Nevertheless, it is accepted defeasibly: more data could lead us to withdraw it.  Questions of various sorts arise: What are the relations between this sort of defeasible reasoning and ordinary deductive reasoning? What is the source of the ""data and background knowledge"" on the basis of which we derive conclusions? Where do the probabilities come from? How do we use this structure for making decisions? How do we choose a level of practical certainty? How does this structure relate to other non-monotonic formalisms? How is it related to probabilistic structures? Both traditional probabilists and non-monotonic reasoners take evidence as ""given,"" and modify beliefs in its light. But we must also consider the reliability of evidence: we evaluate our evidence in the light of what we believe. Is there a vicious circularity here?  What I seek to explore is the question of empirical argument and inference as it concerns us in the world. I don't expect to provide definitive answers to these questions (and I would not presume to think that they are the only questions that can be raised!), but I hope to be able to provide provocative indications of the form that answers might take.",ArtificiallIntelligence
3.txt,"Training a statistical machine translation system starts with tokenizing a parallel corpus. Some languages such as Chinese do not incorporate spacing in their writing system, which creates a challenge for tokenization. Morphologically rich languages such as Korean and Hungarian present an even bigger challenge, since optimal token boundaries for machine translation in these languages are often unclear. Both rule-based solutions and statistical solutions are currently used. In this paper, we present unsupervised methods to solve tokenization problem. Our methods incorporate information available from parallel corpus to determine a good tokenization for machine translation.",ArtificiallIntelligence
8.txt,"The ability to plan is a essential for any agent, artificial or %not, wishing to claim intelligence in both thought and behavior. Not only should a planning agent persist in pursuing a goal as long as the situation justifies the agent's perseverance, but an intelligent planning agent must additionally be proficient with responding to failures, opportunities, and threats in the environment. This distinction leads naturally to a discussion of externally motivated and internally motivated planning systems.  We first survey externally motivated planners, which exist and work only to accomplish user-given goals. These planners can be further classified as either non-hierarchical or hierarchical, depending on whether a high level plan is first developed and then successively elaborated. We then review internally motivated planners, which are endowed with self-awareness and such mental attitudes as beliefs, desires, and intentions. Finally, we present a preliminary proposal of a self-aware, opportunistic planning agent that maximizes its own cumulative utility while achieving user-specified goals.",ArtificiallIntelligence
37.txt,"The focus of this thesis is to improve the ability of a computational system to understand spoken utterances in a dialogue with a human. Available computational methods for word recognition do not perform as well on spontaneous speech in task-oriented dialogue as we would hope. Even a state of the art recognizer achieves slightly worse than 70\% word accuracy on spontaneous speech in a conversation focused on solving a specific problem.  To address this problem, I explore novel methods for post-processing the output of a speech recognizer in order to correct errors. I adopt statistical techniques for modeling the noisy channel from the speaker to the listener in order to correct some of the errors introduced there. The statistical model accounts for frequent errors such as simple word/word confusions and short phrasal problems (one-to-many word substitutions and many-to-one word concatenations). To use the model, a search algorithm is employed to find the most likely correction of a given word sequence from the speech recognizer. The post-processor output contains fewer erors, thus making interpretation by downstream components, such as parsing, more reliable.  The post-processor was employed in the TRAINS-95 and TRAINS-96 conversational planning assistants to great avail. Using these techniques, we were able to reduce the number of word recognition errors in some scenarios by approximately 17\% (absolute) in the TRAINS-95 and TRAINS-96 systems (from just under 40\% to nearly 20\%). Consequently, both systems were significantly more robust to recognition errors when using the post-processor than when not. In the scenario where the speech recognizer is tunable with the availability of new data, the impact of these techniques is not as large, but they do make an improvement nontheless.",ArtificiallIntelligence
119.txt,"This paper presents a computational model of how conversational participants collaborate in order to make a referring action successful. The model is based on the view of language as goal-directed behavior. We propose that the content of a referring expression can be accounted for by the planning paradigm. Not only does this approach allow the processes of building referring expressions and identifying their referents to be captured by plan construction and plan inference, it also allows us to account for how participants clarify a referring expression by using meta-actions that reason about and manipulate the plan derivation that corresponds to the referring expression. To account for how clarification goals arise and how inferred clarification plans affect the agent, we propose that the agents are in a certain state of mind, and that this state includes an intention to achieve the goal of referring and a plan that the agents are currently considering. It is this mental state that sanctions the adoption of goals and the acceptance of inferred plans, and so acts as a link between understanding and generation.",ArtificiallIntelligence
125.txt,"The RHET system is a knowledge representation tool that is intended to support the development of advanced prototype natural language under- standing and planning systems. It is what is currently called a ""hybrid"" representation, which consists of a set of separately defined specialized reasoning systems that are presented to the user within a single uniform framework. It can be used as a horn-clause based theorem proving system, or it can be used as a rich frame-based representation, or used in any way falling between these styles of use. The primary specialized reasoning components include a type hierarchy system, an equality reasoning system, a temporal reasoning system, and a hierarchical context mechanism that support reasoning about different agent's beliefs as well as hypothetical reasoning. This report provides a sequence of tutorials each demonstrating a major feature of the system.",ArtificiallIntelligence
15.txt,"This thesis describes research which attempts to remove some of the barriers to creating true conversational agents---autonomous agents which can communicate with humans in natural language. First, in order to help bridge the gap between research in the natural language and agents communities, we define a model of agent-agent collaborative problem solving which formalizes agent communication at the granularity of human communication. We then augment the model to define an agent-based model of dialogue, which is able to describe a much wider range of dialogue phenomena than plan-based models. The model also defines a declarative representation of communicative intentions for individual utterances.  Recognition of these intentions from utterances will require an augmentation of already intractable plan and intention recognition algorithms. The second half of the thesis describes research in applying statistical corpus-based methods to goal recognition, a special case of plan recognition.  Because of the paucity of data in the plan recognition community, we have generated two corpora in distinct domains. We also define an algorithm which can stochastically generate artificial corpora to be used in learning. We then describe and evaluate fast statistical algorithms for both flat and hierarchical recognition of goal schemas and their parameter values. The recognition algorithms are more scalable than previous work and are able to recognize goal parameter values as well as schemas.",ArtificiallIntelligence
89.txt,"To describe phenomena that occur at different time scales, computational models of the brain necessarily must incorporate different levels of abstraction. We argue that at time scales of approximately one-third of a second, orienting movements of the body play a crucial role in cognition and form a useful computational level, termed the embodiment level. At this level, the constraints of the body determine the nature of cognitive operations, since the natural sequentiality of body movements can be matched to the natural computational economies of sequential decision systems. The way this is done is through a system of implicit reference termed deictic, whereby pointing movements are used to bind objects in the world to cognitive programs. We show how deictic bindings enable the solution of natural tasks and argue that one of the central features of cognition, working memory, can be related to moment-by-moment dispositions of body features such as eye movements and hand movements.",ArtificiallIntelligence
69.txt,"We describe a general framework for modeling transformations in the image plane using a stochastic generative model. Algorithms that resemble the well-known Kalman filter are derived from the MDL principle for estimating both the generative weights and the current transformation state. The generative model is assumed to be implemented in cortical feedback pathways while the feedforward pathways implement an approximate inverse model to facilitate the estimation of current state. Using the above framework, we derive models for invariant recognition, motion estimation, and stereopsis, and present preliminary simulation results demonstrating recognition of objects in the presence of translations, rotations and scale changes.",ArtificiallIntelligence
60.txt,"In many domains, the task can be decomposed into a set of independent sub-goals. Often, such tasks are too complex to be learned using standard techniques such as Reinforcement Learning. The complexity is caused by the learning system having to keep track of the status of all sub-goals concurrently. Thus, if the solution to one sub-goal is known when another sub-goal is in some given state, the known solution must be relearned when the status of the other sub-goal changes.  This dissertation presents a modular approach to reinforcement learning that takes advantage of task decomposition to avoid unnecessary relearning. In the modular approach, modules are created to learn each sub-goal. Each module receives only those inputs relevant to its associated sub-goal, and can therefore learn without being affected by the state of other sub-goals. Furthermore, each module searches a much smaller space than that defined by all inputs considered together, thereby greatly reducing learning time. Since each module learns how to achieve a separate sub-goal, at any given time it may recommend an action different from that recommended by other modules. To select an action that best satisfies as many of the modules as possible, a simple arbitration strategy is used. One such strategy, explored in this dissertation, is called {\em greatest mass\/} which simply combines action utilities from all modules and selects the one with the largest combined utility.  Since the modular approach limits and separates information given to the modules, the solution learned must necessarily differ from that learned by a standard, non-modular approach. However, experiments in a simple driving world indicate that while sub-optimal, the solution learned by the modular system only makes minor errors when compared with that learned by the standard approach. A complex task can thus be learned very quickly, using only small amounts of computational resources, with only small sacrifices in solution quality, using the modular approach.",ArtificiallIntelligence
32.txt,"In their framework for ontological analysis, Guarino and Welty provide a number of insights that are useful for guiding the design of taxonomic hierarchies. However, the formal statements of these insights as logical schemata are flawed in a number of ways, including inconsistent notation that makes the intended semantics of the logic unclear, false claims of logical consequence, and definitions that provably result in the triviality of some of their property features. This paper makes a negative contribution, by demonstrating these flaws in a rigorous way, but also makes a positive contribution wherever possible, by identifying the underlying intuitions that the faulty definitions were intended to capture, and attempting to formalize those intuitions in a more accurate way.",ArtificiallIntelligence
110.txt,"This paper describes how well prosodic information correlates with the topic structure of a cooperative dialogue. To investigate this correlation systematically, first we introduce the notion of utterance unit (UU) as a basic unit in conversations. We define the utterance unit by employing four principles. The grammatical principle is a syntactic criterion in which the UU boundary is set wherever the period can be placed. The pragmatic principle says that each UU corresponds to a basic speech act. In other words, if two neighboring phrases correspond to different speech acts (for instance, acknowledgment and request), they should be taken as two different UUs. The conversational principle addresses the turn-taking aspect of conversations. A UU boundary should be placed wherever the speaker changes. Finally, the prosodic principle says that whenever a medium length or longer pause (750 msec) is inserted between two phrases, they are to be taken as two different UUs. We apply these principles to a speech database containing about one and a half hours of collected dialogue to split the dialogues into a sequence of UUs. We then classify the inter-UU boundaries based on the relationship between two neighboring UUs into four semantic categories: topic shift, topic continuation, elaboration (or clarification), and speech-act continuation. The prosodic parameters measured at each boundary are the onset fundamental frequency (F0), the final F0, and the F0 maximal peak declination ratio (the ratio of the current UUUs maximal peak to that of the preceding UU). Our study shows how these prosodic parameters vary depending on the topic structure. Our results can be summarized as follows. (1) The onset F0 value tends to be higher when the topic is changed at the UU boundary. (2) The final F0 value indicates finality and is much higher (on average) at speech-act continuation boundaries than at other boundaries. (3) The maximal peak declination ratio reflects the degree of subordination to the preceding UU. That is, this ratio is lowest at elaboration boundaries and highest at topic shift boundaries. Finally, we discuss discourse structure identification via the prosodic parameters.",ArtificiallIntelligence
2.txt,"We consider word alignment within the ""bag-of-words"" framework of IBM Model 1, and explore alternative optimization criteria and solutions and show that neither the EM nor the probabilistic constraint is necessary for learning good parameters.",ArtificiallIntelligence
122.txt,"Lexicon coverage is often the limiting factor in natural language processing systems. Recent work has attempted to remedy this situation by extracting information from machine readable dictionaries. Unfortunately, no NLP lexicon system or dictionary could possibly list all the potential words of English. However, humans are often able to interpret novel word forms (that is, words they have not seen before) without difficulty. One way we do this, if the word is complex (e.g., ""undecidability""), is by using cues from the internal structure of the word. Relations in phonological form often correspond to relations in meaning. For example, if someone knows what the verb ""open"" means, a number of educated guesses can be made about the meaning of ""reopen"". Exceptions abound in lexical data and any system that attempts to use lexical generalizations must be able to handle exceptions in a principled fashion. In this report, I will describe the preliminary design of a system that uses relations in form to derive relations in meaning. For a new word, the system will produce meaning postulates that represent an educated guess about the meaning of the new word. These meaning postulates will be written in Episodic Logic, and the entire system will be a module of the TRAINS system.",ArtificiallIntelligence
85.txt,"Hierarchical genetic programming (HGP) approaches rely on the discovery, modification, and use of new functions to accelerate evolution. This paper provides a qualitative explanation of the improved behavior of HGP, based on an analysis of the evolution process from the dual perspective of diversity and causality. From a static point of view, the use of an HGP approach enables the manipulation of a population of higher diversity programs. Higher diversity increases the exploratory ability of the genetic search process, as demonstrated by theoretical and experimental fitness distributions and expanded structural complexity of individuals. From a dynamic point of view, this report analyzes the causality of the crossover operator. Causality relates changes in the structure of an object with the effect of such changes, i.e., changes in the properties or behavior of the object. The analyses of crossover causality suggests that HGP discovers and exploits useful structures in a bottom-up, hierarchical manner. Diversity and causality are complementary, affecting exploration and exploitation in genetic search. Unlike other machine learning techniques that need extra machinery to control the tradeoff between them, HGP automatically trades off exploration and exploitation.",ArtificiallIntelligence
1.txt,"Bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to GIZA++, a widely-used piece of software that computes word alignments for statistical machine translation. We show that using variational Bayes improves the performance of GIZA++, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score.",ArtificiallIntelligence
74.txt,"This paper describes a prototype disambiguation module, KANKEI, which uses two corpora of the TRAINS project. In ambiguous verb phrases of form V...NP PP or V...NP adverb(s), the two corpora have very different PP and adverb attachment patterns; in the first, the correct attachment is to the VP 88.7\% of the time, while in the second, the correct attachment is to the NP 73.5\% of the time. KANKEI uses various n-gram patterns of the phrase heads around these ambiguities, and assigns parse trees (with these ambiguities) a score based on a linear combination of the frequencies with which these patterns appear with NP and VP attachments in the TRAINS corpora. Unlike previous statistical disambiguation systems, this technique thus combines evidence from bigrams, trigrams, and the 4-gram around an ambiguous attachment. In the current experiments, equal weights are used for simplicity but results are still good on the TRAINS corpora (92.2\% and 92.4\% accuracy). Despite the large statistical differences in attachment preferences in the two corpora, training on the first corpus and testing on the second gives an accuracy of 90.9\%. These results suggest that our technique captures attachment patterns that are useful across corpora.",ArtificiallIntelligence
75.txt,"We describe some simple domain-independent improvements to plan-refinement strategies for well-founded partial order planning that promise to bring this style of planning closer to practicality. One suggestion concerns the strategy for selecting plans for refinement among the current (incomplete) candidate plans. We propose an A* heuristic that counts only steps and open conditions, while ignoring ``unsafe conditions'' (threats). A second suggestion concerns the strategy for selecting open conditions (goals) to be established next in a selected incomplete plan. Here we propose a variant of a strategy suggested by Peot \&amp; Smith and studied by Joslin \&amp; Pollack; the variant gives top priority to unmatchable open conditions (enabling the elimination of the plan), second-highest priority to goals that can only be achieved uniquely, and otherwise uses LIFO prioritization. The preference for uniquely achievable goals is a ``zero-commitment'' strategy in the sense that the corresponding plan refinements are a matter of deductive certainty, involving no guesswork. In experiments based on modifications of UCPOP, we have obtained improvements by factors ranging from 5 to more than 1000 for a variety of problems that are nontrivial for the unmodified version. Crucially, the hardest problems give the greatest improvements.",ArtificiallIntelligence
108.txt,"This paper describes a formalism, Statistical Event Logic (SEL), that adds statistical reasoning to Allen's planning language [Allen et al., 1991 (Reasoning about Plans)]. Interval temporal logic supports reasoning about time and events; probability inferred from the planner's experience supports reasoning about incomplete information. Statistical Event Logic can represent knowledge that allows a planner to reason both about choices based on incomplete knowledge and about the future likely to result from these choices.",ArtificiallIntelligence
70.txt,"Finding lineal features in an image is an important step in many object recognition and scene analysis procedures. Previous feature extraction algorithms exhibit poor parallel performance because features often extend across large areas of the data set. This paper describes a parallel method for extracting lineal features based on an earlier sequential algorithm, stick growing. The new method produces results qualitatively similar to the sequential method.  Experimental results show a significant parallel processing speed-up attributable to three key features of the method: a large numbers of lock preemptible search jobs, a random priority assignment to source search regions, and an aggressive deadlock detection and resolution algorithm. This paper also describes a portable generalized thread model. The model supports a light-weight job abstraction that greatly simplifies parallel vision programming.",ArtificiallIntelligence
98.txt,"Spoken dialogue poses many new problems to researchers in the field of computational linguistics. In particular, conversants must detect and correct speech repairs, segment a turn into individual utterances, and identify discourse markers. These problems are interrelated. For instance, there are some lexical items whose role in an utterance can be ambiguous: they can act as discourse markers, signal a speech repair, or even be part of the content of an utterance unit. So, these issues must be addressed together. The resolution of these problems will allow a basic understanding of how a speaker's turn can be broken down into individual contributions to the dialogue. We propose that this resolution must be and can be done using local context. They do not require a full understanding of the dialogue so far, nor, in most cases, a deep understanding of the current turn. Resolving these issues locally also means they can be resolved for the most part before later processing, and so will make a natural language understanding system more robust and able to deal with the unconstrained nature of spoken dialogue.",ArtificiallIntelligence
29.txt,We describe a method for reconstructing indoor scenes from image mosaics using prior knowledge of the cuboid structure of the environment. The method is inspired by traditional approaches to the camera pose estimation problems known as the PnL and PnA problems. We show that a cuboid can be reconstructed from the images of three of its corners. The necessary camera intrinsic parameters are obtained by self-calibration in the image mosaicking process. The major advantages of this method over methods such as single view metrology are (1) it can do metric reconstruction; (2) it is a closed form solution so it is numerically stable; and (3) it requires only minimal user interaction.,Robotics
77.txt,"We describe and demonstrate a construct termed a ``virtual tool'' that provides a flexible interface to sensory-motor control. This interface is, from a user standpoint, substantially less complex and more application-oriented than the raw devices. The basic idea is to use extra degrees of freedom present in a flexible system, in conjunction with sophisticated sensing (e.g. vision), to dynamically configure or ``tailor'' a manipulator so that it is matched to a particular situation and operation. This ``virtual tool'' is created by imposing customized, sensory modulated constraints between various degrees of freedom in the system. The remaining degrees of freedom constitute a small set of control parameters that are fitted to a particular operation. We argue that, within the confines of fairly broad application domains, a small set of ``tool classes'' can be defined that will serve as a general purpose sensory-motor toolbox for a wide variety of applications. We further argue that such class definitions can be made portable not only across tasks, but across platforms as well. The implementation of a number of basic tool classes, on various platforms, using vision and other sensory modalities, is described, and their use in performing multi-stage sensory-modulated manipulation tasks is illustrated.",Robotics
26.txt,"We combine scene-space based methods with Bayesian modeling for recovering the geometric (3d shape, appearance) and dynamic (motion, deformation) properties of real-world scenes from noisy images. Bayesian modeling in scene space helps establish a direct mathematical relationship between the uncertainty in estimating scene properties (e.g., 3d shape and motion) and the uncertainty due to noise and errors in image measurements. This leads to algorithms that optimally recover 3d scene properties directly from image intensities. We apply this approach to two specific problems.  The first problem we study is inferring 3d shape from a set of noisy images. We derive a general probabilistic theory of occupancy and emptiness to specify what one can infer about 3d shape for arbitrarily-shaped, Lambertain scenes and arbitary viewpoint configurations. By modeling the problem in scene space, we formalize the notions of visibility, occupancy, emptiness, and photo-consistency, leading to the Photo Hull Distribution, the tightest probabilistic bound on the scene's true shape that is theoretically computable from the input images. We show how to express this distribution directly in terms of image measurements and represent it compactly by assigning an occupancy probability at every 3d point. We provide a stochastic algorithm that draws fair samples from the Photo Hull Distribution and converges to an optiaml conservative estimate of the occupancy probability. We present experimental results for real, complex scenes.  The second problem we study is recovering nonrigid motion of deformable surfaces from noisy video. We develop linear methods for model-based tracking of nonrigid 3d objects in video. Uncertainty in image measurements is quantified and propagated through the inverse model to yield optimal 3d pose and deformation estimates directly from 2d image intensities. We obtain accurate and optimal closed-form nonrigid motion estimators by minimizing information loss from non-reversible operations. We demonstrate results with 3d nonrigid tracking, model refinement, and super-resolution texture lifting from low-quality, low-resolution video.",Robotics
18.txt,"One of the guiding principles of sparse coding is that neurons should convey as much information as possible with every spike. However, sparse coding models have not lived up to this idea. Many models use neurons which output continuous values over time. This is justified by assuming they communicate by firing rates, but this disregards all temporal information from the spike. Newer models use exact spike times, but they also use synchronous firing chains. For neurons to convey as much information as possible per spike, their spikes must be as independent as possible. Synfire chains are, in contrast, very highly correlated spike trains.  This model is to our knowledge the first sparse coding model on spiking neurons which does not use synfire chains. Instead, each neuron tries to make its spike train as independent as possible from those of its neighbors. The neurons are strictly local, with temporal receptive fields and recurrent inhibitory connections. The input can be reconstructed by summing the convolutions of each neuron's spike trains with their receptive fields. Finally, we argue that the algorithm is related to K-means clustering in a convolutional feature space.",Robotics
22.txt,"We lay out the plans for a series of psychophysics experiments on human shadow perceptions. The Peg in Hole experiments are to find out whether shadows play a role in human 3D perception and how important the role is, compared with other depth cues such as stereoscopy. We also design quantitative experiments to extract the psychometric curves for shadow perception versus various characteristics of scene geometry. The curves will help us to decide different thresholds for our scene reconstruction algorithms. Some background of psychophysics is included for future reference. Expected outputs of the experiments are described. Some implementation details are also discussed.",Robotics
66.txt,"We present a method for autonomous learning of dextrous manipulation skills with multifingered robot hands. We use heuristics derived from observations made on human hands to reduce the degrees of freedom of the task and make learning tractable. Our approach consists of learning and storing a few basic manipulation primitives for a few prototypical objects and then using an associative memory to obtain the required parameters for new objects and/or manipulations. The parameter space of the robot is searched using a modified version of the evolution strategy, which is robust to the noise normally present in real-world complex robotic tasks. Given the difficulty of modeling and simulating accurately the interactions of multiple fingers and an object, and to ensure that the learned skills are applicable in the real world, our system does not rely on simulation; all the experimentation is performed by a physical robot, in this case the 16-degree-of-freedom Utah/MIT hand. Experimental results show that accurate dextrous manipulation skills can be learned by the robot in a short period of time.",Robotics
54.txt,"In this report we consider the problem of 3D object recognition, and the role that perceptual grouping processes must play. In particular, we argue that a single level of perceptual grouping is inadequate, and that reliance on a single level of grouping is responsible for the specific weaknesses of several well-known recognition techniques. Instead, we argue that recognition must utilize a hierarchy of perceptual grouping processes, and describe an appearance-based system that uses four distinct levels of perceptual grouping, the upper two novel, to represent 3D objects in a form that not only allows recognition, but reasoning about 3D manipulation of a sort that has been supported in the past only by 3D geometric models.",Robotics
91.txt,"A selective vision system sequentially collects evidence to support a specified hypothesis about a scene, as long as the additional evidence is worth the effort of obtaining it. Efficiency comes from processing the scene only where necessary, to the level of detail necessary, and with only the necessary operators. Knowledge representation and sequential decision-making are central issues for selective vision, which takes advantage of prior knowledge of a domain's abstract and geometrical structure and models for the expected performance and cost of visual operators.  The TEA-1 selective vision system uses Bayes nets for representation and benefit-cost analysis for control of visual and non-visual actions. It is the high-level control for an active vision system, enabling purposive behavior, the use of qualitative vision modules and a pointable multiresolution sensor. TEA-1 demonstrates that Bayes nets and decision theoretic techniques provide a general, re-usable framework for constructing computer vision systems that are selective perception systems, and that Bayes nets provide a general framework for representing visual tasks. Control, or decision making, is the most important issue in a selective vision system. TEA-1's decisions about what to do next are based on general hand-crafted ``goodness functions'' constructed around core decision theoretic elements. Several goodness functions for different decisions are presented and evaluated.  The TEA-1 system solves a version of the T-world problem, an abstraction of a large set of domains and tasks. Some key factors that affect the success of selective perception are analyzed by examining how each factor affects the overall performance of TEA-1 when solving ensembles of randomly produced, simulated T-world domains and tasks. TEA-1's decision making algorithms are also evaluated in this manner. Experiments in the lab for one specific T-world domain, table settings, are also presented.",Robotics
14.txt,"Predictive coding and temporal invariance are two major unsupervised learning principles which have been used to explain the behavior of parts of the brain (most notably the striate cortex). Although both have been around for a number of years, no formal relationship between them has been established. We prove that temporal invariance is a form of predictive coding. To do this, we begin with the goal of predictive coding, make a set of assumptions about the class of problem we are dealing with, and derive temporal invariance from the predictive coding goal and our added assumptions.",Robotics
38.txt,"Due to recent advances in the art, object recognition may soon replace low-level feature extraction processes in automatic image database annotation. However, improvement in performance is still an important consideration. In addition, model acquisition for appearance-based object recognition is tedious, since such systems usually require training on a large set of segmentable example views that cover variation among class exemplars. These views have to be labeled with object identity and pose.  In this thesis we first develop and analyze a feature-based object recognition system that demonstrates good recognition of a variety of 3D shapes, with full orthographic invariance. We report the results of large-scale tests that evaluate recognition performance in conditions of background clutter and partial occlusion, as well as generic capabilities of the system. We develop a statistical framework for predicting the performance in a variety of situations from a few basic measurements. We investigate the performance of object recognition systems to see which, if any, design axes of such systems hold the greatest potential for improving performance. One conclusion is that the greatest leverage seems to lie at the level of intermediate feature construction. We also analyze the effect of other improvements, such as parallelization and the use of multiple views.  We then formalize a system for constructing 3D recognition models using large, cluttered visual corpora, in a minimally supervised manner. After giving it a few seed pictures of an object class (say a couple of pictures of cars), the system is given access to an unlabeled image database containing, among other images, other pictures of the object. The system then explores the image database, augmenting its representation of the object (in this case the car) class to include new information whenever it finds a near enough match to the existing representation. After exposure to sufficient imagery, the system will usually have a general model of the car that can label cars in the entire database and other databases. We obtain a significant improvement in recognition performance when training the system from unlabeled cluttered background images, as opposed to training only on the labeled, black background seed image. The approach could use any appearance-based 3D object recognition system.",Robotics
73.txt,"A general-purpose object indexing technique is described that combines the virtues of principal component analysis with the favorable matching properties of high-dimensional spaces to achieve high precision recognition. An object is represented by a set of high-dimensional iconic feature vectors comprised of the responses of derivative of Gaussian filters at a range of orientations and scales. Since these filters can be shown to form the eigenvectors of arbitrary images containing both natural and man-made structures, they are well-suited for indexing in disparate domains. The indexing algorithm uses an active vision system in conjunction with a modified form of Kanerva's sparse distributed memory which facilitates interpolation between views and provides a convenient platform for learning the association between an object's appearance and its identity. The robustness of the indexing method was experimentally confirmed by subjecting the method to a range of viewing conditions and the accuracy was verified using a well-known model database containing a number of complex 3D objects under varying pose.",Robotics
95.txt,"This paper is about orienting, that is, establishing and maintaining a spatial relation between a motorized pair of cameras (the eye-head system) and a static or a moving object tracked over time. Motivated by physiological evidence, the paper proposes a simple set of vision-based strategies aimed to perform head, eyes and body movements in a complex environment. Fixation is shown to be an essential feature in visual servoing, and it is used to decouple control on head rotational degrees of freedom, making possible a metric-less approach to the orientation problem. A running implementation of these strategies, using a binocular camera system mounted on a PUMA 700, demonstrates the effectiveness of the approach.",Robotics
28.txt,"We present a new algorithm, GM-Sarsa(0), for finding approximate solutions to multiple-goal reinforcement learning problems that are modeled as composite Markov decision processes. According to our formulation different sub-goals are modeled as MDPs that are coupled by the requirement that they share actions. Existing reinforcement learning algorithms address similar problem formulations by first finding optimal policies for the component MDPs, and then merging these into a policy for the composite task. The problem with such methods is that policies that are optimized separately may or may not perform well when they are merged into a composite solution. Instead of searching for optimal policies for the component MDPs in isolation, our approach finds good policies in the context of the composite task.",Robotics
99.txt,"A Polhemus 3Space Isotrak sensor system is used on the VPL Research DataGlove Model 2 to detect the glove's absolute position and orientation. The sensor system has two components: the Polhemus sensor and the Polhemus source. The sensor is attached to the back of the DataGlove. The sensor sends to the DataGlove Control Unit analog signals, which are converted into the Polhemus data--six parameters that represent the position and orientation of the sensor relative to the source. This document describes how the Polhemus devices work and how the Polhemus data should be interpreted. It also describes the transformation of the Polhemus output into the (X, Y, Z, O, A, T) space of the robot in the University of Rochester Robotics Laboratory. This transformation allows teleoperation of the robot through the use of the Polhemus devices.",Robotics
84.txt,"This tutorial is dedicated to our long-suffering 442 students, and to the excellent authors from whom I shamelessly cribbed this work. It is a pure cut-and-paste job from my favorite sources on this material. This is not my own work---think of me as an editor working without his authors' permissions. Readers should know that original authors are usually easier to understand than rehashed versions. If this presentation helps you, good. If not it at least helped me sort a few things out.  I assume knowledge of all necessary linear systems theory, differential equations, statistics, control theory, etc. We start with the ideas of filtering, smoothing, prediction, and state estimation. Wiener filtering and its associated intellectual framework follows, with a brief foray into ARMA filtering. The idea of recursive estimation is introduced to give some motivation for the slog ahead, and then we start with basic concepts in maximum likelihood, maximum a posteriori, and least-squares estimation. The strategy is to work toward the Kalman filtering equations by showing how they are simply related to general least-squares estimation. After Kalman filtering, some simpler versions of recursive filters are presented. There are appendices on the orthogonality principle, the matrix inversion lemma, singular value decomposition, partial C and LISP code, and a worked example.",Robotics
44.txt,"The human brain has to integrate the inputs it receives from different sensory modalities into a coherent description of its environment. This integration is often adaptive, showing recalibration or suppression of discordant sensory modalities. This paper proposes a qualitative theory of sensory integration which relates these adaptation phenomena to the anatomy of the neocortex and a rapid reversible synaptic mechanism as proposed in von der Malsburg's correlation theory of brain function.",Robotics
68.txt,In this paper we present a kinematic method for 6-degree-of-freedom manipulation of rigid objects using a dextrous robotic hand. Our method requires no prior models of the objects being manipulated; instead it obtains all the information needed directly from the hand's sensors. Its low computational cost makes real-time performance easy to achieve.  We present experimental results showing the implementation of our method using the Utah/MIT dextrous hand. We also show that adding a Cartesian controller significantly improves the accuracy of the manipulation.,Robotics
72.txt,"This thesis presents a bottom-up approach to understanding and extending robotic motor control by integrating human guidance. The focus is on dexterous manipulation using a Utah/MIT robot hand but the ideas apply to other robotic platforms as well.  {\em Teleassistance} is a novel method of human/robot interaction in which the human operator uses a gestural sign language to guide an otherwise autonomous robot through a given task. The operator wears a glove that measures finger joint angles to relay the sign language. Each sign serves to orient the robot within the task action sequence by indicating the next perceptual sub-goal and a relative spatial basis. Teleassistance merges robotic servo loops with human cognition to alleviate the limitations of either full robot autonomy or full human control alone.  The operator's gestures are {\em deictic}, from the Greek {\em deiktikos} meaning pointing or showing, because they circumscribe the possible interpretations of perceptual feedback to the current context and thereby allow the autonomous routines to perform with computational economy and without dependence on a detailed task model. Conversely, the use of symbolic gestures permits the operator to guide the robot strategically without many of the problems inherent to literal master/slave teleoperation, including non-anthropomorphic mappings, poor feedback, and reliance on a tight communication loop.  The development of teleassistance stems from an analysis of autonomous control, in light of recent advances in manipulator technology. This work also presents a {\em qualitative}, context-sensitive control strategy that exploits the many degrees of freedom and compliance of dexterous manipulators. The qualitative strategy governs the underlying autonomous routines in teleassistance.",Robotics
71.txt,"Recent neurophysiological experiments appear to indicate that the responses of visual cortical neurons in a monkey freely viewing a natural scene can sometimes differ substantially from those obtained when the same image subregions are flashed during a conventional fixation task. These new findings attain significance from the fact that neurophysiological research in the past has been based predominantly on cell recordings obtained during fixation tasks, under the assumption that these data would be useful in predicting responses in more general situations. We describe a hierarchical model of visual memory that reconciles the two differing experimental results mentioned above by predicting neural responses in both fixating and free-viewing conditions. The model dynamically combines input-driven bottom-up signals with expectation-driven top-down signals to achieve optimal estimation of current state using a Kalman filter based framework. The architecture of the model posits a role for the reciprocal connections between adjoining visual cortical areas in determining neural response properties.",Robotics
6.txt,"We describe implementation details of an activity detection system whose function is to detect small image regions (pixels) that are inconsistent with a learned and adaptive background model. The system is based on a classic multiple Gaussian model approach, but contains additional model elements to deal with phenomena characteristic of outdoor environment such as wind-blown foliage and moving cloud shadows.",Robotics
19.txt,"In classic pattern recognition problems, classes are mutually exclusive by definition. Classification errors occur when the classes overlap in the feature space. We examine a different situation, occurring when the classes are, by definition, not mutually exclusive. Such problems arise in scene and document classification and in medical diagnosis. We present a framework to handle such problems and apply it to the problem of semantic scene classification, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a field scene with a mountain in the background). Such a problem poses challenges to the classic pattern recognition paradigm and demands a different treatment. We discuss approaches for training and testing in this scenario and introduce new metrics for evaluating individual examples, class recall and precision, and overall accuracy. Experiments show that our methods are suitable for scene classification; furthermore, our work appears to generalize to other classification problems of the same nature.",Robotics
65.txt,"David Lowe's influential and classic algorithm for tracking objects with known geometry is formulated with certain simplifying assumptions. A version implemented by Ishii et al. makes different simplifying assumptions. We formulate a full projective solution and apply the same algorithm (Newton's method). We report results of extensive testing of these three algorithms. We compute two image--space and six pose--space error metrics to quantify the effects of object pose, errors in initial solutions, and image noise levels. We consider several scenaria, from relatively unconstrained conditions to those that mirror real--world and real--time constraints. The conclusion is that the full projective formulation makes the algorithm orders of magnitude more accurate and gives it super--exponential convergence properties with arguably better computation--time properties.",Robotics
86.txt,"The goal of this thesis is to demonstrate the utility of low-level motion features for the purpose of recognition. Although motion plays an important role in biological recognition tasks, motion recognition, in general, has received little attention in the literature compared to the volume of work on static object recognition. It has been shown that in some cases, motion information alone is sufficient for the human visual system to achieve reliable recognition. Previous attempts at duplicating such capability in machine vision have been based on abstract higher-level models of objects, or have required building intermediate representations such as the trajectories of certain feature points of the object. In this work we demonstrate that motion recognition can be accomplished using lower-level motion features, without the use of abstract object models or trajectory representations.  First, we show that certain statistical spatial and temporal features derived from the optic flow field have invariant properties, and can be used to classify regional motion patterns such as ripples on water, fluttering of leaves, and chaotic fluid flow. We then present a novel low-level computational approach for detecting and recognizing temporally repetitive movements, such as those characteristic of walking people or flying birds, on the basis of the periodic nature of their motion signatures. We demonstrate the techniques on a number of real-world image sequences containing complex non-rigid motion patterns. We also show that the proposed techniques are reliable and efficient by implementing a real-time activity recognition system.",Robotics
90.txt,"The location of objects in images is difficult owing to the view variance of geometric features but can be determined by developing view-insensitive descriptions of the intensities local to image points. View-insensitive descriptions are achieved in this work by describing points in terms of the responses of steerable filters at multiple scales. Owing to the use of multiple scales, the vector for each point is, for all practical purposes, unique, and thus can be easily matched to other instances of the point in other images. We show that this method can be extended to handle the case where the area near a point of interest is partially occluded. The method uses a description of the occluder in the form of a template that can be obtained easily via active vision systems using a method such as disparity filtering.",Robotics
87.txt,"This dissertation studies the problem of searching for a target object with a visual sensor. In particular, it studies the task of selecting a sequence of viewpoints, viewing directions, and fields of view that efficiently examines the area being searched. This is made difficult by two problems, namely the need for high image resolution and the presence of obstacles that occlude portions of the search area from certain viewpoints.  Searches for objects that require high image resolution to be recognizable can potentially require the examination of a large number of images; high resolution requires a narrow field of view, and hence more images are necessary to span a given visual angle. This dissertation considers a method for increasing search efficiency by searching only those subregions that are especially likely to contain the object. Searches that use this method, called indirect searches, repeatedly find a cheaply-locatable ""intermediate"" object that commonly participates in a spatial relationship with the target object, and then look for the target in the restricted region specified by this relationship. A decision-theoretic model of search efficiency is developed. The model identifies desiderata for useful intermediate objects and predicts that, in typical indoor situations, indirect search provides up to an eight-fold increase in efficiency. The model is also suitable for use in an on-line system for selecting intermediate objects.  The second problem facing a searcher is that portions of the area being searched are often hidden from view. Multiple viewpoints are therefore often necessary. This dissertation examines the selection of such viewpoints. Traditional viewpoint selection methods involve detailed maps of the scene portions viewed so far. Simpler model-free methods are presented that, though less selective about their viewpoints, find objects without significantly more effort than map-based methods. They suggest that the main requirement for selecting efficient viewpoint sequences is that the searcher possesses a mechanism for ensuring that it systematically traverses the viewpoint space. Such mechanisms can be much simpler than maps. One drawback of model-free methods is that when the object is not present, they can waste more effort before aborting. Suggestions for remedying this are presented.",Robotics
70.txt,"We present an approach for building an affine representation of an unknown curved object viewed under orthographic projection from images of its occluding contour. It is based on the observation that the projection of a point on a curved, featureless surface can be computed along a special viewing direction that {\em does not} belong to the point's tangent plane. We show that by circumnavigating the object on the tangent plane of selected surface points, we can (1) compute two orthogonal projections of every point projecting to the occluding contour during this motion, and (2) compute the affine coordinates of these points. Our approach demonstrates that affine shape of curved objects can be computed {\em directly}, i.e., without Euclidean calibration or image velocity and acceleration measurements.",Robotics
75.txt,"On 7 December 1994, four student-built autonomous robots demonstrated various strategic, tactical, and mechanical approaches to a delivery task. That event was preceded by approximately two years of history and two days of frenzied preparation. Our robotics efforts were based on materials from MIT's well-known 6.270 course. This report summarizes our experiences, from pedagogical goals and organizational matters through mechanical and electronic techniques. Our intended audience is future robot-builders, and organizers of robot-building courses. We assume familiarity with material in Jones and Flynn's Mobile Robotics text, and with the various materials available from MIT over the internet.",Robotics
56.txt,"Since scalp EEG recordings are measured in microvolts, electrical signals may easily interfere during an experiment. As Spehlmann discusses, such interference may be introduced through the lights in the recording room, a nearby television, or even a computer monitor [Spehlmann, 1991]. Thus, when we consider performing EEG/EP/ERP experiments within a virtual reality helmet containing an eye tracker, electrical interference becomes a real possibility. We tested the effects of wearing a VR4 virtual reality (VR) helmet containing an ISCAN eye tracker while asking subjects to do a continuous performance task. The results of this task were then analyzed in the frequency domain and compared to results from the same experiment while looking at a computer screen in two different environments. Results indicate that in an environment with other computers, the vertical refresh from the back of a nearby row of computer monitors added more noise to the signal than wearing the VR helmet and eye tracker. Even in an environment without other computers, the noise while wearing the VR helmet and eye tracker is not significantly different from the noise obtained while viewing a laptop computer screen in the same location.",Robotics
40.txt,"Appearance-based object recognition systems rely on training from imagery, which allows the recognition of objects without requiring a 3d geometric model. It has been little explored whether such systems can be trained from imagery that is unlabeled, and whether they can be trained from imagery that is not trivially segmentable. In this paper we present a method for minimally supervised training of a previously developed recognition system from unlabeled and unsegmented imagery. We show that the system can successfully extend an object representation extracted from one black background image to contain object features extracted from unlabeled cluttered images and can use the extended representation to improve recognition performance on a test set.",Robotics
35.txt,"Tracking is frequently considered a frame-to-frame operation. As such, object recognition techniques are generally too slow to be used for tracking. There are domains, however, where the objects of interest do not move most of the time. In these domains, it is possible to watch for activity in the scene and then apply object recognition techniques to find the object's new location. This makes tracking a discrete process of watching for object disappearances and reappearances. We have developed a memory assistance tool that uses this approach to help people with slight to moderate memory loss keep track of important objects around the house. The system is currently deployed in a prototype smart home.",Robotics
52.txt,"Theories of vision have traditionally confined themselves to the passive analysis of static images, focusing on the extraction of task-independent, 3D reconstructions of the visual world. However, the images projected on the retina are seldom unrelated static signals. Rather, they represent measurements of a coherent and continuous stream of events occurring in the visual environment, constrained both by physical laws of nature and the observer's actions on the immediate environment. In short, vision is inherently a dynamic process.  In this thesis, we propose two related theories of dynamic visual perception. The first theory exploits the ability to make eye movements for dynamically exploring the visual world. The resulting architecture uses appearance-based models of objects in lieu of hand-coded 3D models, and employs two visual routines, one for object identification and another for object location, for solving visual cognitive tasks. The second theory, which can be seen as an elaboration of the first, is based directly on the premise that vision is a stochastic, dynamic process. The task of visual perception is then reduced to the dual problems of optimally estimating visual events occurring in the external environment, and on a longer time scale, learning efficient internal models of the environment. Both estimation and learning are appearance-based, relying only on input images rather than hand-coded object/environment models. Using this framework, we derive estimation and learning algorithms for visual recognition, visual ""attention,"" occlusion-handling, segmentation, prediction, hierarchical recognition, transformation-invariant recognition, and pose estimation. Experimental results are provided to corroborate the viability of these derived algorithms.  In addition to their potential applications in machine vision and robotics, the derived algorithms can also be used to understand human and mammalian vision. We use the visual routines theory to model saccade learning behaviors in infants, visual search/cognitive behaviors in adult subjects, and hemispatial neglect in patients with parietal cortex lesions. The optimal estimation and learning framework is used to interpret the hierarchical and laminar circuitry of the mammalian visual cortex, and to explain neuronal properties such as endstopping, response suppression during free viewing of natural images, and spatiotemporal receptive field development in primary visual cortex.",Robotics
76.txt,"We propose and implement a novel method for visual space trajectory planning, and adaptive high degree-of-freedom (DOF) visual feedback control. The method requires no prior information either about the kinematics of the manipulator, or the placement or calibration of the cameras, and imposes no limitations on the number of degrees of freedom controlled or the number of kind of visual features utilized. The approach provides not only a means of low-level servoing but a means to integrate it with higher level visual space trajectory and task planning. We are thus able to specify and perform complex tasks composed of several primitive behaviors, using both visual servoing and open loop control, where the number of sensed and controlled signals varies during the task. We report experimental results demonstrating a factor of 5 improvement in the repeatability of manipulations using a PUMA arm when comparing visual closed-loop to traditional joint level servoing. We also present experiment statistics showing the advantages of adaptive over non-adaptive control systems, and of using redundant visual information when performing manipulation tasks. Finally, we demonstrate usefulness of the approach by using it to specify and execute complex tasks involving real-world robot manipulation of rigid and non-rigid objects in up to 12 degrees of freedom. The manipulation is performed in the context of a semi-autonomous robot manipulation system.",Robotics
32.txt,"We present the theory behind TOD (the Temporal Object Discoverer), a novel unsupervised system that uses only temporal information to discover objects across image sequences acquired by any number of uncalibrated cameras. The process is divided into three phases: (1) Extraction of each pixel's temporal signature, a partition of the pixel's observations into sets that stem from different objects; (2) Construction of a global schedule that explains the signatures in terms of the lifetimes of a set of quasi-static objects; (3) Mapping of each pixel's observations to objects in the schedule according to the pixel's temporal signature. Our Global Scheduling (GSched) algorithm provably constructs a valid and complete global schedule when certain observability criteria are met. Our Quasi-Static Labeling (QSL) algorithm uses the schedule created by GSched to produce the maximally-informative mapping of each pixel's observations onto the objects they stem from. Using GSched and QSL, TOD ignores distracting motion, correctly deals with complicated occlusions, and naturally groups observations across cameras. The sets of 2D masks recovered are suitable for unsupervised training and initialization of object recognition and tracking systems.",Robotics
36.txt,"Automatic single-frame image orientation detection is a difficult problem. In this report, we describe a system designed to classify the orientation of an image. An algorithm designed by researchers at Michigan State University is used as a baseline. First- and second-order spatial color moments are used as features. Learning Vector Quantization (LVQ) is used to estimate the underlying probability density function needed by a Bayesian classifier. We compare these results with those for 1NN and SVM classifiers, and with LDA used as a feature extractor. Extensive experiments were conducted to gain insight into how and why the MSU algorithm works.  We present results both for a stock photo library (Corel) and for a set of consumer images (JBJL). Analyzing these results shows that certain prototypical images (e.g., those with sky at the top) can be classified correctly over 90% of the time, but that the general problem is much more difficult for low-level feature-based approaches. We obtained best results of 74% accuracy on the Corel set and 68% accuracy on JBJL, assuming equal prior among all four possible image orientations.",Robotics
37.txt,"Recent advances in computer hardware and signal processing have made it feasible to use human EEG signals or ""brain waves"" to communicate with a computer. Locked-in patients now have a means to communicate with the outside world. Even with modern advances, such systems still suffer from communication rates on the order of 2-3 items/minute. In addition, existing systems are not likely to be designed with flexibility in mind, leading to slow systems that are difficult to improve. This dissertation presents a flexible brain-computer interface that is designed to facilitate changes in signal processing methods and user applications. In order to show the flexibility of the system, several applications, ranging from a brain-body actuated video game played with eye movements to a brain-computer interface for environmental control in a virtual apartment, are shown.  The P3 evoked potential is a positive wave in the EEG signal peaking at around 300 milliseconds after task-relevant stimuli and it can be used as a binary control signal. A virtual driving experiment shows that the P3 can be reliably detected within a virtual environment. Several on-line algorithms for processing single trial P3 evoked potentials are presented and compared. It is important that actual EEG signals rather than signal artifacts are being recognized and thus false recognition of artifacts is shown to be small.  Results from an environmental control application within a virtual apartment are presented. Subjects do not perform significantly different between controlling the application from a computer monitor and when fully immersed in the virtual apartment and subjects like the immersive VR environment better. This highlights the fact that the P3 component of the evoked potential is robust over different environments and that usability does not depend solely on performance, but on other factors as well. Future work is discussed within this context.",Robotics
82.txt,"Active vision systems have the capability of continuously interacting with the environment. The rapidly changing environment of such systems means that it is attractive to replace static representations with visual routines that compute information on demand. Such routines place a premium on image data structures that are easily computed and used.  The purpose of this paper is to propose a general active vision architecture based on efficiently computable iconic representations. This architecture employs two primary visual routines, one for identifying the visual image near the fovea (object identification), and another for locating a stored prototype on the retina (object localization). This design allows complex visual behaviors to be obtained by composing these two routines with different parameters.  The iconic representations are comprised of high-dimensional feature vectors obtained from the responses of an ensemble of Gaussian derivative spatial filters at a number of orientations and scales. These representations are stored in two separate memories. One memory is indexed by image coordinates while the other is indexed by object coordinates. Object localization matches a localized set of model features with image features at all possible retinal locations. Object identification matches a foveal set of image features with all possible model features. We present experimental results for a near real-time implementation of these routines on a pipeline image processor and suggest relatively simple strategies for tackling the problems of occlusions and scale variations. We also discuss two additional visual routines one for top-down foveal targeting using log-polar sensors and another for looming detection, which are facilitated by the proposed architecture.",Robotics
74.txt,"The Datacube MaxVideo 200 is a high-speed image processing system that can provide video rate processing of images. A user writes programs using the Datacube ImageFlow libraries to control the hardware. Learning the details of ImageFlow programming is a daunting task for the new user. This learning task is made more difficult because the manuals for the MaxVideo 200 hardware and ImageFlow software are rather obscure to the typical novice user. This user's guide describes several simple ImageFlow programs. Emphasis is placed on providing some of the folklore that is needed to get started in MV200/ImageFlow programming. Also, the organization of the information in the Datacube manuals is described so that the new user can continue exploring on their own system features that are not in the sample programs.",Robotics
30.txt,"In this report, we describe methods of acquiring an environment map by image mosaicking. We focus on technique details of the different algorithms involved. These algorithms include image matching, homography estimation, linear image warping and linear self calibration. Many of these details are distributed in various publications and we here bring them together. Furthermore, these algorithms are widely used in other circumstances and we provide all the C++ code for each of the algorithms.",Robotics
11.txt,"Our goal is to isolate and ultimately identify objects in cluttered and possibly partially-obscuring background environments. Multiple images (usually six in our case) of a scene, including at least five flash images with different flash locations, locate depth edges in the scene. Morphological processing turns the collection of edges into a collection of regions. These regions are candidates for non-background status, and can be compared by color and shape analysis to background regions. Further, we think the edges, labeled with depth and color information, will provide a richer and more reliable set of inputs to an object recognizer like that of Nelson and Selinger, which so far has only had intensity edges to work with.",Robotics
3.txt,"We propose a means of extending Conditional Random Field modeling to decision-theoretic planning where valuation is dependent upon fully observable factors. Representation is discussed, and a comparison with existing decision problem methodologies is presented. Included are exact and inexact message passing schemes for policy making, examples of decision making in practice, extensions to solving general decision problems, and suggestions for future use.",Robotics
7.txt,"There has been much research in recent decades aimed at discovering what the underlying principles are, if any, that drive the brain. As the cortex appears to be basically uniform, it seems that if there is an underlying principle, it is ubiquitous. However, the principles which have been proposed to explain the brain have largely been specialized principles, which each explain a particular aspect of the brain.  Principles such as efficient coding, predictive coding, and temporal invariance have been proposed to explain sensory coding, and have succeeded to some measure in reproducing the receptive field properties of neurons in the visual cortex. Bayesian surprise has been offered as an explanation of attention, and has enjoyed some success in modeling human saccades, while reinforcement learning and intelligent adaptive curiosity have been aimed at explaining how actions are chosen.  In this dissertation we propose a novel principle which we call predictive action. It is an information theoretic principle which unifies all of the above proposals. We show its relationship to each of the above proposals, and give several algorithms which approximate predictive action for specific environments. We hope that this principle will allow not only for a greater understanding of the brain, but also serve as a principled basis for the design of future algorithms to solve a broad range of problems in artificial intelligence.",Robotics
48.txt,Several decades of research have made many advances towards the goal of interpreting the neural spike train but a comprehensive understanding remains elusive. This paper pursues this goal in the context of a new class of models termed predictive models. Predictive models characterize the cortex as a memory whose parameters can be used to predict its input. This allows the input to be economically coded as a residual difference between itself and the prediction. Such models have recently had considerable success in modeling features of visual cortex. This paper shows that the predictive coding model can be extended to a lower level of detail that includes individual spikes as primitives. This is a significant improvement in perspicuity compared to the firing rate variables used by most current models. The specific model we describe exploits the use of coincidence of spike arrival times and the fact that neural representations can be distributed over large numbers of cells.,Robotics
46.txt,"Image-based object recognition systems developed recently don't require the construction of a 3D geometric model, allowing recognition of objects for which current geometric recognition technologies do not apply. Such systems are typically trained with labeled, clean views that cover the whole viewing sphere and can sometimes handle generic, visually similar classes with moderate variation. It has been little explored whether such systems can be trained from imagery that is unlabeled, and whether they can be trained from imagery that is not trivially segmentable.  In this report we investigate how an object recognition system developed previously can be trained from clean images of objects with minimal supervision. After training this system on a single or a small number of views of each object, a simple learning algorithm is able to attract additional views to the object representation, building clusters of views belonging to the same object. We explore how the learning performance improves by extending the set of views, introducing a small amount of supervision, or using more complicated learning algorithms.",Robotics
88.txt,"When a reinforcement learning agent's next course of action depends on information that is hidden from the sensors because of problems such as occlusion, restricted range, bounded field of view and limited attention, we say the agent suffers from the Hidden State Problem. State identification techniques use history information to uncover hidden state. Previous approaches to encoding history include: finite state machines [Chrisman 1992; McCallum 1992], recurrent neural networks [Lin and Mitchell 1992], and genetic programming with indexed memory [Teller 1994]. A chief disadvantage of all these techniques is their long training time.  This report presents Instance-Based State Identification, a new approach to reinforcement learning with state identification that learns with much fewer training steps. Noting that learning with history and learning in continuous spaces both share the property that they begin without knowing the granularity of the state space, the approach applies instance-based (or ``memory-based'') learning to history sequences---instead of recording instances in a continuous geometrical space, we record instances in action-perception-reward sequence space. The first implementation of this approach, called Nearest Sequence Memory, learns with an order of magnitude fewer steps than several previous approaches.",Robotics
43.txt,"In this paper we introduce a formalism for optimal sensor parameter selection for iterative state estimation in static systems. In contrast to common approaches, where a certain metric---for example, the mean squared error between true and estimated state---is optimized during state estimation, in this work the optimality is defined in terms of reduction in uncertainty in the state estimation process. The main assumption is that state estimation becomes more reliable if the uncertainty and ambiguity in the state estimation process can be reduced.  We consider a framework based on Shannon's information theory and select the camera parameters that maximize the mutual information, i.e., optimize the information that the captured image conveys about the true state of the system. The technique implicitly takes into account the a priori probabilities governing the computation of the mutual information. Thus a sequential decision process can be formed by treating the a priori probability at a certain time step in the decision process as the a posteriori probability of the previous time step.  We demonstrate the benefits of our approach using an object recognition scenario and an active pan/tilt/zoom camera. During the sequential decision process the camera looks to parts of the object that allow the most reliable distinction of similar looking objects. We performed experiments with discrete density representation as well as with continuous densities and Monte Carlo evaluation of the mutual information. The results show that the sequential decision process outperforms a random gaze control, both in the sense of recognition rate and number of views necessary to return a decision.",Robotics
47.txt,In this report we describe a method for extracting curves from an image using directional pixel variances instead of gradient measures as low-level boundary evidence. The advantage of the variance over the image gradient is that we can accurately compute the direction of a local edge even if a sudden contrast change occurs in the background. This allows curves belonging to object contours to be followed more easily. We compared our method to a similar method based on the image gradient and we found that it obtains better results when run on synthetic and natural images. Our method also improved the performance of a contour-based 3D object recognition system in cluttered images.,Robotics
21.txt,"Shadows provide valuable information about the scene geometry, especially the whereabout of the light source. This paper investigates the geometry of point light sources and cast shadows. It is known that there is redundancy in the object-shadow correspondences. We explicitly show that no matter how many such correspondences are available, it is impossible to locate a point light source from shadows with a single view. We discuss the similarity between a point light source and a conventional pinhole camera and show that the above conclusion is in accordance to traditional camera self calibration theory. With more views, however, the light source can be located by triangulation. We proceed to solve the problem of establishing correspondences between the images of an object with extended size and its cast shadow. We prove that a supporting line, which, put simply, is a tangent line of the image regions of the object and its shadow, provides one correspondence. We give an efficient algorithm to find supporting lines and prove that at most two supporting lines can be found. The intersection of these two lines gives the direction of the point light source. All this can be done without any knowledge of the object. Experiment results using real images are shown.",Robotics
17.txt,"This paper addresses the issue of how verbal communication arises from the complex and uncertain representations that seem necessary to robustly carry out perception in real-world domains. We propose that the generation of natural language in such domains should be addressed as the optimization problem of finding, under various constraints, the verbalization that has the greatest probability of achieving a specific change that the speaker wants to induce in the mental state or behavior of the listener. This most likely effective or MLE strategy has the advantage of making the problem concrete, and allowing (possibly empathic) models of the perceptual and behavioral processes to be used in a principled way. We illustrate these issues in the context of the specific problem of describing real objects in native domains using basic color language (e.g., ""mostly brown,"" ""partly red""). The term ""native domains"" refers to real-world environments that have not been tailored to suit the application.",Robotics
89.txt,"Machine learning aims towards the acquisition of knowledge based on either experience from the interaction with the external environment or by analyzing the internal problem-solving traces. Both approaches can be implemented in the Genetic Programming (GP) paradigm. Hillis [1990] proves in an ingenious way how the first approach can work. There have not been any significant tests to prove that GP can take advantage of its own search traces. This paper presents an approach to automatic discovery of functions in GP based on the ideas of discovery of useful building blocks by analyzing the evolution trace, generalizing of blocks to define new functions and finally adapting of the problem representation on-the-fly. Adaptation of the representation determines a hierarchical organization of the extended function set which enables a restructuring of the search space so that solutions can be found more easily. Complexity measures of solution trees are defined for an adaptive representation framework and empirical results are presented.",Robotics
62.txt,"This paper presents a new class of interactive image editing operations designed to maintain physical consistency between multiple images of a physical 3D object. The distinguishing feature of these operations is that edits to any one image propagate automatically to all other images as if the (unknown) 3D object had itself been modified. The approach is useful first as a power-assist that enables a user to quickly modify many images by editing just a few, and second as a means for constructing and editing image-based scene representations by manipulating a set of photographs. The approach works by extending operations like image painting, scissoring, and morphing so that they alter an object's plenoptic function in a physically-consistent way, thereby affecting object appearance from all viewpoints simultaneously. A key element in realizing these operations is a new volumetric decomposition technique for reconstructing an object's plenoptic function from an incomplete set of camera viewpoints.",Robotics
85.txt,"Designing real-world applications can involve coordinating many pieces of hardware and integrating multiple software components. Increased processing power has allowed complex real-world applications to be designed, and there has been increasing interest in the issues involved in designing both the applications and their support. In this paper we describe the issues involved in designing the application. The shepherding application we have chosen is representative of many real-world applications. This report focuses on technical details. We describe the underlying hardware, including the camera, vision processing boards, processors, and puma robot arm. We then discuss the software components we designed to integrate the hardware components in real-time. At each stage we describe the trade-offs between the different possibilities and why the ones chosen were best suited for our environment. We also present results supporting our selection. At appropriate points we indicate underlying support that would have eased and improved our implementation.",Robotics
96.txt,"We present a computational, constructive theory of tunable, open loop trajectory skills. A skill is a controller whose outputs achieve any task in a space characterized by n parameters, n &gt; 1. Throwing a ball at a target is a 3-dimensional task if the target may be anywhere within a 3-dimensional volume. Repetitous pick and place tasks are zero-dimensional, and thus not skills. Skills are performed open loop for speed reasons: we assume the entire command sequence is generated before any feedback can become available. We do not assume prior knowledge of plant or task models, so skills must be at least partly learned. A skill output is a vector of values---in our work so far it is generated as the sum of a base vector and a weighted change vector whose weight accomplishes the tuning. Learning consists of a search for the best set of base and change vectors. An interpretation process maps skill outputs into sequences of commands for the plant by using basis functions (given a priori in this paper). The basis functions may be arbitrarily complex. We claim that appropriate basis functions can speed up the learning process and overcome the limitations of the linear trajectory tuning algorithm. This report describes a skill learning algorithm and experiments done with various basis functions and control methods for a one-dimensional throwing task. It concludes with a discussion of future work in learning basis functions, higher dimensional tasks, and comparisons against common learning and control algorithms.",Robotics
34.txt,"In this paper, we present a method for propagating segmentation information across a saccade for a foveating camera. In particular, we take a region of interest from a wide-angle, low-fidelity image and propagate its segmentation information to a zoomed, high-fidelity image containing that region. Our method uses normalized greyscale templates to estimate the change in translation and magnification required to transform the segmented region. This process is useful for systems which detect regions of interest at low-fidelity and then perform a saccade to provide a high-fidelity view of that region of interest. We show how using this method increases the performance of an active object recognition system.",Robotics
67.txt,"Reinforcement learning is a machine learning framework in which an agent manipulates its environment through a series of actions, and in response to each action, receives a reward value. The agent stores its knowledge about how to choose reward-maximizing actions in a mapping from agent-internal states to actions.  Agents often struggle with two opposite, yet intertwined, problems regarding their internal state space. First, the agent's state space may have too many distinctions---meaning that an abundance of perceptual data has resulted in a state space so large that it overwhelms the agent's limited resources for computation, storage and learning experience. This problem can often be solved if the agent uses selective perception to prune away unnecessary distinctions, and focus its attention only on certain features. Second, even though there are too many distinctions, the agent's state space may simultaneously contain too few distinctions---meaning that perceptual limitations (such as field of view, acuity and occlusions), have temporarily hidden crucial features of the environment from the agent. This problem, called hidden state, can often be solved by using memory of features from previous views to augment the agent's perceptual inputs.  This dissertation presents algorithms that use selective perception and short-term memory to simultaneously prune and augment the state space provided by the agent's perceptual inputs. During learning, the agent selects task-relevant state distinctions with a utile distinction test that uses robust statistics to determine when a distinction helps the agent predict reward. The dissertation also advocates using instance-based (or memory-based) learning for making efficient use of accumulated experience, and using a tree structure to hold variable-length memories. Four new algorithms are shown to perform a variety of tasks well---in some cases with more than an order-of-magnitude better performance than previous algorithms.",Robotics
1.txt,"Gait recognition is an important research problem in the field of computer vision. The goal is to identify people by analysis of gait patterns. Because the technique can be performed remotely, it has been applied to access control, surveillance, etc. Most research is based on the assumption that people's walking direction is perpendicular to the camera axis. In this case the silhouette can be extracted to identify individuals. This limits the application and development of gait recognition. Consequently, walking direction has recently become a popular and challenging research problem. An improved gait recognition approach is proposed. It can give high recognition rates in cases where people's walking direction is not perpendicular to the camera axis. We describe a novel approach to walking direction computation using information about camera position. The walking direction angle and camera affine projection model are used to define features that can be related to a kinematic model of a human being. Support Vector Machine is used for classification and to evaluate the power of the approach. We apply our method to real human walking image sequences, and achieve relatively high recognition rates. Our approach illustrates how changes in walking direction affect gait parameters in terms of recognition performance. We show that the use of the walking direction algorithm improves recognition rates under variation in viewing direction.",Robotics
45.txt,"Appearance-based object recognition systems are currently the most successful approach for dealing with 3D recognition of arbitrary objects in the presence of clutter and occlusion. However, no current system seems directly scalable to human performance levels in this domain. In this report we describe a series of experiments on a previously described object recognition system that try to see which, if any, design axes of such systems hold the greatest potential for improving performance. We look at the potential effect of different design modifications and we conclude that the greatest leverage lies at the level of intermediate feature construction.",Robotics
92.txt,"TEA-1 is a selective vision system that uses Bayes nets for representation and benefit-cost analysis for control of visual and nonvisual actions. TEA-1 solves T-world problems, a class of problems involving static two-dimensional scenes. For example, TEA-1 has been demonstrated to answer questions about scenes of dinner tables.  This paper presents dTEA-1, an extension to TEA-1 that allows tasks to be performed on dynamic scenes. Currently, dTEA-1 successfully performs a task on a simulated train scene. The objects in the scene include a train on a track and a herd of cows, but the domain may be extended to include static objects and other classifications of moving objects. The task is to keep track of the locations of objects, and the system intelligently allocates its effort to keep uncertainty and cost to a minimum.",Robotics
94.txt,"An agent with selective perception focuses its sensors on those parts of the environment that are relevant to the task at hand. Selective perception is an efficient method of gathering information from the world, but it presents problems for a learning agent when different actions are required in situations for which the selective perception system cannot produce distinguishing outputs. If this happens the agent is said to have incomplete perception, and the agent may be able to use internal state determined by past perceptions and actions in order to choose the correct action.  I propose research on learning algorithms that use short-term memory to disambiguate the incomplete perception that arises with selective perception. I present the Utile Distinction Memory (UDM) algorithm that solves the incomplete perception problem using a partially observable Markov decision process to represent the agent's internal state space. A significant feature of the algorithm is that it will build an internal state space proportionate to the task at hand, not as large as would be required to represent all of the perceivable world. A second algorithm, part of work in progress, will keep the advantages of UDM while improving learning speed and the ability to recognize the significance of memories that span multiple time steps.  Learning to use memory is difficult and will require a strong bias to learn efficiently. I will investigate ``learning by watching'' as a method of providing bias. Two applications I propose to study are: driving a simulated car using vision from the driver's point of view; and setting a table with human cooperation or interference. Using the results of psychophysical experiments, I will compare my algorithm's perceptual actions with the perceptual actions made by human subjects.",Robotics
12.txt,"We propose a 3-D object reconstruction method using a stationary camera and a planar mirror. No calibration is required. The mirror provides the extra views needed for a multiple-view reconstruction. We examine the imaging geometry of the camera-mirror setup and prove a theorem that gives us the point correspondences to compute the orientation of the mirror. The correspondences are derived from the convex hull of the silhouettes of the images of the object and its mirror reflection. The distance between the mirror and the camera can be then obtained by a single object point and a pair of points on the mirror surface. After the pose of the mirror is determined, we have multiple calibrated views of the object. We show two reconstruction methods that utilize the special imaging geometry. The system setup is simple. The algorithm is fast and easy to implement.",Robotics
58.txt,"In this paper, we discuss how to process visual information in convoying applications, using only low-cost, off-the-shelf hardware. We introduce a numerical algorithm for real-time perspective pose estimation that uses strong task-specific constraints to achieve efficiency and stability. Through extensive experiments performed with synthetic data, we show that this approach yields more accurate recovery than a general-purpose structure-and-motion recovery framework known as the Variable State Dimension Filter, even when some of its fundamental task-specific assumptions are only partially valid. In addition, we discuss efficient ways to perform low-level vision with off-the-shelf hardware, and we present a two-level control strategy that uses high-frequency odometry data to stabilize visual control. Real-world convoying experiments show that our tracking-and-control system performs quite well in the sense that it manages to keep targets in view, tolerates changes in lighting conditions, and enables vehicles to keep up with complex maneuvers performed by other members of the convoy, such as 180-degree turns.",Robotics
15.txt,"Predictive coding is an unsupervised learning principle which has been proposed to explain the brain's perceptual abilities. While it has enjoyed success in modeling the receptive field properties of cells in the visual and auditory cortex, its application has so far been limited only to perceptual areas of the cortex. Given the uniformity of the cortex, it seems likely that one principle can account for the operation of the entire cortex.  We believe that predictive coding is such a principle, and that its utility extends well beyond perception. All that is necessary for a predictive coding network to do motor control is that it have some feedback. If its outputs affect its inputs, then it will use its outputs to affect the world in a way that it can predict. Therefore, motor control falls naturally out of the predictive coding framework.  One must be very careful in the development of a predictive coding network in order to avoid making assumptions that prevent the network from using its outputs in this fashion. In particular, the network must not assume that its input is independent of its output. It also must not assume that its outputs should be independent of each other. Finally, and most importantly, it must maximize Shannon information about its input, instead of Fisher information. We are not aware of any predictive coding network which satisfies these criteria, so we present a derivation of one here.",Robotics
9.txt,"We argue that due to engineering choices in the design of computational machinery, the fundamental difficulty of achieving translation invariance in vision systems is widely misunderstood in the image-processing community. Far from being a relatively trivial problem, translation invariance represents a complex abstraction that is of equivalent difficulty to (and can be considered complete for) an important class of structural distortion problems. We also argue that this class of abstractions is, in an important sense, efficiently learnable. These facts have significant implications for the abilities of any plastic system (e.g. the brain) that is able to acquire or ""learn"" some member of the class.",Robotics
23.txt,"In this report, we describe an obstacle identification method using affine structures from motion. We first identify a reference plane by tracking feature points across image sequences. We then compute the homographies between images induced by the reference plane from these feature points. Once the feature points are categorized as on the plane and off the plane, we reconstruct the affine structure from all the off-plane points. The obstacles are identified from the affine structure. Our method doesn't require a calibrated camera. Results of simulated and real experiments show that our method work very well.",Robotics
79.txt,"In robot skill learning the robot must obtain data for training by executing expensive practice trials and recording their results. The thesis is that the high cost of acquiring training data is the limiting factor in the performance of skill learners. Since the data is obtained from practice trials, it is important that the system make intelligent choices about what actions to attempt while practicing. In this dissertation we present several algorithms for intelligent experimentation in skill learning.  In open-loop skills the execution goal is presented and the controller must then choose all the control signals for the duration of the task. Learning is a high-dimensional search problem. The system must associate a sequence of actions with each commandable goal. We propose an algorithm that selects practice actions most likely to improve performance by making use of information gained on previous trials. On the problem of learning to throw a ball using a robot with a flexible link, the algorithm takes only 100 trials to find a ``whipping'' motion for long throws.  Most closed loop learners improve their performance by gradient descent on a cost function. The main drawback of this method is convergence to non-optimal local minima. We introduce the concept of cooperation as a means of escaping these local minima. We assume the existence of several coaches that each improve some aspect of the controller's performance. Switching training between coaches can help the controller avoid locally minimal solutions. On the task of curve tracing with an inverted pendulum the cooperative algorithm learns to track faster than with a traditional method.  In an integrated system with scarce sensor resources it is preferable to perform tasks without sensing. We observe that closed loop learning can function as an efficient search technique for open-loop control. Our system starts with closed loop learning. As it improves its ability to control the plant, it replaces sensor information with estimates. The result is a controller that tracks long segments of a reference curve open loop.",Robotics
69.txt,"Using a combination of techniques from visual representations, view synthesis, and visual-motor transfer function estimation, we present a method for animating movements of an active agent (e.g., robot), without the use of any prior models or explicit 3d information. The information needed to generate simulated images can be acquired either on or off line, by watching the agent doing an arbitrary, possibly unrelated task. We present experimental results synthesizing image sequences of the movement of a simulated PUMA 760 robot arm, using both joint space and Cartesian world coordinate control. We have created a user interface, where a user can input a robot movement program, and then upon execution, view movies of the (simulated) robot executing the program, along with the instantaneous dynamic variables from the simulated robot.",Robotics
64.txt,"We describe the design and implementation of a video-based augmented reality system capable of overlaying three-dimensional graphical objects on live video of dynamic environments. The key feature of the system is that it is completely uncalibrated: it does not use any metric information about the calibration parameters of the camera or the 3D locations and dimensions of the environment's objects. The only requirement is the ability to track across frames at least four feature points that are specified by the user at system initialization time and whose world coordinates are unknown. Our approach is based on the following observation: Given a set of four or more non-coplanar 3D points, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points. We exploit this observation by (1) tracking lines and fiducial points at frame rate, and (2) representing virtual objects in a non-Euclidean, affine frame of reference that allows their projection to be computed as a linear combination of the projection of the fiducial points.",Robotics
33.txt,"In a teleconference, reprojecting a face can make it appear to be looking in a particular direction. Thus reprojection can substitute for an individual camera for each member of the conference, saving on hardware and transmission bandwidth. Our reprojection algorithm has an off-line part, which calculates fundamental matrices expressing the relationships of different points of view.  During operation, the on-line part converts a single image into possibly several others that give conference participants the consistent impression that the speaker is addressing a particular person. Our offline algorithm presents only an easy version of the generally difficult correspondence problem.",Robotics
57.txt,"In this paper we consider the problem of computing the 3D shape of an unknown, arbitrarily-shaped scene from multiple color photographs taken at known but arbitrarily-distributed viewpoints. By studying the equivalence class of all 3D shapes that reproduce the input photographs, we prove the existence of a special member of this class, the maximal photo-consistent shape, that (1) can be computed from an arbitrary volume that contains the scene, and (2) subsumes all other members of this class. We then give a provably-correct algorithm for computing this shape and present experimental results from applying it to the reconstruction of a real 3D scene from several photographs. The approach is specifically designed to (1) build 3D shapes that allow faithful reproduction of all input photographs, (2) resolve the complex interactions between occlusion, parallax, shading, and their effects on arbitrary collections of photographs of a scene, and (3) follow a ""least commitment"" approach to 3D shape recovery.",Robotics
98.txt,"Using a binocular, maneuverable visual system, a robot that holds its gaze on a visual target can enjoy improved visual perception and performance in interacting with the world. This dissertation examines the problem of holding gaze on a moving object from a moving platform, without requiring the ability to recognize the target. A novel aspect of the approach taken is the use of controlled camera movements to simplify the visual processing necessary to keep the cameras locked on the target. A gaze holding system on the Rochester robot's binocular head demonstrates this approach. Even while the robot is moving, the cameras are able to track an object that rotates and moves in three dimensions.  The key observation is that visual fixation can help separate an object of interest from distracting surroundings. Camera vergence produces a horopter (surface of zero stereo disparity) in the scene. Binocular features with no disparity can be extracted with a simple filter, showing the object's location in the image. Similarly, an object that is being tracked will be imaged near the center of the field of view, so spatially-localized processing helps concentrate on the target. Rochester's binocular robot exploits these observations. The vergence and smooth tracking systems cooperate to hold the cameras on an object moving in three dimensions. The vergence system changes the vergence angle of the cameras to drive the disparity of the target to zero, relying on the tracking system to keep the target in the central field of view. The tracking system centers the cameras on the zero-disparity signals, relying on the vergence system to hold vergence on the target. Instead of requiring a way to recognize the target, the system relies on active control of camera movements and binocular fixation segmentation.",Robotics
53.txt,"In this paper we consider the problem of computing the 3D shape of an unknown, arbitrarily-shaped scene from multiple color photographs taken at known but arbitrarily-distributed viewpoints. By studying the equivalence class of all 3D shapes that reproduce the input photographs, we prove the existence of a special member of this class, the maximal photo-consistent shape, that (1) can be computed from an arbitrary volume that contains the scene, and (2) subsumes all other members of this class. We then give a provably-correct algorithm, called Space Carving, for computing this shape and present experimental results from applying it to the reconstruction of geometrically-complex scenes from several photographs. The approach is specifically designed to (1) build 3D shapes that allow faithful reproduction of all input photographs, (2) resolve the complex interactions between occlusion, parallax, shading, and their effects on arbitrary collections of photographs of a scene, and (3) follow a ""least commitment"" approach to 3D shape recovery.",Robotics
27.txt,"Semantic scene classification, categorizing images into one of a set of physical (e.g., indoor/outdoor, orientation) or semantic categories (e.g., beach or party), is a relatively new field. Most of the existing techniques used primarily low-level features to classify scenes and achieved some success on constrained problems. We report on the state of the art, presenting summaries of major scene classification systems and identifying the features and inference engines they use.",Robotics
50.txt,"Virtual reality (VR) provides immersive and controllable experimental environments. It expands the bounds of possible evoked potential (EP) experiments by providing complex, dynamic environments in order to study cognition without sacrificing environmental control. The addition of quick, on-line analysis enables feedback to subjects, making the system we present ideal for safe Brain Computer Interface (BCI) research. In this context, we describe an experiment to recognize the existence of P300 EP epochs at red stoplights and the absence of this signal at yellow stoplights in a virtual driving environment. In order to determine the plausibility of single trial on-line P300 epoch analysis in the artifact ridden driving environment, we have compared the use of Independent Component Analysis (ICA), a Kalman filter, a robust Kalman filter, and correlation with the stoplight averages for recognition ability off-line. We report that while all methods perform better than correlation, the robust Kalman filter gives the highest recognition accuracy, and we discuss future work in this context.",Robotics
25.txt,"The task of generic object recognition involves learning to identify members of a class of objects based on a few exemplars from that class. Generic object classes are inherently ill-defined. Objects can be grouped into classes based on varying criteria such as form, function, color, size, etc. In this work, we develop two extensions to a well-studied, 3D view-based, rigid-object recognizer that improve its performance on generic object classes grouped on shape and a related class of objects we call loosely structured objects.  The first extension uses clustering on the underlying local context features to discover features that recur within object classes. The modification improves performance for rigid, generic, and loosely structure classes, but it does not reliably discover recurrent features. Further analysis shows most of the performance improvement comes from a side-effect of the clustering algorithm. Namely, features that tend to create noise in the system become marginalized.  The second extension takes a principled approach to estimating the quality of each object model feature based on its robustness and commonness. Noisy features get a low quality score and thus contribute less noise to the recognition process. This approach further improves recognition for rigid, generic, and loosely structured object classes over the clustering method.  We also develop an active recognition system that achieves better recognition by utilizing additional information available in the active vision setting. The system uses change detection to perform foreground/background segmentation on the scene. This segmentation information is used to command a pan, tilt, zoom camera to acquire a high-resolution image of target regions in the scene. Furthermore, the segmentation information is used to reduce background clutter in these high-resolution target images.  Finally, we describe the Memory Assistant application built on top of the active recognition system. This application is designed to assist people with mild to moderate memory loss keep track of important objects in a home environment. A prototype of this system is currently deployed at the Center for Future Health at the University of Rochester Medical Center.",Robotics
59.txt,"In this paper we review and compare several techniques for model-based pose recovery (extrinsic camera calibration) from monocular images. We classify the solutions reported in the literature as analytical perspective, affine and numerical perspective. We also present reformulations for two of the most important numerical perspective solutions: Lowe's algorithm and Phong-Horaud's algorithm. Our improvement to Lowe's algorithm consists of eliminating some simplifying assumptions on its projective equations. A careful experimental evaluation reveals that the resulting fully projective algorithm has superexponential convergence properties for a wide range of initial solutions and, under realistic usage conditions, it is up to an order of magnitude more accurate than the original formulation, with arguably better computation-time properties. Our extension to Phong-Horaud's algorithm is, to the best of our knowledge, the first method for independent orientation recovery that actually exploits the theoretical advantages of point correspondences over line correspondences. We show that in the context of a specific real-life application (visual navigation), it is either more accurate than other similar techniques with the same computational cost, or more efficient with the same accuracy.",Robotics
2.txt,"The Conservation Laboratory of the George Eastman House International Museum of Photography and Film (GEH) and the Department of Computer Science at the University of Rochester (URCS) are collaborating on the problems of preservation and access to daguerreotypes. Parallel (cluster) computation provides high speed image processing to find, classify, and ultimately to eliminate defects and artifacts of deterioration. This TR describes early low-level techniques and applies them to scanner lighting, dust, and scratch defects.",Robotics
24.txt,"This report documents our experience with different optical flow estimation methods and our attempt to use optical flow both qualitatively and quantitatively. Special attention is devoted to improving the Lucas-Kanade method to obtain dense flow. We use a simple clustering technique to find looming objects. This method has the potential of supporting obstacle avoidance using optical flow. Experiments using real images demonstrate that this simple clustering is effective for certain scenes. We also point out when this technique will fail. We try to use optical flow quantitatively to recover the structure of a piecewise planar environment. First, we use the widely-known 8-parameter planar flow equations to locate individual planes in the scene.  Second, in lieu of full flow, we try to use normal flow to compute both the ego-motion and the structure. Both trials fail ungracefully, mostly due to noisy flow data. We describe the mathematics ofboth methods and our experimental results.",Robotics
16.txt,"We have modified the public-domain Quake II game to support research and teaching. Our research is in multi-agent control and supporting human-computer interfaces. Teaching applications have so far been in an undergraduate Artificial Intelligence class and include natural language understanding, machine learning, computer vision, and production system control. The motivation for this report is mainly to document our system development and interface. Only early results are in, but they appear promising. Our source code and user-level documentation is available on the web. The information document is a somewhat motion-blurred snapshot of the situation in September 2004.",Robotics
100.txt,"Traditional analytic robotics defines grasping by knowing the task geometry and the forces acting on the manipulator precisely. This method is particularly important for non-compliant manipulators with few degrees of freedom, such as a parallel jaw gripper, that overconstrain the solution space. In contrast, the advent of anthropomorphic, high degree-of-freedom grippers allows us to use closed-loop strategies that depend heavily on the task context but do not require precise positioning knowledge. To demonstrate, a robotic hand flips a plastic egg, using the finger joint tendon tensions as the sole control signal. The manipulator is a compliant, sixteen degree-of-freedom, Utah/MIT hand mounted on a Puma 760 arm. The completion of each subtask, such as picking up the spatula, finding the pan, and sliding the spatula under the egg, is detected by sensing when the tensions of the hand tendons pass a threshold. Beyond this use of tendon tensions and the approximate starting position of the spatula and pan, no model of the task is constructed. The routine is found to be robust to different spatulas and to changes in the location and orientation of the spatula, egg, and table, with some exceptions.  The egg-flipping example relies on interpreting fluctuating tension values within a known temporal sequence of actions. For instance, knowing when the manipulator is trying to touch the pan with the spatula provides the context to interpret changes in tendon tensions. Given the success of this task, we go on to propose a method for analyzing the temporal sensory output for tasks that have not been previously segmented. This method suggests a means for automatically generating robust force-control programs to perform previously teleoperated manipulation tasks.",Robotics
10.txt,"Augmented Reality (AR) systems insert graphics objects into the images of real scenes. Geometric and photometric consistency has to be achieved to make AR systems effective and bring photorealism to the augmented graphics. Particularly, global illumination effects between the graphics objects and scene objects need to be simulated. This thesis investigates ways to improve AR rendering by creating cast shadows between the real and graphics objects. This requires knowledge about the scene lighting and the scene structure. We first give novel methods of recovering the light sources from the input images. For indoor scenes, we take advantage of scene regularities such as parallel and orthogonal walls. For outdoor scenes and indoor scenes where the lights can be approximated by a directional light source, we show a method of finding the light source from cast shadows present in the real scene.  Besides the light source structure, we also need to know the 3D structure of the scene so that we can render the shadow cast on a real object by a graphics object. Using spheres as primitives, we develop an algorithm to approximate the shape of the scene objects from multiple silhouettes.  With all the above components, one can build an AR system that infers necessary information of the scene from shadows and inserts graphics object with convincing shadows. To justify our endeavor in terms of shadows being important in human spatial perception, we investigate shadow perceptions in the context of cue integration.",Robotics
42.txt,"We study the dynamics of visual cue integration in a tracking / identification task, where subjects track a target object among distractors and identify the target after an occlusion. Objects are defined by three different attributes (color, shape, size) which change randomly within a singe trial. When the attributes differ in their reliability (two change frequently, one is stable) our results show that subjects rapidly re-weight the different cues, putting more emphasis on the stable cue. The re-weighting takes place in less than one second. Our results suggest that cue integration can exhibit adaptive phenomena on a very fast time scale. We propose a probabilistic model with temporal dynamics that accounts for the observed effect.",Robotics
49.txt,"To meet the demands of driving in complex environments, the perception subsystem of an intelligent vehicle must be able to extract the information needed for behaviors from the input video stream. An attractive way of achieving this is to have a library of basic image processing sub-functions (visual routines), which can be composed to subserve more elaborate goal-directed programs. The crucial compositional capability allows the visual routines to span the huge space of different task goals.  The visual routines presented here are developed in a unique platform. The view from a car driving in a simulated world is fed into a Datacube pipeline video processor. The integration of photo-realistic simulation and real-time image processing represents a proof of concept for a new system design that allows testing computer vision algorithms under controllable conditions, thus leading to rapid prototyping. In addition to the simulations, the routines are also tested on similar images generated by driving in the real world to assure the generalizability of the simulation.  The simulator can also be used with human subjects who can drive a kart through the virtual environment while wearing head mounted displays (HMDs). A unique feature of the driving simulator we have built is the ability to track eye movements within a freely moving HMD. This allows the assessment of exigencies in complex situations that can be used to guide the development of automated routines.",Robotics
78.txt,"The recognition of nonrigid motion, particularly that arising from human movement (and by extension from the locomotory activity of animals) has typically made use of high-level parametric models representing the various body parts (legs, arms, trunk, head, etc.) and their connections to each other. Such model-based recognition has been successful in some cases; however, the methods are often difficult to apply to real-world scenes, and are severely limited in their generalizability. The first problem arises from the difficulty of acquiring and tracking the requisite model parts, usually specific joints such as knees, elbows or ankles. This generally requires some prior high-level understanding and segmentation of the scene, or initialization by a human operator. The second problem is due to the fact that the human model is not much good for dogs or birds; for each new type of motion, a new model must be hand-crafted. In this paper, we show that the recognition of human or animal locomotion, and, in fact, any repetitive activity, can be done using low-level, non-parametric representations. Such an approach has the advantage that the same underlying representation is used for all examples, and no individual tailoring of models or prior scene understanding is required. We show in particular that repetitive motion is such a strong cue that the moving actor can be segmented, normalized spatially and temporally, and recognized by matching against a spatio-temporal template of motion features. We have implemented a real-time system that can recognize and classify repetitive motion activities in normal gray-scale image sequences. Results on a number of real-world sequences are described.",Robotics
39.txt,"This paper addresses the inference of 3D shape from a set of n noisy photos. We derive a probabilistic framework to specify what one can infer about 3D shape for arbitrarily-shaped, Lambertian scenes and arbitrary viewpoint configurations. Based on formal definitions of visibility, occupancy, emptiness, and photo-consistency, the theoretical development yields a formulation of the Photo Hull Distribution, the tightest probabilistic bound on the scene's true shape that can be inferred from the photos. We show how to (1) express this distribution in terms of image measurements, (2) represent it compactly by assigning an occupancy probability to each point in space, and (3) design a stochastic reconstruction algorithm that draws fair samples (i.e., 3D photo hulls) from it. We also show experimental results on two complex scenes.",Robotics
61.txt,"Visual cognition depends critically on the moment-to-moment orientation of gaze. Gaze is changed by saccades, rapid eye movements that orient the fovea over targets of interest in a visual scene. Saccades are ballistic; a prespecified target location is computed prior to the movement and visual feedback is precluded. Once a target is fixated, gaze is typically held for about 300 milliseconds, although it can be held for both longer and shorter intervals. Despite these distinctive properties, there has been no specific computational model of the gaze targeting strategy employed by the human visual system during visual cognitive tasks. This paper proposes such a model that uses iconic scene representations derived from oriented spatiochromatic filters at multiple scales. Visual search for a target object proceeds in a coarse-to-fine fashion with the target's largest scale filter responses being compared first. Task-relevant target locations are represented as saliency maps which are used to program eye movements. Once fixated, targets are remembered by using spatial memory in the form of object-centered maps. The model was empirically tested by comparing its performance with actual eye movement data from human subjects in natural visual search tasks. Experimental results indicate excellent agreement between eye movements predicted by the model and those recorded from human subjects.",Robotics
83.txt,"In this paper we present a system for vision-based planning and execution of fingertip grasps using a four-fingered dextrous hand. Our system does not rely on prior models of the objects to be grasped; it obtains all the information it needs from vision and from tactile sensors located at the fingertips of the hand. The grasp planner is based on a genetic algorithm modified to allow the use of real numbers as the basic representation unit. The grasp executer is based on differential visual feedback, which allows the system to specify goals and monitor progress in image space without needing absolute calibration between the camera and the hand. We present experimental results showing the application of the system to grasping unknown objects with the Utah/MIT hand.",Robotics
97.txt,"This paper studies the task of using a mobile camera platform to search a region of space for a target object. Our goal is to maximize the efficiency of such searches. The problem is analyzed using a simple mathematical description of the factors that affect search efficiency. This analysis suggests that one way to improve efficiency is to take advantage of the spatial relationships in which the target object commonly participates. Searches that do so, which we call indirect searches, are modeled as two-stage processes that first find an intermediate object that commonly participates in a spatial relationship for the target object, and then look for the target in the restricted region specified by this relationship. A mathematical model of search efficiency is then used to analyze the efficiency of indirect search over a wide range of situations that vary the spatial structure of the domain as well as recognition performance. The model predicts that, for searches that involve rotating a camera about a fixed location, indirect searches improve efficiency by factors of 2 to 8. An implemented robot search system substantiates these predictions. Finally, we highlight some areas in need of further research if these efficiencies are to be achieved.",Robotics
13.txt,"Scene classification, the automatic categorization of images into semantic classes such as beach, field, or party, is useful in applications such as content-based image organization and context-sensitive digital enhancement. Most current scene-classification systems use low-level features and pattern recognition techniques; they achieve some success on limited domains.  Several contemporary classifiers, including some developed in Rochester, incorporate semantic material and object detectors. Classification performance improves because because the gap between the features and the image semantics is narrowed. We propose that spatial relationships between the objects or materials can help by distinguishing between certain types of scenes and by mitigating the effects of detector failures. While past work on spatial modeling has used logic- or rule-based models, we propose a probabilistic framework to handle the loose spatial relationships that exist in many scene types.  To this end, we have developed MASSES, an experimental testbed that can generate virtual scenes. MASSES can be used to experiment with different spatial models, different detector characteristics, and different learning parameters. Using a tree-structured Bayesian network for inference on a series of simulated natural scenes, we have shown that the presence of key spatial relationships are needed to disambiguate other types of scenes, achieving a gain of 7% in one case.  However, our simple Bayes net is not expressive enough to model the faulty detection at the level of individual regions. As future work, we propose first to evaluate full (DAG) Bayesian networks and Markov Random Fields as potential probabilistic frameworks. We then plan to extend the chosen framework for our problem. Finally, we will compare our results on real and simulated sets of images with those obtained by other systems using spatial features represented implicitly.",Robotics
81.txt,"Manipulators with large numbers of degrees of freedom, from the human hand to the trunk of an elephant, are common in the biological world. These freedoms allow highly flexible and robust performance of complex tasks. However, progress in developing and controlling artificial high-degree-of freedom manipulators has been slow. The main problem is that traditional robotics has focussed on the solution of systems of kinematic equations where there is a unique solution. Such approaches tend not to generalize well to situations with a high-dimensional solution space, and controlling redundant systems has acquired a reputation as a hard problem. However, this need not be the case. In this paper, we describe a behavioral method for using extra degrees of freedom to simplify rather than complicate manipulation problems, while at the same time obtaining more flexibility than would be available with a simpler system. The method is developed in the context of a high DOF robot hand, but it has the potential to generalize to other sorts of manipulators.  The basic idea is based on the observation that, for a particular task, using a custom-designed fitting can greatly simplify the control problem. Using a wrench sized for a particular nut is an extreme example. We use the extra degrees of freedom to dynamically configure or ``tailor'' the manipulator to match the particular object and task at hand. This creates a virtual tool. The tailoring is accomplished by imposing low-level, task-specific constraints on the degrees of freedom. These constraints are selected dynamically from a large set of potential constraints in response to the demands of the current task. The process of smoothly transitioning from one virtual tool to another in the course of task execution is referred to as morphing. We apply the technique to the control of a 16-DOF Utah/MIT hand, and perform fine manipulations on a range of objects using virtual tools that are dynamically instantiated on the basis of sensory information.",Robotics
5.txt,"We describe a self bootstrapping and adaptive system designed to make observations of an outdoor environment and determine some simple environmental condi tions, specifically whether it is day or night, and whether the conditions are clear, cloudy, or some mixtur e. Doing this so that the system will self- adapt and operate reliably in a variety of locations and envi ronments, robust against changes in seasons, weather, and typical human and non-human disturbances (e.g. streetlights, thunderstorms) is a more complex problem than might first be thought. We describe some of the practical issues, and techniques for dealing with them.",Robotics
8.txt,"Semantic scene classification, automatically categorizing images into a discrete set of classes such as beach, sunset, or field, is a difficult problem. Current classifiers rely on low-level image features, such as color, texture, or edges, and achieve limited success on constrained image sets. However, the domain of unconstrained consumer photographs requires the use of new features and techniques.  One source of information that can help classification is the context associated with the image. We have explored three types of context. First, spatial context enables the use of scene configurations (identities of regions and the spatial relationships between the regions) for classification purposes. Second, temporal context allows us to use information contained in neighboring images to classify an image. We exploit elapsed time between images to help determine which neighboring images are most closely related. Third, image capture condition context in the form of camera parameters (e.g., flash, exposure time, and subject distance) recorded at the time the photo was taken provides cues that are effective at distinguishing certain scene types.  We developed and used graphical models to incorporate image content with these three types of context. These systems are highly modular and allow for probabilistic input, output, and inference based on the statistics of large image collections. We demonstrate the effectiveness of each context model on several classification problems.",Robotics
63.txt,"Using results from the field of robust statistics, we derive a class of Kalman filters that are robust to structured and unstructured noise in the input data stream. Each filter from this class maintains robust optimal estimates of the input process's hidden state by allowing the measurement covariance matrix to be a non-linear function of the prediction errors. This endows the filter with the ability to reject outliers in the input stream. Simultaneously, the filter also learns an internal model of input dynamics by adapting its measurement and state transition matrices using two additional Kalman filter-based adaptation rules. We present experimental results demonstrating the efficacy of such filters in mediating appearance-based segmentation and recognition of objects and image sequences in the presence of varying degrees of occlusion, clutter, and noise.",Robotics
80.txt,"We describe a method of 3-D object recognition based on two stage use of a general purpose associative memory and a principal views representation. The basic idea is to make use of semi-invariant objects called keys. A key is any robustly extractable feature that has sufficient information content to specify a 2-D configuration of an associated object (location, scale, orientation) plus sufficient additional parameters to provide efficient indexing and meaningful verification. The recognition system utilizes an associative memory organized so that access via a key feature evokes associated hypotheses for the identity and configuration of all objects that could have produced it. These hypothesis are fed into a second stage associative memory, which maintains a probabilistic estimate of the likelihood of each hypothesis based on statistics about the occurrence of the keys in the primary database. Because it is based on a merged percept of local features rather than global properties, the method is robust to occlusion and background clutter, and does not require prior segmentation. Entry of objects into the memory is an active, automatic procedure. We have implemented a version of the system that allows arbitrary definitions for key features. Experiments using keys based on perceptual groups of line segments are reported. Good results were obtained on a database derived from of approximately 150 images representing different views of 7 polyhedral objects.",Robotics
60.txt,"After abandoning an attempt to build our own gasoline-powered automated outdoor vehicle in 1995, we purchased two M68332-controlled wheelchairs for indoor and outdoor mobile robotics research. Much of the first year has been spent on various infrastructure projects, several of which are described here. At this writing we are beginning to be in a position to do nontrivial applications and research using these platforms. This compendium of facts and experiences is meant to be useful in getting to know the organization and capabilities of our mobile robots. We first cover the basic hardware and the serial protocol used to communicate between the main computing engine and the microcontroller responsible for sensor management, motor control, and low-level sensori-motor control loops. We describe the interface to the video digitizer, a low-level obstacle avoidance routine, and a general software organization for a control architecture based on video streams. Dynamic nonholonomic models and a virtual environment for debugging and experimenting with them are described next, followed up by a visual servoing application that uses ``engineered vision'' and special assumptions.",Robotics
41.txt,"Object recognition from a single view fails when the available features are not sufficient to determine the identity of a single object, either because of similarity with another object or because of feature corruption due to clutter and occlusion. Active object recognition systems have addressed this problem successfully, but they require complicated systems with adjustable viewpoints that are not always available. In this paper we investigate the performance gain available by combining the results of a single view object recognition system applied to imagery obtained from multiple fixed cameras. In particular, we address performance in cluttered scenes with varying degrees of information about relative camera pose. We argue that a property common to many recognition systems, which we term a weak target error, is responsible for two interesting limitations of multi-view performance enhancement: the lack of significant improvement in systems whose single-view performance is weak, and the plateauing of performance improvement as additional multi-view constraints are added.",Robotics
55.txt,"We describe an appearance-based object recognition system using a keyed, multi-level context representation reminiscent of certain aspects of cubist art. Specifically, we utilize distinctive intermediate-level features, in this case automatically extracted 2D boundary fragments, as keys, which are then verified within a local context, and assembled within a loose global context to evoke an overall percept. This system demonstrates extraordinarly good recognition of a variety of 3D shapes, ranging from sports cars and fighter planes to snakes and lizards with full orthographic invariance. We report the results of large-scale tests, involving over 2000 separate test images, that evaluate performance with increasing number of items in the database, in the presence of clutter, background change, and occlusion, and also the results of some generic classification experiments where the system is tested on objects never previously seen or modelled. To our knowledge, the results we report are the best in the literature for full-sphere tests of general shapes with occlusion and clutter resistance.",Robotics
93.txt,"Table lookup with interpolation is used for many learning and adaptation tasks. Redundant mappings capture the important concept of ``motor skill,'' which is important in real, behaving systems. Few, if any, robot skill implementations have dealt with redundant mappings, in which the space to be searched to create the table has much higher dimensionality than the table itself. A practical method for inverting redundant mappings is important in physical systems with limited time for trials. We present the ``Guided Table Fill In'' algorithm, which uses data already stored in the table to guide search through the space of potential table entries. The algorithm is illustrated and tested on a robot skill learning task both in simulation and on a robot with a flexible link. Our experiments show that the ability to search high dimensional action spaces efficiently allows skill learners to find new behaviors that are qualitatively different from what they were presented or what the system designer may have expected. Thus the use of this technique can allow researchers to seek higher dimensional action spaces for their systems rather than constraining their search space at the risk of excluding the best actions.",Robotics
20.txt,"One of the biggest challenges in systems neuroscience is a satisfactory model of neural signaling. From rate coding to temporal coding, models of neural signaling have been challenged by the fact that neurons fire highly irregularly. A typical interpretation of the variability is ``noise other than signal'', which not only has difficulty accounting for the speed, accuracy, efficiency and complexity of biological systems, but is also contradicted by recent studies that show both spike generation and transmission are highly reliable.  Challenged with the discrepancy between theory and data, we take a fresh view of the subject with the proposal that the randomness associated with neuronal outputs is certain to have a purpose. In particular, we model neurons as probabilistic devices that not only compute probabilities but also fire probabilistically to signal their computations. According to our model, signaling of probabilities is done by having cells with similar receptive fields fire synchronously to achieve fast communication, this is consistent with observations of neurons coding as ensembles and topographic map organization. Our proposal of probabilistic, distributed synchronous volleys as a neural signaling strategy not only accounts for variable neural responses, but also provides the advantage of robust and fast computation. Furthermore, the involvements of probabilistic firing and distributed coding explicate how synchronous firing can appear to be a rate code, accounting for the vast amount of data supporting a rate code assumption.  Any neural signaling model must support cortical computation in a biologically realistic fashion. Going beyond simply addressing the role of spikes in cortical cells' communication, we show that our distributed synchrony model can be implemented in a predictive coding framework and can be used to learn structures in the natural environment. Trained with patches from natural images, our model V1 cells develop localized and oriented receptive fields, consistent with V1 simple cell properties. Unlike most cortical computation models, our predictive coding model makes use of single spikes, instead of abstracting spikes away with analog quantities. This close resemblance to biology makes our model well suited for guiding experimental research.",Robotics
51.txt,"Augmented reality is the merging of synthetic sensory information into a user's perception of a real environment. Until recently, it has presented a passive interface to its human users, who were merely viewers of the scene augmented only with visual information. In contrast, practically since its inception, computer graphics--and its outgrowth into virtual reality--has presented an interactive environment. It is our thesis that the agumented reality interfce can be made interactive. We present: techniques that can free the user from restricttive requirements such as working in calibrated environments, resutls with haptic interface technology incorporated into augmented reality domains, and systems considerations that underlie the practical realization of these interactive augmented reality techinques.",Robotics
4.txt,"The web has the potential to serve as an excellent source of example imagery for visual concepts. Image search engines based on text keywords can fetch thousands of images for a given query; however, their results tend to be visually noisy. We present a technique that allows a user to refine noisy search results and characterize a more precise visual object class. With a small amount of user intervention we are able to re-rank search engine results to obtain many more examples of the desired concept. Our approach is based on semi-supervised machine learning in a novel probabilistic graphical model composed of both generative and discriminative elements. Learning is achieved via a hybrid expectation maximization / expected gradient procedure initialized with a small example set defined by the user. We demonstrate our approach on images of musical instruments collected from Google image search. The rankings given by our model show significant improvement with respect to the user-refined query. The results are suitable for improving user experience in image search applications and for collecting large labeled datasets for computer vision research.",Robotics
31.txt,"In this thesis we study the problem of recovering non-rigid motion, shape and reflectance properties of dynamic 3D scenes from image sequences. Our goals are both to advance towards a firmer mathematical understanding of the constraints that exist in this problem and to develop practical methods that extract the desired properties directly from visual data, using as little prior knowledge about the scenes being observed as possible.  To recover motion, shape and reflectance simultaneously when they are all unknown and the scenes potentially have discontinuities, we observe that scenes composed of curves and surfaces with piecewise-smooth shape and motion trace manifolds embedded in 4D space-time as they move. Moreover, we show that these manifolds have a well-defined differential-geometric structure and, consequently, can be used as the basis to create spatiotemporally-distributed geometric and radiometric representations.  This insight is supported by a mathematical analysis of how multi-view image sequences constrain spatiotemporally-localized scene properties such as the instantaneous 3D velocity, position and orientation of individual scene points. Based on this analysis, we develop a general framework for visual reconstruction of dynamic scenes, and propose specific representational primitives that are both powerful enough to capture a broad class of scenes with arbitrarily-high accuracy and simple enough to be unambiguously recovered from visual data alone. The use of these primitives leads us to develop algorithms that break the complex problem of reconstructing entire dynamic scenes into collections of spatiotemporally-localized, well-posed optimization problems.  Experiments with complex real scenes (paper, clothing, skin, shiny objects) and scenes for which ground-truth geometry is known illustrate our methods' ability to (1) explain pixels and pixel variations in terms of their underlying physical causes---3D shape, surface reflectance, 3D motion, illumination, and visibility, (2) recover dense and non-rigid instantaneous velocity fields even in the presence of moving specularities, and (3) incorporate spatio-temporal coherence into computations for improved stability, and accuracy gains with respect to static multi-view analysis techniques.",Robotics
9.txt,"Fast track is a software speculation system that enables unsafe optimization of sequential code. It speculatively runs optimized code to improve performance and then checks the correctness of the speculative code by running the original program on multiple processors.  We present the interface design and system implementation for Fast Track. It lets a programmer or a proï¬ling tool mark fast-track code regions and uses a run-time system to manage the parallel execution of the speculative process and its checking processes and ensures the correct display of program outputs. The core of the run-time system is a novel concurrent algorithm that balances exploitable parallelism and available processors when the fast track is too slow or too fast. The programming interface closely affects the run-time support. Our system permits both explicit and implicit end markers for speculatively optimized code regions as well as extensions that allow the use of multiple tracks and user deï¬ned correctness checking. We discuss the possible uses of speculative optimization and demonstrate the effectiveness of our prototype system by examples of unsafe semantic optimization and a general system for fast memory-safety checking, which is able to reduce the checking time by factors between 2 and 7 for large sequential code on a 8-CPU system.",Systems
1.txt,"The safety of speculative parallelization depends on monitoring all program access to shared data. Automatic solutions use either program instrumentation, which can be costly, or hardware-based triggering, which incurs false sharing. In addition, not all access requires monitoring. It is worth considering a manual approach in which programmers insert access annotations to reduce the cost and increase the precision of program monitoring.  This report first presents an execution model and its interface for access annotation. The semantics of an annotated program is defined by the output of a (sequential) canonical execution. The report then describes a quadratic-time checker that can verify the safety of annotations, that is, whether a speculative execution always produces the canonical output. The report demonstrates the usability of the annotation interface by safely parallelizing a number of code fragments that have uncertain parallelism.",Systems
8.txt,"Coarse-grained task parallelism exists in sequential code and can be leveraged to boost the use of chip multi-processors. However, large tasks may execute thousands of lines of code and are often too complex to analyze and manage statically. This report describes a programming system called \emph{suggestible parallelization}. It consists of a programming interface and a support system. The interface is a small language with three primitives for marking possibly parallel tasks and their possible dependences. The support system is implemented in software and ensures correct parallel execution through speculative parallelization, speculative communication and speculative memory allocation. It manages parallelism dynamically to tolerate unevenness in task size, inter-task delay and hardware speed. When evaluated using four full-size benchmark applications, suggestible parallelization obtains up to a 6 times speedup over 10 processors for sequential legacy applications up to 35 thousand lines in size. The overhead of software speculation is not excessively high compared to unprotected parallel execution.",Systems
20.txt,"There has been considerable recent interest in the support of transactional memory (TM) in both hardware and software. We present an intermediate approach, in which hardware is used to accelerate a TM implementation controlled fundamentally by software. Our hardware support reduces the overhead of common TM tasks, namely, conflict detection and data isolation, for bounded transactions. Software control allows policy flexibility for conflict detection, contention management, and data granularity, in addition to enabling transactions unbounded in space and time. Our hardware consists of 1) an alert-on-update mechanism for fast event-based communication, used for software-controlled conflict detection; and 2) support for programmable data isolation, allowing multiple concurrent transactional readers and writers at the software's behest, along with fast data commit and abort support (using only a few cycles of completely local operation).  Our results show that for common-case bounded transactions, the proposed hardware mechanisms eliminate data copying and dramatically reduce the overhead of bookkeeping and validation (resulting in a factor of 2 improvement in performance on average). Moreover, RTM shows good scalability as the number of threads is increased and graceful degradation in performance when transactions overflow available hardware support. Detecting conflicts eagerly (on first access) or lazily (at commit time), enabled by the ability to handle multiple concurrent transactional writers and readers, can result in differences in performance in either direction depending on the application access pattern (up to two orders of magnitude at 16 threads for one workload), demonstrating the need for policy flexibility.",Systems
17.txt,"A filesystem's sole purpose is to store data so it can be easily accessed at a later time. Part of that entails recovering properly from a system crash. But a difficulty that modern filesystems face is the advent of write caching in a disk. The disk will report that a write operation has completed before the data is actually secure on the magnetic platter. How is a programmer to respond to this outright lie from the hardware?  The problem goes even further. Journaled filesystems depend on the order of the write operations they send to the disk. If the real data is written before the journal, then there is not only no point in having the journal, but it actually gives a false sense of security. After a crash, the disk checker will only replay the journal, never bothering to examine the real data on the disk for consistency.  There is a solution to this problem. It is called Tagged Command Queuing (TCQ) in the SCSI-2 specification. And it is optional. Filesystems, Ext3 in particular in this paper, use TCQ exclusively and have no fallback. We present a fallback solution that depends on a required feature of the SCSI-2 specification, Force Unit Access (FUA). Our results showed that write-intensive workloads display a significant slowdown from the FUA-based solution. However, the performance impact is still less than that incurred by using other methods, such as synchronizing the cache after each journal write.",Systems
22.txt,"Limiting the amount of memory available to a program can hamstring its performance, however in a garbage collected environment allowing too large of a heap size can also be detrimental. Because garbage collection will occasionally access the entire heap, having a significant amount of virtual memory becomes expensive. Determining the appropriate size for a program's heap is not only important, but difficult in light of various virtual machines, operating systems, and levels of multi-programming with which the program may be run.  We present a model for program memory usage with which we can show how effective multi-programming is likely to be. In addition, we present an automated system for adding control at the program level that allows runtime adaptation of a program's heap size. The process is fully automatic and requires no extra coding on the part of programmers. We discuss two adaptive schemes: The first acts independently, and while performing competitively, the system behaves politely in a multi-programmed environment. The second scheme explicitly cooperates when multiple instances are running. Both schemes are evaluated in terms of their response time, throughput, and fairness.",Systems
16.txt,"This paper addresses interoperability of software transactions and ad hoc nonblocking algorithms. Specifically, we explain how to modify %arbitrary nonblocking methods so that (1) they can be used both inside and outside transactions, (2) external uses serialize with transactions, and (3) internal uses succeed if, only if, and when the  surrounding transaction commits. Interoperability has two important benefits. First, it allows nonblocking methods to play the role of fast, closed nested transactions, with potentially significant performance benefits. Second, it allows programmers to safely mix transactions and nonblocking methods, e.g., to update legacy code, call nonblocking libraries, or atomically compose nonblocking methods.  We demonstrate our ideas in the context of the Java-based ASTM system on several lock-free datastructures. Our findings are encouraging: Although performance of transaction-safe nonblocking objects does not match that of the original nonblocking objects, the degradation is not unacceptably high (particularly after application of an optimization we call lazy logging). It is, moreover, significantly better than that of analogous transactional objects. We conclude that transaction safe nonblocking objects can be a significant enhancement to software transactional memory.",Systems
15.txt,"A high-concurrency Transactional memory (TM) implementation needs to track concurrent accesses, buffer speculative updates, and manage conflicts. We propose that the requisite hardware mechanisms be decoupled from one another. Decoupling (a) simplifies hardware development, by allowing mechanisms to be developed independently; (b) enables software to manage these mechanisms and control policy (e.g., conflict management strategy and laziness of conflict detection); and (c) makes it easier to use the hardware for purposes other than TM.  We present a system, FlexTM (FLEXible Transactional Memory), that employs three decoupled hardware mechanisms: read and write  signatures, which summarize per-thread access sets; per-thread  conflict summary tables, which identify the threads with which  conflicts have occurred; and a lazy versioning mechanism, which  maintains the speculative updates in the local cache and employs a  thread-private buffer (in virtual memory) only in the rare event of  an overflow. The conflict summary tables allow lazy conflict  management to occur locally, with no global arbitration (they also  support eager management). All three mechanisms are kept  software-accessible, to enable virtualization and to support  transactions of arbitrary length. In experiments with a prototype on  the Simics/GEMS testbed, FlexTM provides a 5 times speedup over  high-quality software TM, with no loss in policy flexibility. Our  analysis highlights the importance of lazy conflict detection, which  maximizes concurrency and helps to ensure forward progress. Eager  detection provides better overall system utilization in a  mixed-programming environment. We also present a preliminary case  study in which FlexTM components aid in the development of a tool to  detect memory-related bugs.",Systems
11.txt,"It has been widely suggested that memory transactions should behave as if they acquired and released a single global lock. Unfortunately, this behavior can be expensive to achieve, particularly when---as in the natural publication/privatization idiom---the same data are accessed both transactionally and nontransactionally. To avoid overhead, we propose selective strict serializability (SSS) semantics, in which transactions have a global total order, but nontransactional accesses are globally ordered only with respect to explicitly marked transactions. Our definition of SSS formally characterizes the permissible behaviors of an STM system without recourse to locks. If all transactions are marked, then SSS, single- lock semantics, and database-style strict serializability are equivalent.  We evaluate several SSS implementations in the context of a TL2-like STM system. We also evaluate a weaker model, selective flow serializability (SFS), which is similar in motivation to the asymmetric lock atomicity of Menon et al. We argue that ordering-based semantics are conceptually preferable to lock-based semantics, and just as efficient.",Systems
24.txt,"The data layout of a program is critical to performance because it determines the spatial locality of the data access. Most quantitative notions of spatial locality are based on the overall miss rate and leave three questions not fully answered: how much can the locality of a given data layout be improved, can a data layout be improved if the miss rate cannot be lowered, and can the overall spatial locality be decomposed into smaller components? This paper describes a new definition of spatial locality that addresses these questions. The model is based on off-line profiling of a sequential execution. It has been used to analyze the spatial locality of 14 SPEC2000 benchmarks.",Systems
21.txt,"Software transactional memory systems enable a programmer to easily write concurrent data structures such as lists, trees, hashtables, and graphs, where non-conflicting operations proceed in parallel. Many of these structures take the abstract form of a dictionary, in which each transaction is associated with a search key. By regrouping transactions based on their keys, one may improve locality and reduce conflicts among parallel transactions.  In this paper, we present an executor that partitions transactions among available processors. Our key-based adaptive partitioning monitors incoming transactions, estimates the probability distribution of their keys, and adaptively determines the (usually nonuniform) partitions. By comparing the adaptive partitioning with uniform partitioning and round-robin keyless partitioning on a 16-processor SunFire 6800 machine, we demonstrate that key-based adaptive partitioning significantly improves the throughput of fine-grained parallel operations on concurrent data structures.",Systems
2.txt,"The goal of cache management is to maximize data reuse. Collaborative caching provides an interface for software to communicate access information to hardware. In theory, it can obtain optimal cache performance.  In this paper, we study a collaborative caching system that allows a program to choose different caching methods for its data. As an interface, it may be used in arbitrary ways, sometimes optimal but probably suboptimal most times and even counter productive. We develop a theoretical foundation for collaborative cache to show the inclusion principle and the existence of a distance metric we call LRU-MRU stack distance. The new stack distance is important for program analysis and transformation to target a hierarchical collaborative cache system rather than a single cache configuration. We use 10 benchmark programs to show that optimal caching may reduce the average miss ratio by 24%, and a simple feedback-driven compilation technique can utilize collaborative cache to realize 38% of the optimal improvement.",Systems
14.txt,"Reuse distance is a basic metric for program locality. The distribution of reuse distances, called the reuse signature, shows the average locality or the amount of actively used data. Random access is often assumed in analytical models about program behavior. An interesting question is whether the reuse behavior of random data access has a closed-form answer. In this paper we prove that the length of reuse distances of random access is uniformly distributed from 0 to n-1 when n is the size of data. We also test random traces of different lengths to show the effect on the distribution.",Systems
7.txt,"Memory hardware reliability is an indispensable part of whole-system dependability. Its importance is evidenced by a plethora of prior research work studying the impact of memory errors on software systems. However, the absence of solid understanding of the error characteristics prevents software system researchers from making well reasoned assumptions, and it also hinders the careful evaluations over different choices of fault tolerance design. In this paper, we present our realistic memory hardware error traces collected from production computer systems with more than 800GB memory for around nine months. Based on the traces (including detailed information on the error addresses and patterns), we explore the implications of different hardware ECC protection schemes so as to identify the most common error causes and approximate error rates exposed to the software level. Lastly, we investigate the software system susceptibility to some major error causes, with the particular goal to validate, question, and augment results of prior system studies.",Systems
19.txt,"Early implementations of software transactional memory (STM) assumed that sharable data would be accessed only within transactions. Memory may appear inconsistent in programs that violate this assumption, even when program logic would seem to make extra-transactional accesses safe. Designing STM systems that avoid such inconsistency has been dubbed the privatization problem. We argue that privatization comprises a pair of symmetric subproblems: private operations may fail to see updates made by transactions that have committed but not yet completed; conversely, transactions that are doomed but have not yet aborted may see updates made by private code, causing them to perform erroneous, externally visible operations. We explain how these problems arise in different styles of STM, present strategies to address them, and discuss their implementation tradeoffs. We also propose a taxonomy of contracts between the system and the user, analogous to programmer-centric memory con- sistency models, which allow us to classify programs based on their privatization requirements. Finally, we present empirical comparisons of several privatization strategies. Our results suggest that the best strategy may depend on application characteristics",Systems
3.txt,"We argue for transactions as the synchronization primitive of an ordering-based memory consistency model. Rather than define transactions in terms of locks, our model defines locks, conditions, and atomic/volatile variables in terms of transactions. A traditional critical section, in particular, is a region of code, bracketed by transactions, in which certain data have been privatized. Our memory model, originally published at OPODIS'08, is based on the database notion of strict serializability (SS). In an explicit analogy to the DRF0 of Adve and Hill, we demonstrated that SS provides the appearance of transactional sequential consistency (TSC) for programs that are transactional data-race free (TDRF). We argue against relaxation of the total order on transactions, but show that selective relaxation of the relationship between program order and transaction order (selective strict serializability - SSS) can allow the implementation of transaction-based locks to be as efficient as conventional locks. We also show that condition synchronoication (in the form of the transaction retry primitive) can be accommodated in our model without explicit mention of speculation, opacity, or aborted transactions. Finally, we compare SS and SSS to the notion of strong isolation (SI), arguing the SI is neither sufficient for TSC nor necessary in programs that are TDRF.",Systems
5.txt,"Most computing users today have access to clusters of multi-core computers. To fully utilize a cluster, one must combine two levels of parallelism: shared-memory parallelism within a machine and distributed memory parallelism across machines. Such programming is difficult. Either a user has to mix two programming languages in a single program and use fixed computation and data partitioning between the two, or the user has to rewrite a program from scratch. Even after careful programming, a program may still have hidden concurrency bugs. Users who are accustomed to sequential programming do not find the same level of debugging and performance analysis support especially for a distributed environment. The paper presents a language of suggestions for distributive parallelization. The suggestion language is designed for a user or a profiling-based tool to annotate possible parallelism in C/C++ programs by inserting hints. The hints are safe against any type of misuse and expressive enough to specify independent, pipelined, and speculative parallel execution on a cluster of multi-core computers.",Systems
18.txt,"Bloom filters are compact set representations that support set membership queries with small, one-sided error probabilities. Standard Bloom filters are oblivious to object popularity in sets and membership queries. However, sets and queries in many distributed applications follow known, stable, highly skewed distributions (e.g., Zipf-like). This paper studies the problem of minimizing the false-positive probability of a Bloom filter by adapting the number of hashes used for each data object to its popularity in sets and membership queries. We model the problem as a constrained nonlinear integer program and propose two polynomial-time solutions with bounded approximation ratios --- one is a 2-approximation algorithm with O(N^c) running time (c&gt;=6 in practice); the other is a (2+e)-approximation algorithm with running time O(N^2/e), e&gt;0. Here N denotes the total number of distinct data objects that appear in sets or queries. We quantitatively evaluate our proposed approach on two distributed applications (cooperative caching and full-text keyword searching) driven by real-life data traces. Compared to standard Bloom filters, our data popularity-conscious Bloom filters achieve up to 24 and 27 times false-positive probability reduction for the two applications respectively. The quantitative evaluation also validates our solution's bounded approximation ratio to the optimal.",Systems
23.txt,"The use of multi-core, multi-processor machines is opening new opportunities for software speculation, where program code is speculatively executed to improve performance at the additional cost of monitoring and error recovery. In this paper we describe a new system that uses software speculation to support unsafely optimized code. We open a fast, unsafe track of execution but run the correct code on other processors to ensure correctness. We have developed an analytical model to measure the effect of major parameters including the speed of the fast track, its success rate, and its overheads. We have implemented a prototype and verified the correctness and performance using a synthetic benchmark on a 4-CPU machine.",Systems
12.txt,"Software Transactional Memory (STM) systems, if they support condition synchronization, typically do so through a retry mechanism. Using retry, a transaction explicitly self aborts and deschedules itself when it discovers that a precondition for its operation does not hold. The underlying implementation may then track the set of locations read by the retrying transaction, and refrain from scheduling the transaction for re-execution until at least one location in the set has been modified by another transaction.  While retry is elegant and simple, the conventional implementation has several potential drawbacks that may limit both its efficiency and its generality. In this note, we present a retry mechanism based on Bloom filters that is entirely orthogonal to TM implementation. Our retry is compatible with hardware, software, and hybrid TM implementations, and has no impact on memory management or on the cache behavior of shared locations. It does, however, serialize writer transactions after their commit point when there are retrying transactions. We describe our mechanism and compare it to an optimized version of the conventional implementation.",Systems
4.txt,"The paper presents delta send-recv, an MPI extension for overlapping coarse-grained computation and communication. It provides an interface for marking the data computation and its communication. It automatically blocks computation and divides communication into increments. Delta sends and recvs are dynamically chained to effect sender-receiver pipelining, which is superior to pipelining only at the sender or the receiver side. The evaluation uses kernel tests to find the best increment size for different MPI implementations and types of machines and networks. It shows 2 to 3 times performance improvement for large-volume data reduce involving 16 or 32 processors. In addition, the new interface enables computation and communication pipelining in an interpreted programming language, Rmpi.",Systems
25.txt,"Many sequential applications are difficult to parallelize because of problems such as unpredictable data access, input-dependent parallelism, and custom memory management. These difficulties led us to build a system for behavior-oriented parallelization (BOP), which allows a program to be parallelized based on partial information about program behavior, for example, a user reading just part of the source code, or a profiling tool examining merely one or few inputs.  The basis of BOP is programmable software speculation, where a user or an analysis tool marks possibly parallel regions in the code, and the run-time system executes these regions speculatively. It is imperative to protect the entire address space during speculation. The main goal of the paper is to demonstrate that the general protection can be made cost effective by three novel techniques: programmable speculatio",Systems
6.txt,"While a conventional program uses exactly as much memory as it needs, the memory use of a garbage-collected program can be adjusted by changing the size of the heap used by the garbage collector. This difference can allow applications to adapt their memory demands in response to the changing amount of available memory in a shared environment, which is increasingly important for todayâs multicore, multiprocessor machines. We present a memory performance model that better addresses issues that occur with the more changeable memory demands of garbage-collected applications. Traditional locality models are not directly applicable because the application demand may change based on the available memory size. We describe time-memory curves, which can be used to derive optimal static memory allocation a shared environment. For dynamic environments, however, more will be needed. In this work, we describe Poor Richardâs Memory Manager, a lightweight system to reduce paging costs that can be incorporated into existing runtime systems. We describe the design of the memory manager and show how it can be added to most existing garbage collectors with little to no effort. Using an ex- perimental evaluation of both homogeneous and heterogeneous Java workloads on a dual processor machine, we show that Poor Richardâs Memory Manager improves average performance by a factor of 3 or more when paging, while adding almost no overhead when there is no memory pressure. We further show that this system is not specific to any single algorithm, but improves every garbage collector on which it is tested. We finally demonstrate the versatility of our memory manager by using it to improve the performance of a range of .Net workloads.",Systems
13.txt,"The current state of the art seems to favour blocking software transactional memory (STM) implementations over nonblocking ones, and a common belief is that nonblocking STMs fundamentally cannot be made to perform as well as blocking ones. But this belief is based on experience, intuition, and anecdote, not on rigorous analysis.  We believe there is still plenty of room for improvement in the performance of nonblocking STMs and that, regardless of performance, blocking is unacceptable in some contexts. It is therefore important to continue improving nonblocking STMs, both as a goal in its own right, as well as to inform research aimed at determining whether a fundamental gap exists between blocking and nonblocking STMs.  We describe a novel nonblocking copyback mechanism for a word-based software transactional memory (STM), which closely follows simple and efficient blocking mechanisms in the common case. Previous nonblocking copyback mechanisms impose significant overhead on the common case. Our performance experiments show that this approach yields significant performance improvement over the previous best nonblocking word-based STM. Our design approach can be applied to some other blocking STMs to achieve nonblocking counterparts that perform similarly in the common case.",Systems
10.txt,"In the search for high performance, most transactional memory (TM) systems execute atomic blocks concurrently and must thus be prepared for data conflicts. These conflicts must be detected and the system must choose a policy in terms of when and how to manage the resulting contention. Conflict detection essentially determines when the conflict manager is invoked, which can be dealt with eagerly (when the transaction reads/writes the location), lazily at commit time, or somewhere in between.  In this paper, we analyze the interaction between conflict detection and contention manager heuristics. We show that this has a significant impact on exploitation of available parallelism and overall throughput. First, our analysis across a wide range of applications reveals that simply stalling before arbitrating helps side-step conflicts and avoid making the wrong decision. HTM systems that don't support stalling after detecting a conflict seem to be prone to cascaded aborts and livelock. Second, we show that the time at which the contention manager is invoked is an important policy decision: lazy systems are inherently more robust while eager systems seem prone to pathologies, sometimes introduced by the contention manager itself. Finally, we evaluate a \textit{mixed} conflict detection mode that combines the best of eager and lazy. It resolves write-write conflicts early, saving wasted work, and read-write conflicts lazily, allowing the reader to commit/serialize prior to the writer while executing concurrently.",Systems
14.txt,"We study the complexity of manipulation for a family of election systems derived from Copeland voting via introducing a parameter alpha that describes how ties in head-to-head contests are valued. We show that the problem of manipulation for unweighted Copeland^alpha elections is NP-complete even if the size of the manipulating coalition is limited to two. Our result holds for all rational values of alpha such that 0 &lt; alpha &lt; 1 except for alpha = 1/2. We contrast our result with the fact that microbribery for Copeland^alpha is currently known to be in P exactly for alpha in {0,1/2,1} (complexity results for other values of alpha are unknown). Microbribery is a problem very closely related to manipulation. Since it is well known that manipulation via a single voter is easy for Copeland, ourresult is the first one where an election system originally known to be vulnerable to manipulation via a single voter is shown to be resistant to manipulation via a coalition of a constant number of voters. We also study the complexity of manipulation for Copeland^alpha for the case of a constant number of candidates. We show that here the exact complexity of manipulation often depends closely on the winner model as well as on the parameter alpha: Depending whether we try to make our favorite candidate a winner or a unique winner and whether alpha is 0, 1 or between these values, the problem of weighted manipulation for Copeland^alpha with three candidates is either in P or is NP-complete. Our results show that ways in which ties are treated in an election system, here Copeland voting, can be crucial to establishing complexity results for this system.",Theory
6.txt,"For many election systems, bribery (and related) attacks have been shown NP-hard using constructions on combinatorially rich structures such as partitions and covers. It is important to learn how robust these hardness protection results are, in order to find whether they can be relied on in practice. This paper shows that for voters who follow the most central political-science model of electorates---single-peaked preferences---those protections vanish. By using single-peaked preferences to simplify combinatorial covering challenges, we for the first time show that NP-hard bribery problems---including those for Kemeny and Llull elections---fall to polynomial time for single-peaked electorates. By using single-peaked preferences to simplify combinatorial partition challenges, we for the first time show that NP-hard partition-of-voters problems fall to polynomial time for single-peaked electorates. We show that for single-peaked electorates, the winner problems for Dodgson and Kemeny elections, though \Theta_2^p-complete in the general case, fall to polynomial time. And we completely classify the complexity of weighted coalition manipulation for scoring protocols in single-peaked electorates.",Theory
36.txt,"A king in a directed graph is a node from which each node in the graph can be reached via paths of length at most two. There is a broad literature on tournaments (completely oriented digraphs), and it has been known for more than half a century that all tournaments have at least one king [Lan53]. Recently, kings have proven useful in theoretical computer science, in particular in the study of the complexity of the semifeasible sets [HNP98,HT05] and in the study of the complexity of reachability problems [Tan01,NT02].  In this paper, we study the complexity of recognizing kings. For each succinctly specified family of tournaments, the king problem is known to belong to $\Pi_2^p$ [HOZZ]. We prove that this bound is optimal: We construct a succinctly specified tournament family whose king problem is $\Pi_2^p$-complete. It follows easily from our proof approach that the problem of testing kingship in succinctly specified graphs (which need not be tournaments) is $\Pi_2^p$-complete. We also obtain $\Pi_2^p$-completeness results for k-kings in succinctly specified j-partite tournaments, $k,j \geq 2$, and we generalize our main construction to show that $\Pi_2^p$-completeness holds for testing k-kingship in succinctly specified families of tournaments for all $k \geq 2$.",Theory
25.txt,"Given a function based on the computation of an NP machine, can one in general eliminate some solutions? That is, can one in general decrease the ambiguity? This simple question remains, even after extensive study by many researchers over many years, mostly unanswered. However, complexity-theoretic consequences and enabling conditions are known. In this tutorial-style article we look at some of those, focusing on the most natural framings: reducing the number of solutions of NP functions, refining the solutions of NP functions, and subtracting from or otherwise shrinking #P functions. We will see how small advice strings are important here, but we also will see how increasing advice size to achieve robustness is central to the proof of a key ambiguity-reduction result for NP functions.",Theory
13.txt,"We present the Bitwise Bloom Filter, a data structure for maintaining counts for a large number of items. The bitwise filter is an extension of the Bloom filter, a space-efficient data structure for storing a large set efficiently by discarding the identity of the items being held while still being able to determine whether it is in the set or not with high probability. We show how this idea can be extended to maintaining counts of items by maintaining a separate Bloom filter for every position in the bit representations of all the counts. We give both theoretical analysis of the accuracy of the Bitwise filter together with validation via experiments on real network data.",Theory
26.txt,"We further siimplify Paterson's version of the Ajtai-Komlos-Szemeredi sorting network, and its analysis, mainly by tuning the invariant to be maintained.",Theory
21.txt,"A king in a directed graph is a vertex from which each vertex in the graph can be reached via paths of length at most two. There is a broad literature on tournaments (completely oriented digraphs), and it has been known for more than half a century that all tournaments have at least one king [Lan53]. Recently, kings have proven useful in theoretical computer science, in particular in the study of the complexity of reachability problems [NT05] and semifeasible sets [HNP98, HT06, HOZZ06].  In this paper, we study the complexity of recognizing kings. For each succinctly specified family of tournaments, the king problem is already known to belong to $\Pi_2^{\mathrm p}$ [HOZZ06]. We prove that the complexity of kingship problems is a rich enough vocabulary to pinpoint every nontrivial many-one degree in $\Pi_2^{\mathrm p}$. That is, we show that \emph{every} set in $\Pi_2^{\mathrm p}$ other than $\emptyset$ and $\Sigma^*$ is equivalent to a king problem under $\leq_{\mathrm m}^{\mathrm p}$-reductions. Indeed, we show that the equivalence can even be instantiated via relatively simple padding, and holds even if the notion of kings is redefined to refer to $k$-kings (for any fixed $k \geq 2$)---vertices from which the all vertices can be reached via paths of length at most $k$. In contrast, we prove that recognizing whether a given vertex is a source (i.e., there exists a $k$ such that it is a $k$-king) yields languages that also fall within $\Pi_2^{\mathrm p}$, yet cannot be $\Pi_2^{\mathrm p}$-complete---or even $\Class{NP}$-hard---unless $\Class{P} = \Class{NP}$.  Using these and related techniques, we obtain a broad range of additional results about the complexity of king problems, diameter problems, and radius problems. It follows easily from our proof approach that the problem of testing kingship in succinctly specified graphs (which need not be tournaments) is $\Pi_2^{\mathrm p}$-complete. We show that the radius problem for arbitrary succinctly represented graphs is $\Sigma_3^{\mathrm p}$-complete, but that in contrast the diameter problem for arbitrary succinctly represented graphs (or even tournaments) is $\Pi_2^{\mathrm p}$-complete.",Theory
23.txt,"Electoral control refers to attempts by an election's organizer (``the chair'') to influence the outcome by adding/deleting/partitioning voters or candidates. The groundbreaking work of Bartholdi, Tovey, and Trick on (constructive) control proposes computational complexity as a means of resisting control attempts: Look for election systems where the chair's task in seeking control is itself computationally infeasible.  We introduce and study a method of combining two or more candidate-anonymous election schemes in such a way that the combined scheme possesses all the resistances to control (i.e., all the NP-hardnesses of control) possessed by any of its constituents: It combines their strengths. From this and new resistance constructions, we prove for the first time that there exists an election scheme that is resistant to all twenty standard types of electoral control.",Theory
19.txt,"Control of elections refers to attempts by an agent to, via such actions as addition/deletion/partition of candidates or voters, ensure that a given candidate wins [BTT92]. An election system in which such an agent's computational task is NP-hard is said to be resistant to the given type of control. The only election systems known to be resistant to all the standard control types are highly artificial election systems created by hybridization [HHR07]. In this paper, we prove that an election system developed by the 13th century mystic Ramon Llull and the well-studied Copeland election system are both resistant to all the standard types of (constructive) electoral control other than one variant of addition of candidates. This is the most comprehensive resistance to control yet achieved by any natural election system. In addition, we show that Llull and Copeland voting are very broadly resistant to bribery attacks, and we integrate the potential irrationality of voter preferences into many of our results.",Theory
11.txt,"Control and bribery are settings in which an external agent seeks to influence the outcome of an election. Constructive control of elections refers to attempts by an agent to, via such actions as addition/deletion/partition of candidates or voters, ensure that a given candidate wins [BTT92]. Destructive control refers to attempts by an agent to, via the same actions, preclude a given candidate's victory [HHR07a]. An election system in which an agent can sometimes affect the result and it can be determined in polynomial time on which inputs the agent can succeed is said to be vulnerable to the given type of control. An election system in which an agent can sometimes affect the result, yet in which it is NP-hard to recognize the inputs on which the agent can succeed, is said to be resistant to the given type of control.  Aside from election systems with an NP-hard winner problem, the only systems previously known to be resistant to all the standard control types were highly artificial election systems created by hybridization [HHR07b]. This paper studies a parameterized version of Copeland voting, denoted by Copeland^\alpha, where the parameter \alpha is a rational number between 0 and 1 that specifies how ties are valued in the pairwise comparisons of candidates. In every previously studied constructive or destructive control scenario, we determine which of resistance or vulnerability holds for Copeland^\alpha for each rational \alpha, 0 \leq \alpha \leq 1. In particular, we prove that Copeland^{0.5}, the system commonly referred to as ``Copeland voting,'' provides full resistance to constructive control, and we prove the same for Copeland^\alpha, for all rational \alpha, 0 &lt; \alpha &lt; 1. Among systems with a polynomial-time winner problem, Copeland voting is the first natural election system proven to have full resistance to constructive control. In addition, we prove that both Copeland^0 and Copeland^1 (interestingly, Copeland^1 is an election system developed by the thirteenth-century mystic Ramon Llull) are resistant to all standard types of constructive control other than one variant of addition of candidates. Moreover, we show that for each rational \alpha, 0 \leq \alpha \leq 1, Copeland^\alpha voting is fully resistant to bribery attacks, and we establish fixed-parameter tractability of bounded-case control for Copeland^\alpha.  We also study Copeland^\alpha elections under more flexible models such as microbribery and extended control, we integrate the potential irrationality of voter preferences into many of our results, and we prove our results in both the unique-winner model and the nonunique-winner model. Our vulnerability results for microbribery are proven via a novel technique involving min-cost network flow.",Theory
2.txt,"Many electoral bribery, control, and manipulation problems (which we will refer to in general as ""manipulative actions"" problems) are NP-hard in the general case. It has recently been noted that many of these problems fall into polynomial time if the electorate is single-peaked (i.e., is polarized along some axis/issue). However, real-world electorates are not truly single-peaked. There are usually some mavericks, and so real-world electorates tend to merely be nearly single-peaked. This paper studies the complexity of manipulative-action algorithms for elections over nearly single-peaked electorates, for various notions of nearness and various election systems. We provide instances where even one maverick jumps the manipulative-action complexity up to $\np$-hardness, but we also provide many instances where a reasonable number of mavericks can be tolerated without increasing the manipulative-action complexity.",Theory
31.txt,"We study the robustness---the invariance under definition changes---of the cluster class CL#P [HHKW05]. This class contains each #P function that is computed by a balanced Turing machine whose accepting paths always form a cluster with respect to some length-respecting total order with efficient adjacency checks. The definition of CL#P is heavily influenced by the defining paper's focus on (global) orders. In contrast, we define a cluster class, CLU#P, to capture what seems to us a more natural model of cluster computing. We prove that the naturalness is costless: CL#P = CLU#P. Then we exploit the more natural, flexible features of CLU#P to prove new robustness results for CL#P and to expand what is known about the closure properties of CL#P.  The complexity of recognizing edges---of an ordered collection of computation paths or of a cluster of accepting computation paths---is central to this study. Most particularly, our proofs exploit the power of unique discovery of edges---the ability of nondeterministic functions to, in certain settings, discover on exactly one (in some cases, on at most one) computation path a critical piece of information regarding edges of orderings or clusters.",Theory
44.txt,"We investigate the relative complexity of the graph isomorphism problem (GI) and problems related to the reconstruction of a graph from its vertex-deleted or edge-deleted subgraphs (in particular, deck checking (DC) and legitimate deck (LD) problems). We show that these problems are closely related for all amounts $c \geq 1$ of deletion:  1) $GI \equiv^{l}_{iso} VDC_{c}$, $GI \equiv^{l}_{iso} EDC_{c}$, $GI \leq^{l}_{m} LVD_c$, and $GI \equiv^{p}_{iso} LED_c$.  2) For all $k \geq 2$, $GI \equiv^{p}_{iso} k-VDC_c$ and $GI \equiv^{p}_{iso} k-EDC_c$.  3) For all $k \geq 2$, $GI \leq^{l}_{m} k-LVD_c$.  4) $GI \equiv^{p}_{iso} 2-LVC_c$.  5) For all $k \geq 2$, $GI \equiv^{p}_{iso} k-LED_c$.  For many of these results, even the $c = 1$ case was not previously known.  Similar to the definition of reconstruction numbers $vrn_{\exists}(G)$ [HP85] and $ern_{\exists}(G)$ (see page 120 of [LS03]), we introduce two new graph parameters, $vrn_{\forall}(G)$ and $ern_{\forall}(G)$, and give an example of a family $\{G_n\}_{n \geq 4}$ of graphs on $n$ vertices for which $vrn_{\exists}(G_n) &lt; vrn_{\forall}(G_n)$. For every $k \geq 2$ and $n \geq 1$, we show that there exists a collection of $k$ graphs on $(2^{k-1}+1)n+k$ vertices with $2^{n}$ 1-vertex-preimages, i.e., one has families of graph collections whose number of 1-vertex-preimages is huge relative to the size of the graphs involved.",Theory
33.txt,"Recently Gla{\ss}er et al. have shown that for many classes $C$ including PSPACE and NP it holds that all of its nontrivial many-one complete languages are autoreducible. This immediately raises the question of whether all many-one complete languages are Turing self-reducible for such classes $C$.  This paper considers a simpler version of this question---whether all PSPACE-complete (NP-complete) languages are length-decreasing self-reducible. We show that if all  PSPACE-complete languages are length-decreasing self-reducible then PSPACE = P and that if all NP-complete languages are length-decreasing self-reducible then NP = P.  The same type of result holds for many other natural complexity classes. In particular, we show that (1) not all NL-complete sets are logspace length-decreasing self-reducible, (2) unconditionally not all PSPACE-complete languages are logspace length-decreasing self-reducible, and (3) unconditionally not all EXP-complete languages are polynomial-time length-decreasing self-reducible.",Theory
3.txt,"We note that for each k \in {0,1,2, ...} the following holds: NE has (nonuniform) ACC^k circuits if and only if NE has P^{NE}-uniform ACC^k circuits. And we mention how to get analogous results for other circuit and complexity classes",Theory
30.txt,"In the year 1876 the mathematician Charles Dodgson, who wrote fiction under the now more famous name of Lewis Carroll, devised a beautiful voting system that has long fascinated political scientists. However, determining the winner of a Dodgson election is known to be complete for the \Theta_2^p level of the polynomial hierarchy. This implies that unless P=NP no polynomial-time solution to this problem exists, and unless the polynomial hierarchy collapses to NP the problem is not even in NP. Nonetheless, we prove that when the number of voters is much greater than the number of candidates---although the number of voters may still be polynomial in the number of candidates---a simple greedy algorithm very frequently finds the Dodgson winners in such a way that it ``knows'' that it has found them, and furthermore the algorithm never incorrectly declares a nonwinner to be a winner.",Theory
46.txt,"Computer scientists, programmers, and engineers need to determine the complexity of computational problems on a daily basis, and they typically ask the following questions: Is the problem easy or hard? If it is easy, is there a really efficient algorithm for the problem? If the problem is hard, how hard is it? Are there large subclasses of problems that are easy? Are there efficient approximation algorithms for the problem? Finding the answers to these questions pertaining to problem classification can be arduous and daunting for someone who is not an expert in the domain. Different problems, even from the same domain, may require vastly different proof techniques for problem classification. Thus, it is highly desirable to have easily applicable tools (theorems, classification tests, algorithms, and dichotomy results) that classify a wide range of problems. In this thesis we provide such general tools for determining the complexity of problems arising in the following settings: boolean circuits, language properties of central complexity classes (such as NP, PP, and ParityP), cycles in graphs, oracle (database) access, theoretical bmodels of computer simulation, and structural restrictions on the witness functions of nondeterministic polynomial-time Turing machines.",Theory
29.txt,"We eliminate some special cases from the proofs of two theorems in which a machine instantiating a many-query reduction to a p-selective set is made to use only one query. The first theorem, originally proved by Buhrman, Torenvliet, and van Emde Boas [BTvEB93], states that any set that positively reduces to a p-selective set has a many-one reduction to that same set. The second, originally proved by Buhrman and Torenvliet [BT96], states that self-reducible p-selective sets are in P.",Theory
43.txt,"Rabi and Sherman [RS97,RS93] proved that the hardness of factoring is a sufficient condition for there to exist one-way functions (i.e., p-time computable, honest, p-time noninvertible functions; this paper is in the worst-case model, not the average-case model) that are total, commutative, and associative but not strongly noninvertible. In this paper we improve the sufficient condition to ``P does not equal NP.''  More generally, in this paper we completely characterize which types of one-way functions stand or fall together with (plain) one-way functions---equivalently, stand or fall together with P not equaling NP. We look at the four attributes used in Rabi and Sherman's seminal work on algebraic properties of one-way functions (see [RS97,RS93]) and subsequent papers---strongness (of noninvertibility), totality, commutativity, and associativity---and for each attribute, we allow it to be required to hold, required to fail, or ``don't care.'' In this categorization there are 3^4 = 81 potential types of one-way functions. We prove that each of these 81 feature-laden types stand or fall together with the existence of (plain) one-way functions.",Theory
22.txt,"We provide an overview of some recent progress on the complexity of election systems. The issues studied include the complexity of the winner, manipulation, bribery, and control problems.",Theory
40.txt,"To study data placement on memory hierarchy, we present a model called {\em reference affinity}. Given a program trace, the model divides program data into hierarchical partitions (called affinity groups) based on a parameter $k$, which specifies the number of distinct data elements between accesses to members of each affinity group. Trivial solutions exist for the two ends of the hierarchy. At the top, when $k$ is no less than the data size, all program data belong to one affinity group. At the bottom, when $k$ is 0, each element is an affinity group.  We present two theoretical results. The first is the complexity. We show that finding and checking affinity groups are in P when $k=1$ and $k=2$. When $k=3$, the checking problem is NP-complete, and the finding problem is NP-hard. The second is the uses. We show that reference affinity captures the hierarchical data locality from the trace of a hierarchical computation. As additional evidence, we cite empirical results for general-purpose programs.",Theory
7.txt,"Much work has been devoted, during the past twenty years, to using complexity to protect elections from manipulation and control. Many results have been obtained showing NP-hardness shields, and recently there has been much focus on whether such worst-case hardness protections can be bypassed by frequently correct heuristics or by approximations. This paper takes a very different approach: We argue that when electorates follow the canonical political science model of societal preferences the complexity shield never existed in the first place. In particular, we show that for electorates having single-peaked preferences, many existing NP-hardness results on manipulation and control evaporate.",Theory
35.txt,"The study of semifeasible algorithms was initiated by Selman's work a quarter of century ago [Sel79,Sel81,Sel82]. Informally put, this research stream studies the power of those sets L for which there is a deterministic (or in some cases, the function may belong to one of various nondeterministic function classes) polynomial-time function f such that when at least one of x and y belongs to L, then f(x,y) \in L \cap \{x,y\}. The intuition here is that it is saying: ""Regarding membership in L, if you put a gun to my head and forced me to bet on one of x or y as belonging to L, my money would be on f(x,y).""  In this article, we present a number of open problems from the theory of semifeasible algorithms. For each we present its background and review what partial results, if any, are known.",Theory
24.txt,"Unger studied the balanced leaf languages defined via poly-logarithmically sparse leaf pattern sets. Unger shows that $\np$-complete sets are not polynomial-time many-one reducible to such balanced leaf language unless the polynomial hierarchy collapses to Theta^p_2 and that Sigam^p_2-complete sets are not polynomial-time bounded-truth-table reducible (respectively, polynomial-time Turing reducible) to any such balanced leaf language unless the polynomial hierarchy collapses to Delta^p_2 (respectively, Sigma^p_4).  This paper studies the complexity of the class of such balanced leaf languages, which will be denoted by VSLL. In particular, the following tight upper and lower bounds of VSLL are shown:  1. coNP is included in VSLL and VSLL is included in coNP/poly (the former inclusion is already shown by Unger).  2. coNP/1 is not included in VSLL unless PH collapses to Theta^p_2.  3. For no constant c&gt;0, VSLL is included coNP/n^c.  4. P/(loglog(n) + O(1)) is included in VSLL.  5. For no h(n) = loglog(n) + omega(1), P/h is included in VSLL.",Theory
17.txt,"Redundancy is a basic property of many computational settings. This thesis concerns techniques for eliminating redundancy in some cases, and exploiting it in others.  We study one-way functions, i.e., functions that are easy to compute but hard to invert. Such functions were previously studied as cryptographic primitives. Since it remains an open question whether one-way functions exist, we study the question of their existence in relation to a variety of complexity-theoretic hypotheses.  Starting with one-way functions in which redundancy in the preimage is absolutely minimal, i.e. one-to-one, we provide the first characterization of the existence of one-way permutations by a complexity class separation hypothesis, namely $\P \neq \up \inter \coup$.  Next, we study a type of one-way function that provably can never be one-to-one. Strong, total, associative, one-way functions are two-argument, one-way functions that are hard to invert, even if one of their arguments is known. Such special, one-way functions were originally used to construct secret-key agreement and digital signature protocols. We study techniques for creating such functions whose amount of preimage redundancy (as a function of the length of the corresponding image element) is minimized. We show that, if $\p \neq \up$, then such special one-way functions exist and that we can go from total, associative polyomial-time computable functions to strong, total, associative, one-way functions at no cost in increased preimage redundancy.  Continuing our study of eliminating redundancy in functions, we examine the complexity of counting the sizes of intervals over orders having certain, natural, computational and redundancy properties. We show that having redundancy in the adjacency relations of the order adds almost nothing to the computational complexity of computing such intervals.  Finally, we look at a problem in routing on ad-hoc networks whose solution exploits redundancy. We provide a theoretical framework for analyzing the behavior of a variety of tableless routing schemes. We show that such schemes work well when there is redundancy between the network distance and the objective functions used to make routing decisions.",Theory
10.txt,We prove that every distributional problem solvable in polynomial time on the average with respect to the uniform distribution has a frequently self-knowingly correct polynomial-time algorithm. We also study some features of probability weight of correctness with respect to generalizations of Procaccia and Rosenschein's junta distributions [PR07b].,Theory
45.txt,"Unambiguity in alternating Turing machines has received considerable attention in the context of analyzing globally-unique games by Aida et al. [ACRW04] and in the design of efficient protocols involving globally-unique games by Crasmaru et al. [CGRS04]. This paper explores the power of unambiguity in alternating Turing machines in the following settings:  (1) We show that unambiguity based hierarchies---AUPH, UPH, and \slant{UPH}---are infinite in some relativized world. For each $k$ &gt;= 2, we construct another relativized world where the unambiguity based hierarchies collapse so that they have exactly $k$ distinct levels and their $k$'th levels coincide with PSPACE. These results shed light on the relativized power of the unambiguity based hierarchies, and parallel the results known for the case of the polynomial hierarchy.  (2) We define the bounded-level unambiguous alternating solution class UAS(k), for every $k &gt;= 1, as the class of sets for which strings in the set are accepted unambiguously by some polynomial-time alternating Turing machine N with at most $k$ alternations, while strings not in the set either are rejected by $N$ or are accepted with ambiguity by N. We construct a relativized world where, for all $k &gt;= 1$, $UP_{\leq k}$ is a subset of $UP_{\leq k+1}$ and $UAS(k)$ is a subset of $UAS(k+1)$.  (3) Finally, we show that robustly $k$-level unambiguous alternating polynomial-time Turing machines accept languages that are computable in $P^{\Sigma^{p}_{k} \oplus A}$, for every oracle $A$. This generalizes a result of Hartmanis and Hemachandra [HH90].",Theory
8.txt,"This paper is concerned with the computational aspects of approval voting and some of its variants, with a particular focus on the complexity of problems that model various ways of tampering with the outcome of an election: manipulation, control, and bribery. For example, in control settings, the election's chair seeks to alter the outcome of an election via control actions such as adding/deleting/partitioning either candidates or voters. In particular, sincere-strategy preference-based approval voting (SP-AV), a variant of approval voting proposed by Brams and Sanver [BS06], is computationally resistant to 19 of the 22 common types of control. Thus, among those natural voting systems for which winner determination is easy, SP-AV is the system currently known to display the broadest resistance to control. We also present the known complexity results for various types of bribery. Finally, we study local search heuristics for minimax approval voting, a variant of approval voting proposed by Brams, Kilgour, and Sanver [BKS04] (see also [BKS07a,BKS07b]) for the purpose of electing a committee of fixed size.",Theory
41.txt,"Given a p-order A over a universe of strings (i.e., a transitive, reflexive, antisymmetric relation such that if (x, y) is an element of A then |x| is polynomially bounded by |y|), an interval size function of A returns, for each string x in the universe, the number of strings in the interval between strings b(x) and t(x) (with respect to A), where b(x) and t(x) are functions that are polynomial-time computable in the length of x.  By choosing sets of interval size functions based on feasibility requirements for their underlying p-orders, we obtain new characterizations of complexity classes. We prove that the set of all interval size functions whose underlying p-orders are polynomial-time decidable is exactly #P. We show that the interval size functions for orders with polynomial-time adjacency checks are closely related to the class FPSPACE(poly). Indeed, FPSPACE(poly) is exactly the class of all nonnegative functions that are an interval size function minus a polynomial-time computable function.  We study two important functions in relation to interval size functions. The function #DIV maps each natural number n to the number of nontrivial divisors of n. We show that #DIV is an interval size function of a polynomial-time decidable partial p-order with polynomial-time adjacency checks. The function #MONSAT maps each monotone boolean formula F to the number of satisfying assignments of F. We show that #MONSAT is an interval size function of a polynomial-time decidable total p-order with polynomial-time adjacency checks.  Finally, we explore the related notion of cluster computation.",Theory
1.txt,"This note is a commentary on, and critique of, Andre Luiz  Barbosa's paper entitled ""P != NP Proof."" Despite its  provocative title, what the paper is seeking to do is not to prove  P \neq NP in the standard sense in which that notation is used in the  literature. Rather, Barbosa is (and is aware that he is) arguing that a  different meaning should be associated with the notation P \neq NP,  and he claims to prove the truth of the statement P \neq NP  in his quite different sense of that statement. However,  we note that (1) the paper fails even on its own terms, as due to a  uniformity problem, the paper's proof does not establish, even in its  unusual sense of the notation, that P \neq NP; and (2) what the  paper means by the claim P \neq NP in fact implies that  P \neq NP holds even under the standard meaning that that notation has  in the literature (and so it is exceedingly unlikely that  Barbosa's proof can be fixed any time soon).",Theory
38.txt,"We prove that P-sel, the class of all P-selective sets, is EXP-immune, but is not EXP/1-immune. That is, we prove that some infinite P-selective set has no infinite EXP-time subset, but we also prove that every infinite P-selective set has some infinite subset in EXP/1. Informally put, the immunity of P-sel is so fragile that it is pierced by a single bit of information.  The above claims follow from broader results that we obtain about the immunity of the P-selective sets. In particular, we prove that for every recursive function f, P-sel is DTIME(f)-immune. Yet we also prove that P-sel is not \Pi_2^p/1-immune.",Theory
9.txt,"Voting and elections are at the core of democratic societies. People vote to elect leaders, decide policies, and organize their lives, but elections also have natural applications in computer science. For example, agents in multiagent systems often need to work together to complete some task, but each agent may have its own set of beliefs, preferences, and goals. Voting provides agents with a natural way to reach decisions that take all their preferences into account. With elections playing such an important role both in real-life political settings and in computer science, it is natural to ask about their resistance to misuse.  Two particular types of election misuse are manipulation and bribery. In manipulation, a group of voters chooses to misrepresent its preferences in order to obtain a more desirable outcome, and in bribery an outside agent, the briber, asks (possibly at a cost) a group of voters to change its votes, to obtain some outcome desirable for the briber. Classical results from political science show that, for any reasonable election system, there are scenarios where at least some voters have an incentive to attempt manipulation.  In this thesis we seek to protect elections from manipulators and bribers by making their computational task of finding good manipulations/bribes prohibitively expensive. When this is not possible, we seek to better understand (and even improve) the algorithmic attacks that manipulators and bribers can employ. In doing so, we develop new models of manipulation and bribery, and provide new approaches to studying the computational complexity of bribery and manipulation in elections.",Theory
15.txt,"Control and bribery are settings in which an external agent seeks to influence the outcome of an election. Faliszewski et al. [FHHR07] proved that Llull voting (which is here denoted by Copeland^1) and a variant (here denoted by Copeland^0) of Copeland voting are computationally resistant to many, yet not all, types of constructive control and that they also provide broad resistance to bribery. We study a parameterized version of Copeland voting, denoted by Copeland^alpha where the parameter alpha is a rational number between 0 and 1 that specifies how ties are valued in the pairwise comparisons of candidates in Copeland elections. We establish resistance or vulnerability results, in every previously studied control scenario, for Copeland^alpha, for each rational alpha, 0 &lt;alpha &lt; 1. In particular, we prove that Copeland^0.5, the system commonly referred to as ``Copeland voting,'' provides full resistance to constructive control. Among the systems with a polynomial-time winner problem, this is the first natural election system proven to have full resistance to constructive control. Results on bribery and fixed-parameter tractability of bounded-case control proven for Copeland^0 and Copeland^1 in [FHHR07] are extended to Copeland^alpha for each rational alpha, 0 &lt; alpha &lt; 1; we also give results in more flexible models such as microbribery and extended control.",Theory
18.txt,"We investigate issues related to two hard problems related to voting, the optimal weighted lobbying problem and the winner problem for Dodgson elections. Regarding the former, Christian et al. [CFRS06] showed that optimal lobbying is intractable in the sense of parameterized complexity. We provide an efficient greedy algorithm that achieves a logarithmic approximation ratio for this problem and even for a more general variant---optimal weighted lobbying. We prove that essentially no better approximation ratio than ours can be proven for this greedy algorithm.  The problem of determining Dodgson winners is known to be complete for parallel access to NP [HHR97]. Homan and Hemaspaandra [HH06] proposed an efficient greedy heuristic for finding Dodgson winners with a guaranteed frequency of success, and their heuristic is a ``frequently self-knowingly correct algorithm.'' We prove that every distributional problem solvable in polynomial time on the average with respect to the uniform distribution has a frequently self-knowingly correct polynomial-time algorithm. Furthermore, we study some features of probability weight of correctness with respect to Procaccia and Rosenschein's junta distributions [PR07].",Theory
12.txt,"We study the complexity of the following problem: Given two weighted voting games G' and G'' that each contain a player p, in which of these games is p's power index value higher? We study this problem with respect to both the Shapley-Shubik power index [SS54] and the Banzhaf power index [Ban65,DS79]. Our main result is that for both of these power indices the problem is complete for probabilistic polynomial time (i.e., is $\pp$-complete). We apply our results to partially resolve some recently proposed problems regarding the complexity of weighted voting games. We also study the complexity of the raw Shapley-Shubik power index. Deng and Papadimitriou [DP94] showed that the raw Shapley-Shubik power index is #P-metric-complete. We strengthen this by showing that the raw Shapley-Shubik power index is many-one complete for #P. And our strengthening cannot possibly be further improved to parsimonious completeness, since we observe that, in contrast with the raw Banzhaf power index, the raw Shapley-Shubik power index is not #P-parsimonious-complete.",Theory
42.txt,We obtain the first nontrivial worst-case upper bound on the number of iterations required by the well-known Hoffman-Karp algorithm for the simple stochastic game problem. We also describe a randomized variant of the Hoffman-Karp algorithm and analyze the expected number of iterations required by it in the worst case.,Theory
34.txt,"Preference aggregation in a multiagent setting is a central issue in both human and computer contexts. In this paper, we study in terms of complexity the vulnerability of preference aggregation to destructive control. That is, we study the ability of an election's chair to, through such mechanisms as voter/candidate addition/suppression/partition, ensure that a particular candidate (equivalently, alternative) does not win. And we study the extent to which election systems can make it impossible, or computationally costly (NP-complete), for the chair to execute such control. Among the systems we study---plurality, Condorcet, and approval voting---we find cases where systems immune or computationally resistant to a chair choosing the winner nonetheless are vulnerable to the chair blocking a victory. Beyond that, we see that among our studied systems no one system offers the best protection against destructive control. Rather, the choice of a preference aggregation system will depend closely on which types of control one wishes to be protected against. We also find concrete cases where the complexity of or susceptibility to control varies dramatically based on the choice among natural tie-handling rules.",Theory
16.txt,"We study the concept of bribery in the situation where voters are willing to change their votes as we ask them, but where their prices depend on the nature of the change we request. Our model is an extension of the one of Faliszewski et al. [FHH06], where each voter has a single price for any change we may ask for. We show polynomial-time algorithms for our version of bribery for a broad range of voting protocols, including plurality, veto, approval, and utility based voting. In addition to our polynomial-time algorithms we provide NP-completeness results for a couple of our nonuniform bribery problems for weighted voters, and a couple of approximation algorithms for NP-complete bribery problems defined in [FHH06] (in particular, an FPTAS for plurality-weighted-$bribery problem).",Theory
32.txt,"In POPL 2002, Petrank and Rawitz showed a universal result---finding optimal data placement is not only NP-hard but also impossible to approximate within a constant factor if P &lt;&gt; NP. Here we study a recently published concept called reference affinity, which characterizes a group of data that are always accessed together in computation. On the theoretical side, we give the complexity for finding reference affinity in program traces, using a novel reduction that converts the notion of distance into satisfiability. We also prove that reference affinity automatically captures the hierarchical locality in divide-and-conquer computations including matrix solvers and N-body simulation. The proof establishes formal links between computation patterns in time and locality relations in space.  On the practical side, we show that efficient heuristics exist. In particular, we present a sampling method and show that it is more effective than the previously published technique, especially for data that are often but not always accessed together. We show the effect on generated and real traces. These theoretical and empirical results demonstrate that effective data placement is still attainable in general-purpose programs because common (albeit not all) locality patterns can be precisely modeled and efficiently analyzed.",Theory
20.txt,"This paper studies the notions of autoreducibility and length-decreasing self-reducibility of functions and languages. Recently Glasser et al. have shown that for many classes C, including PSPACE and NP, it holds that all nontrivial complete languages are polynomial-time many-one autoreducible. In contrast, this paper shows that for many classes C such that P is a subset of C (e.g., PSPACE and NP) some complete languages in C are not polynomial-time length-decreasing self-reducible unless C is a subset of P and for classes C such that L is a subset of C and C is a subset of P (e.g., P and NL) some complete languages in C are not logarithmic-space length-decreasing self-reducible unless C is a subset of L.  This paper also shows that contrast between autoreducibility and length-decreasing self-reducibility for the case of functions. In particular, the paper shows that many function complexity classes FC (including well-studied #P, SpanP, and GapP and not-so-well-studied but highly natural #PE and TotP) have the property that all complete functions in FC are polynomial-time Turing-autoreducible. For #P and TotP, the autoreductions can be made to be polynomial-time one-Turing (one query per input).  These results show that, under reasonable assumptions, the notions of length-decreasing self-reducibility and autoreducibility differ both on complete languages and on complete functions. In a similar vein, this paper shows that under reasonable assumptions autoreducibility and random-self-reducibility differ with respect to functions.",Theory
37.txt,"We attain two main objectives in this thesis. First, we employ test languages to prove limitations of proof techniques to resolve certain questions in complexity theory. In this part of the thesis, we study the relationship between quantum classes and counting classes via closure properties, collapses, and relativized separations. We show that the best known classical bounds for quantum classes such as EQP and BQP cannot be significantly improved using relativizable proof techniques. In some cases, we strengthen known relativized separations between quantum and counting classes to their relativized immunity separations. Furthermore, using the closure properties of certain gap-definable counting classes, we prove strong consequences, in terms of the complexity of the polynomial hierarchy, of the following hypotheses: NQP is contained in BQP, and EQP equals NQP. Aside from using test languages to study the relationship between quantum and counting classes, we use test languages to construct, via degree bounds of polynomials, relativized worlds that exhibit separations of classes and nonexistence of complete sets.  Second, we study certain concrete problems and characterize their complexity either by showing completeness results for complexity classes or by relating their complexity to some well-studied computational problem (e.g., the graph isomorphism problem). In this part of the thesis, we study concrete problems related to the reconstruction of a graph from a collection of vertex-deleted or edge-deleted subgraphs, and concrete problems related to a notion of linear connectivity in directed hypergraphs. We show that the problems we study related to the reconstruction of graphs either are isomorphic (in complexity-theoretic sense) to the graph isomorphism problem or are many-one hard for the graph isomorphism problem. In our study related to directed hypergraphs, we introduce a notion of linear hyperconnectivity, denoted by L-hyperpath, in directed hypergraphs and show how this notion can be used to model problems in diverse domains. We study problems related to the cyclomatic number of directed hypergraphs with respect to L-hypercycles (the minimum number of hyperedges that need to be deleted so that the directed hypergraph becomes free of L-hypercycles) and obtain completeness results for different levels of the polynomial hierarchy.",Theory
39.txt,"Scoring protocols are a broad class of voting systems. Each is defined by a vector $(\alpha_1,\alpha_2,\ldots,\alpha_m)$, $\alpha_1 \geq \alpha_2 \geq \cdots \geq \alpha_m$, of integers such that each voter contributes $\alpha_1$ points to his/her first choice, $\alpha_2$ points to his/her second choice, and so on, and any candidate receiving the most points is a winner.  What is it about scoring-protocol election systems that makes some have the desirable property of being NP-complete to manipulate, while others can be manipulated in polynomial time? We find the complete, dichotomizing answer: Diversity of dislike. Every scoring-protocol election system having two or more point values assigned to candidates other than the favorite---i.e., having $||\{\alpha_i \condition 2 \leq i \leq m\}||\geq 2$---is NP-complete to manipulate. Every other scoring-protocol election system can be manipulated in polynomial time. In effect, we show that---other than trivial systems (where all candidates alway tie), plurality voting, and plurality voting's transparently disguised translations---\emph{every} scoring-protocol election system is NP-complete to manipulate.",Theory
5.txt,"We study the behavior of Range Voting and Normalized Range Voting with respect to electoral control. Electoral control encompasses attempts from an election chair to alter the structure of an election in order to change the outcome. We show that a voting system resists a case of control by proving that performing that case of control is computationally infeasible. Range Voting is a natural extension of approval voting, and Normalized Range Voting is a simple variant which alters each vote to maximize the potential impact of each voter. We show that Normalized Range Voting has among the largest number of control resistances among natural voting systems.",Theory
28.txt,"Using entropy of traffic distributions has been shown to aid a wide variety of network monitoring applications such as anomaly detection, clustering to reveal interesting patterns, and traffic classification. However, realizing this potential benefit in practice requires accurate algorithms that can operate on high-speed links, with low CPU and memory requirements. Estimating the entropy in a streaming model to enable such fine-grained traffic analysis has been a challenging problem. We give lower bounds for this problem, showing that neither approximation nor randomization alone will let us compute the entropy efficiently.  We present two algorithms for randomly approximating the entropy in a time and space efficient manner, applicable for use on very high speed (greater than OC-48) links. Our first algorithm for entropy estimation, inspired by the seminal work of Alon et al. for estimating frequency moments, has strong theoretical guarantees on the error and resource usage. Our second algorithm utilizes the observation that the efficiency can be substantially enhanced by separating the high-frequency items (or elephants), from the low-frequency items (or mice). Evaluations on real-world traffic traces from different deployment scenarios demonstrate the utility of our approaches.",Theory
27.txt,"We study the complexity of influencing elections through bribery: How computationally complex is it for an external actor to determine whether by a certain amount of bribing voters a specified candidate can be made the election's winner? We study this problem for election systems as varied as scoring protocols and Dodgson voting, and in a variety of settings regarding homogeneous-vs.-nonhomogeneous electorate bribability, bounded-size-vs.-arbitrary-sized candidate sets, weighted-vs.-unweighted voters, and succinct-vs.-nonsuccinct input specification. We obtain both polynomial-time bribery algorithms and proofs of the intractability of bribery, and indeed our results show that the complexity of bribery is extremely sensitive to the setting. For example, we find settings in which bribery is NP-complete but manipulation (by voters) is in P, and we find settings in which bribing weighted voters is NP-complete but bribing voters with individual bribe thresholds is in P. For the broad class of elections (including plurality, Borda, k-approval, and veto) known as scoring protocols, we prove a dichotomy result for bribery of weighted voters: We find a simple-to-evaluate condition that classifies every case as either NP-complete or in P.",Theory
4.txt,"In 1992, Bartholdi, Tovey, and Trick  opened the study of control attacks on elections---attempts to  improve the election outcome by such actions as adding/deleting  candidates or voters. That work has led to many results on how  algorithms can be used to find attacks on elections and how  complexity-theoretic hardness results can be used as shields against  attacks. However, all the work in this line has assumed that the  attacker employs just a single type of attack. In this paper, we  model and study the case in which the attacker launches a  multipronged (i.e., multimode) attack. We do so to more  realistically capture the richness of real-life settings. For  example, an attacker might simultaneously try to suppress some  voters, attract new voters into the election, and introduce a  spoiler candidate. Our model provides a unified framework for such  varied attacks, and by constructing polynomial-time multiprong  attack algorithms we prove that for various election systems even  such concerted, flexible attacks can be perfectly planned in  deterministic polynomial time.",Theory
