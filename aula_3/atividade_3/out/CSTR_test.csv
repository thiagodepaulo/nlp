,text,class
212,"In this paper we present a system for vision-based planning and execution of fingertip grasps using a four-fingered dextrous hand. Our system does not rely on prior models of the objects to be grasped; it obtains all the information it needs from vision and from tactile sensors located at the fingertips of the hand. The grasp planner is based on a genetic algorithm modified to allow the use of real numbers as the basic representation unit. The grasp executer is based on differential visual feedback, which allows the system to specify goals and monitor progress in image space without needing absolute calibration between the camera and the hand. We present experimental results showing the application of the system to grasping unknown objects with the Utah/MIT hand.",Robotics
72,"WordNet is a lexical database that, among other things, arranges English nouns into a hierarchy ranked by specificity, providing links between a more general word and words that are specializations of it. For example, the word ""mammal"" is linked (transitively via some intervening words) to ""dog"" and to ""cat."" This hierarchy bears some resemblance to the hierarchies of types (or properties, or predicates) often used in artificial intelligence systems. However, WordNet was not designed for such uses, and is organized in a way that makes it far from ideal for them. This report describes our attempts to arrive at a quantitative measure of the quality of the information that can be extracted from WordNet by interpreting it as a formal taxonomy, and to design automatic techniques for improving the quality by filtering out dubious assertions.",ArtificiallIntelligence
172,"In this paper we introduce a formalism for optimal sensor parameter selection for iterative state estimation in static systems. In contrast to common approaches, where a certain metric---for example, the mean squared error between true and estimated state---is optimized during state estimation, in this work the optimality is defined in terms of reduction in uncertainty in the state estimation process. The main assumption is that state estimation becomes more reliable if the uncertainty and ambiguity in the state estimation process can be reduced.  We consider a framework based on Shannon's information theory and select the camera parameters that maximize the mutual information, i.e., optimize the information that the captured image conveys about the true state of the system. The technique implicitly takes into account the a priori probabilities governing the computation of the mutual information. Thus a sequential decision process can be formed by treating the a priori probability at a certain time step in the decision process as the a posteriori probability of the previous time step.  We demonstrate the benefits of our approach using an object recognition scenario and an active pan/tilt/zoom camera. During the sequential decision process the camera looks to parts of the object that allow the most reliable distinction of similar looking objects. We performed experiments with discrete density representation as well as with continuous densities and Monte Carlo evaluation of the mutual information. The results show that the sequential decision process outperforms a random gaze control, both in the sense of recognition rate and number of views necessary to return a decision.",Robotics
136,"Predictive coding and temporal invariance are two major unsupervised learning principles which have been used to explain the behavior of parts of the brain (most notably the striate cortex). Although both have been around for a number of years, no formal relationship between them has been established. We prove that temporal invariance is a form of predictive coding. To do this, we begin with the goal of predictive coding, make a set of assumptions about the class of problem we are dealing with, and derive temporal invariance from the predictive coding goal and our added assumptions.",Robotics
135,"A selective vision system sequentially collects evidence to support a specified hypothesis about a scene, as long as the additional evidence is worth the effort of obtaining it. Efficiency comes from processing the scene only where necessary, to the level of detail necessary, and with only the necessary operators. Knowledge representation and sequential decision-making are central issues for selective vision, which takes advantage of prior knowledge of a domain's abstract and geometrical structure and models for the expected performance and cost of visual operators.  The TEA-1 selective vision system uses Bayes nets for representation and benefit-cost analysis for control of visual and non-visual actions. It is the high-level control for an active vision system, enabling purposive behavior, the use of qualitative vision modules and a pointable multiresolution sensor. TEA-1 demonstrates that Bayes nets and decision theoretic techniques provide a general, re-usable framework for constructing computer vision systems that are selective perception systems, and that Bayes nets provide a general framework for representing visual tasks. Control, or decision making, is the most important issue in a selective vision system. TEA-1's decisions about what to do next are based on general hand-crafted ``goodness functions'' constructed around core decision theoretic elements. Several goodness functions for different decisions are presented and evaluated.  The TEA-1 system solves a version of the T-world problem, an abstraction of a large set of domains and tasks. Some key factors that affect the success of selective perception are analyzed by examining how each factor affects the overall performance of TEA-1 when solving ensembles of randomly produced, simulated T-world domains and tasks. TEA-1's decision making algorithms are also evaluated in this manner. Experiments in the lab for one specific T-world domain, table settings, are also presented.",Robotics
154,"On 7 December 1994, four student-built autonomous robots demonstrated various strategic, tactical, and mechanical approaches to a delivery task. That event was preceded by approximately two years of history and two days of frenzied preparation. Our robotics efforts were based on materials from MIT's well-known 6.270 course. This report summarizes our experiences, from pedagogical goals and organizational matters through mechanical and electronic techniques. Our intended audience is future robot-builders, and organizers of robot-building courses. We assume familiarity with material in Jones and Flynn's Mobile Robotics text, and with the various materials available from MIT over the internet.",Robotics
66,"We propose a semantics for belief in which the derivation of new beliefs from old ones is modeled as a computational process. Using this model, we characterize conditions under which it is appropriate to reason about other agents by simulating their inference processes with one's own.",ArtificiallIntelligence
185,"An agent with selective perception focuses its sensors on those parts of the environment that are relevant to the task at hand. Selective perception is an efficient method of gathering information from the world, but it presents problems for a learning agent when different actions are required in situations for which the selective perception system cannot produce distinguishing outputs. If this happens the agent is said to have incomplete perception, and the agent may be able to use internal state determined by past perceptions and actions in order to choose the correct action.  I propose research on learning algorithms that use short-term memory to disambiguate the incomplete perception that arises with selective perception. I present the Utile Distinction Memory (UDM) algorithm that solves the incomplete perception problem using a partially observable Markov decision process to represent the agent's internal state space. A significant feature of the algorithm is that it will build an internal state space proportionate to the task at hand, not as large as would be required to represent all of the perceivable world. A second algorithm, part of work in progress, will keep the advantages of UDM while improving learning speed and the ability to recognize the significance of memories that span multiple time steps.  Learning to use memory is difficult and will require a strong bias to learn efficiently. I will investigate ``learning by watching'' as a method of providing bias. Two applications I propose to study are: driving a simulated car using vision from the driver's point of view; and setting a table with human cooperation or interference. Using the results of psychophysical experiments, I will compare my algorithm's perceptual actions with the perceptual actions made by human subjects.",Robotics
248,"The paper presents delta send-recv, an MPI extension for overlapping coarse-grained computation and communication. It provides an interface for marking the data computation and its communication. It automatically blocks computation and divides communication into increments. Delta sends and recvs are dynamically chained to effect sender-receiver pipelining, which is superior to pipelining only at the sender or the receiver side. The evaluation uses kernel tests to find the best increment size for different MPI implementations and types of machines and networks. It shows 2 to 3 times performance improvement for large-volume data reduce involving 16 or 32 processors. In addition, the new interface enables computation and communication pipelining in an interpreted programming language, Rmpi.",Systems
79,"Visual cognition depends critically on the moment-to-moment orientation of gaze. Gaze is changed by saccades, rapid eye movements that orient the fovea over targets of interest in a visual scene. Saccades are ballistic; a prespecified target location is computed prior to the movement and visual feedback is precluded. Once a target is fixated, gaze is typically held for about 300 milliseconds, although it can be held for both longer and shorter intervals. Despite these distinctive properties, there has been no specific computational model of the gaze targeting strategy employed by the human visual system during visual cognitive tasks. This paper proposes such a model that uses iconic scene representations derived from oriented spatiochromatic filters at multiple scales. Visual search for a target object proceeds in a coarse-to-fine fashion with the target's largest scale filter responses being compared first. Task-relevant target locations are represented as saliency maps which are used to program eye movements. Once fixated, targets are remembered by using spatial memory in the form of object-centered maps. The model was empirically tested by comparing its performance with actual eye movement data from human subjects in natural visual search tasks. Experimental results indicate excellent agreement between eye movements predicted by the model and those recorded from human subjects.",ArtificiallIntelligence
138,"A general-purpose object indexing technique is described that combines the virtues of principal component analysis with the favorable matching properties of high-dimensional spaces to achieve high precision recognition. An object is represented by a set of high-dimensional iconic feature vectors comprised of the responses of derivative of Gaussian filters at a range of orientations and scales. Since these filters can be shown to form the eigenvectors of arbitrary images containing both natural and man-made structures, they are well-suited for indexing in disparate domains. The indexing algorithm uses an active vision system in conjunction with a modified form of Kanerva's sparse distributed memory which facilitates interpolation between views and provides a convenient platform for learning the association between an object's appearance and its identity. The robustness of the indexing method was experimentally confirmed by subjecting the method to a range of viewing conditions and the accuracy was verified using a well-known model database containing a number of complex 3D objects under varying pose.",Robotics
197,"In this paper we consider the problem of computing the 3D shape of an unknown, arbitrarily-shaped scene from multiple color photographs taken at known but arbitrarily-distributed viewpoints. By studying the equivalence class of all 3D shapes that reproduce the input photographs, we prove the existence of a special member of this class, the maximal photo-consistent shape, that (1) can be computed from an arbitrary volume that contains the scene, and (2) subsumes all other members of this class. We then give a provably-correct algorithm, called Space Carving, for computing this shape and present experimental results from applying it to the reconstruction of geometrically-complex scenes from several photographs. The approach is specifically designed to (1) build 3D shapes that allow faithful reproduction of all input photographs, (2) resolve the complex interactions between occlusion, parallax, shading, and their effects on arbitrary collections of photographs of a scene, and (3) follow a ""least commitment"" approach to 3D shape recovery.",Robotics
145,"This thesis presents a bottom-up approach to understanding and extending robotic motor control by integrating human guidance. The focus is on dexterous manipulation using a Utah/MIT robot hand but the ideas apply to other robotic platforms as well.  {\em Teleassistance} is a novel method of human/robot interaction in which the human operator uses a gestural sign language to guide an otherwise autonomous robot through a given task. The operator wears a glove that measures finger joint angles to relay the sign language. Each sign serves to orient the robot within the task action sequence by indicating the next perceptual sub-goal and a relative spatial basis. Teleassistance merges robotic servo loops with human cognition to alleviate the limitations of either full robot autonomy or full human control alone.  The operator's gestures are {\em deictic}, from the Greek {\em deiktikos} meaning pointing or showing, because they circumscribe the possible interpretations of perceptual feedback to the current context and thereby allow the autonomous routines to perform with computational economy and without dependence on a detailed task model. Conversely, the use of symbolic gestures permits the operator to guide the robot strategically without many of the problems inherent to literal master/slave teleoperation, including non-anthropomorphic mappings, poor feedback, and reliance on a tight communication loop.  The development of teleassistance stems from an analysis of autonomous control, in light of recent advances in manipulator technology. This work also presents a {\em qualitative}, context-sensitive control strategy that exploits the many degrees of freedom and compliance of dexterous manipulators. The qualitative strategy governs the underlying autonomous routines in teleassistance.",Robotics
280,"This paper is concerned with the computational aspects of approval voting and some of its variants, with a particular focus on the complexity of problems that model various ways of tampering with the outcome of an election: manipulation, control, and bribery. For example, in control settings, the election's chair seeks to alter the outcome of an election via control actions such as adding/deleting/partitioning either candidates or voters. In particular, sincere-strategy preference-based approval voting (SP-AV), a variant of approval voting proposed by Brams and Sanver [BS06], is computationally resistant to 19 of the 22 common types of control. Thus, among those natural voting systems for which winner determination is easy, SP-AV is the system currently known to display the broadest resistance to control. We also present the known complexity results for various types of bribery. Finally, we study local search heuristics for minimax approval voting, a variant of approval voting proposed by Brams, Kilgour, and Sanver [BKS04] (see also [BKS07a,BKS07b]) for the purpose of electing a committee of fixed size.",Theory
5,"An investigation of the referring behavior of personal and demonstrative pronouns in two corpora: a collection of problem-solving dialogs from the TRAINS93 corpus and prepared news stories from the Boston University Radio Corpus. Unlike most studies of pronominal reference, which limit themselves to pronouns that co-specify the meaning of another noun phrase (called coreference annotation), this study has a wider scope and includes all pronouns in the discourse. As a result, a broader characterization is possible for the pronouns in question. This study shows that current models of pronoun resolution that assume each pronoun to have a nominal antecedent are of limited utility when applied to spontaneous language.",ArtificiallIntelligence
19,"Recognition of motion sequences is a crucial ability for biological and robot vision systems. We present an architecture for the higher-level processes involved in recognition of complex structured motion. The work is focused on modeling human recognition of Moving Light Displays. MLDs are image sequences that contain only motion information at a small number of locations. Despite the extreme paucity of information in these displays, humans can recognize MLDs generated from a variety of common human movements. This dissertation explores the high-level representations and computational processes required for the recognition task. The structures and algorithms are articulated in the language of structured connectionist models. The implemented network can discriminate three human gaits from data generated by several actors.  Recognition of any motion involves indexing into stored models of movement. We present a representation for such models, called scenarios, based on coordinated sequences of discrete motion events. A method for indexing into this representation is described. We develop a parallel model of spatial and conceptual attention that is essential for disambiguating the spatially and temporally diffuse MLD data. The major computational problems addressed are: (1) representation of time-varying visual models; (2) integration of visual stimuli over time; (3) gestalt formation in and between spatially-localized feature maps and central movement representations; (4) contextual feedback to lower levels; and (5) the use of attention to focus processing on particular spatial locations and particular high-level representations. Several novel connectionist mechanisms are developed and used in the implementation.  In particular, we present advances in connectionist representation of temporal sequences and in using high-level knowledge to control an attentional mechanism. We show that recognition of gait can be achieved directly from motion features, without complex shape information, and that the motion information need not be finely quantized. We show how the ""what"" and ""where"" processes in vision can be tightly coupled in a synergistic fashion. These results indicate the value of the structured connectionist paradigm in modeling perceptual processes: no previous computational model has accounted for MLD recognition and we do not know how it would be approached in any other paradigm.",ArtificiallIntelligence
14,"Natural language generation is a knowledge-intensive, goal-directed process involving many interacting choices. Some questions that a generation system must answer include: (1) What information needs to be included in the output to satisfy the speaker's or writer's communicative goals? (2) How should a discourse contribution be structured to ensure its coherence? (3) Which modalities should be used to maximize the information exchange? (4) How can output be tailored to specific users? In this paper, we examine some aspects of natural language generation that constrain the planning process, including theories of discourse structure, models of discourse context and of users, and multimodal generation.",ArtificiallIntelligence
159,"We propose and implement a novel method for visual space trajectory planning, and adaptive high degree-of-freedom (DOF) visual feedback control. The method requires no prior information either about the kinematics of the manipulator, or the placement or calibration of the cameras, and imposes no limitations on the number of degrees of freedom controlled or the number of kind of visual features utilized. The approach provides not only a means of low-level servoing but a means to integrate it with higher level visual space trajectory and task planning. We are thus able to specify and perform complex tasks composed of several primitive behaviors, using both visual servoing and open loop control, where the number of sensed and controlled signals varies during the task. We report experimental results demonstrating a factor of 5 improvement in the repeatability of manipulations using a PUMA arm when comparing visual closed-loop to traditional joint level servoing. We also present experiment statistics showing the advantages of adaptive over non-adaptive control systems, and of using redundant visual information when performing manipulation tasks. Finally, we demonstrate usefulness of the approach by using it to specify and execute complex tasks involving real-world robot manipulation of rigid and non-rigid objects in up to 12 degrees of freedom. The manipulation is performed in the context of a semi-autonomous robot manipulation system.",Robotics
247,"Software Transactional Memory (STM) systems, if they support condition synchronization, typically do so through a retry mechanism. Using retry, a transaction explicitly self aborts and deschedules itself when it discovers that a precondition for its operation does not hold. The underlying implementation may then track the set of locations read by the retrying transaction, and refrain from scheduling the transaction for re-execution until at least one location in the set has been modified by another transaction.  While retry is elegant and simple, the conventional implementation has several potential drawbacks that may limit both its efficiency and its generality. In this note, we present a retry mechanism based on Bloom filters that is entirely orthogonal to TM implementation. Our retry is compatible with hardware, software, and hybrid TM implementations, and has no impact on memory management or on the cache behavior of shared locations. It does, however, serialize writer transactions after their commit point when there are retrying transactions. We describe our mechanism and compare it to an optimized version of the conventional implementation.",Systems
289,"Preference aggregation in a multiagent setting is a central issue in both human and computer contexts. In this paper, we study in terms of complexity the vulnerability of preference aggregation to destructive control. That is, we study the ability of an election's chair to, through such mechanisms as voter/candidate addition/suppression/partition, ensure that a particular candidate (equivalently, alternative) does not win. And we study the extent to which election systems can make it impossible, or computationally costly (NP-complete), for the chair to execute such control. Among the systems we study---plurality, Condorcet, and approval voting---we find cases where systems immune or computationally resistant to a chair choosing the winner nonetheless are vulnerable to the chair blocking a victory. Beyond that, we see that among our studied systems no one system offers the best protection against destructive control. Rather, the choice of a preference aggregation system will depend closely on which types of control one wishes to be protected against. We also find concrete cases where the complexity of or susceptibility to control varies dramatically based on the choice among natural tie-handling rules.",Theory
100,"Planning invariants are formulae that are true in every reachable state of a planning world. We describe a novel approach to the problem of discovering such invariants in propositional form---by analyzing only a set of reachable states of the planning domain, and not its operators. Our system works by exploiting perceived patterns of propositional covariance across the set of states: It hypothesizes that strongly-defined patterns represent features of the planning world.  We demonstrate that, in practice, our system overwhelmingly produces correct invariants. Moreover, we compare it with a well-known system from the literature that uses complete operator descriptions, and show that it discovers a comparable number of invariants, and moreover, does so hundreds or thousands of times faster.  We also show how an existing operator-based invariant finder can be used to verify the correctness of the invariants we find, should operator information be available. We show that such hybrid systems can efficiently produce verifiably true invariants.",ArtificiallIntelligence
203,"This report documents our experience with different optical flow estimation methods and our attempt to use optical flow both qualitatively and quantitatively. Special attention is devoted to improving the Lucas-Kanade method to obtain dense flow. We use a simple clustering technique to find looming objects. This method has the potential of supporting obstacle avoidance using optical flow. Experiments using real images demonstrate that this simple clustering is effective for certain scenes. We also point out when this technique will fail. We try to use optical flow quantitatively to recover the structure of a piecewise planar environment. First, we use the widely-known 8-parameter planar flow equations to locate individual planes in the scene.  Second, in lieu of full flow, we try to use normal flow to compute both the ego-motion and the structure. Both trials fail ungracefully, mostly due to noisy flow data. We describe the mathematics ofboth methods and our experimental results.",Robotics
84,"We analyze the problem of computing the minimal labels for a network of temporal relations in the Point Algebra. van Beek proposes an algorithm for accomplishing this task which takes $O(max(n^3,n^2\cdot m))$ time (for $n$ points and $m$ $\neq$-relations). We show that the proof of the correctness of this algorithm given by van Beek and Cohen is faulty, and we provide a new proof showing that the algorithm is indeed correct.",ArtificiallIntelligence
58,"This report proposes a generalization of Dynamic Predicate Logic that allows a straightforward treatment of functional anaphora in texts such as ""Most men had a gun, but only a few used it,"" or ""If all of the graduates received a job offer, then all of them accepted their offer."" The approach dynamically assigns (partial) functions as values of variables that are existentially quantified within the scopes of quantifiers like ""all"" and ""most."" The proposed method is also applicable to bridging anaphora and functionally dependent entities in frames, scripts, and generic sentences.",ArtificiallIntelligence
42,In uncertain reasoning one often needs to combine conflicting pieces of evidence. We show how the need for evidence combination arises in Kyburg's Evidential Probability system and investigate various methods of dealing with it.,ArtificiallIntelligence
92,"This report presents a method by which a reinforcement learning agent can solve the incomplete perception problem using memory. The agent uses a Hidden Markov Model (HMM) to represent its internal state space and creates memory capacity by splitting states of the HMM. The key idea is a test to determine when and how a state should be split: the agent only splits a state when the split will help the agent predict utility. Thus the agent can build an internal state space proportionate to the task at hand, not as large as would be required to represent all of its perceivable world. I call the technique UDM, for Utile Distinction Memory.",ArtificiallIntelligence
139,"This paper is about orienting, that is, establishing and maintaining a spatial relation between a motorized pair of cameras (the eye-head system) and a static or a moving object tracked over time. Motivated by physiological evidence, the paper proposes a simple set of vision-based strategies aimed to perform head, eyes and body movements in a complex environment. Fixation is shown to be an essential feature in visual servoing, and it is used to decouple control on head rotational degrees of freedom, making possible a metric-less approach to the orientation problem. A running implementation of these strategies, using a binocular camera system mounted on a PUMA 700, demonstrates the effectiveness of the approach.",Robotics
120,"Lexicon coverage is often the limiting factor in natural language processing systems. Recent work has attempted to remedy this situation by extracting information from machine readable dictionaries. Unfortunately, no NLP lexicon system or dictionary could possibly list all the potential words of English. However, humans are often able to interpret novel word forms (that is, words they have not seen before) without difficulty. One way we do this, if the word is complex (e.g., ""undecidability""), is by using cues from the internal structure of the word. Relations in phonological form often correspond to relations in meaning. For example, if someone knows what the verb ""open"" means, a number of educated guesses can be made about the meaning of ""reopen"". Exceptions abound in lexical data and any system that attempts to use lexical generalizations must be able to handle exceptions in a principled fashion. In this report, I will describe the preliminary design of a system that uses relations in form to derive relations in meaning. For a new word, the system will produce meaning postulates that represent an educated guess about the meaning of the new word. These meaning postulates will be written in Episodic Logic, and the entire system will be a module of the TRAINS system.",ArtificiallIntelligence
142,"This tutorial is dedicated to our long-suffering 442 students, and to the excellent authors from whom I shamelessly cribbed this work. It is a pure cut-and-paste job from my favorite sources on this material. This is not my own work---think of me as an editor working without his authors' permissions. Readers should know that original authors are usually easier to understand than rehashed versions. If this presentation helps you, good. If not it at least helped me sort a few things out.  I assume knowledge of all necessary linear systems theory, differential equations, statistics, control theory, etc. We start with the ideas of filtering, smoothing, prediction, and state estimation. Wiener filtering and its associated intellectual framework follows, with a brief foray into ARMA filtering. The idea of recursive estimation is introduced to give some motivation for the slog ahead, and then we start with basic concepts in maximum likelihood, maximum a posteriori, and least-squares estimation. The strategy is to work toward the Kalman filtering equations by showing how they are simply related to general least-squares estimation. After Kalman filtering, some simpler versions of recursive filters are presented. There are appendices on the orthogonality principle, the matrix inversion lemma, singular value decomposition, partial C and LISP code, and a worked example.",Robotics
226,"The web has the potential to serve as an excellent source of example imagery for visual concepts. Image search engines based on text keywords can fetch thousands of images for a given query; however, their results tend to be visually noisy. We present a technique that allows a user to refine noisy search results and characterize a more precise visual object class. With a small amount of user intervention we are able to re-rank search engine results to obtain many more examples of the desired concept. Our approach is based on semi-supervised machine learning in a novel probabilistic graphical model composed of both generative and discriminative elements. Learning is achieved via a hybrid expectation maximization / expected gradient procedure initialized with a small example set defined by the user. We demonstrate our approach on images of musical instruments collected from Google image search. The rankings given by our model show significant improvement with respect to the user-refined query. The results are suitable for improving user experience in image search applications and for collecting large labeled datasets for computer vision research.",Robotics
48,"A linguistic form's compositional, timeless meaning can be surrounded or even contradicted by various social, aesthetic, or analogistic companion meanings. This paper addresses a series of problems in the structure of spoken language discourse, including turn-taking and grounding. It views these processes as composed of fine-grained actions, which resemble speech acts both in resulting from a computational mechanism of planning and in having a rich relationship to the specific linguistic features which serve to indicate their presence.  The resulting notion of Conversation Acts is more general than speech act theory, encompassing not only the traditional speech acts but turn-taking, grounding, and higher-level argumentation acts as well. Furthermore, the traditional speech acts in this scheme become fully joint actions, whose successful performance requires full listener participation.  This paper presents a detailed analysis of spoken language dialogue. It shows the role of each class of conversation acts in discourse structure, and discusses how members of each class can be recognized in conversation. Conversation acts, it will be seen, better account for the success of conversation than speech act theory alone.",ArtificiallIntelligence
206,"Augmented Reality (AR) systems insert graphics objects into the images of real scenes. Geometric and photometric consistency has to be achieved to make AR systems effective and bring photorealism to the augmented graphics. Particularly, global illumination effects between the graphics objects and scene objects need to be simulated. This thesis investigates ways to improve AR rendering by creating cast shadows between the real and graphics objects. This requires knowledge about the scene lighting and the scene structure. We first give novel methods of recovering the light sources from the input images. For indoor scenes, we take advantage of scene regularities such as parallel and orthogonal walls. For outdoor scenes and indoor scenes where the lights can be approximated by a directional light source, we show a method of finding the light source from cast shadows present in the real scene.  Besides the light source structure, we also need to know the 3D structure of the scene so that we can render the shadow cast on a real object by a graphics object. Using spheres as primitives, we develop an algorithm to approximate the shape of the scene objects from multiple silhouettes.  With all the above components, one can build an AR system that infers necessary information of the scene from shadows and inserts graphics object with convincing shadows. To justify our endeavor in terms of shadows being important in human spatial perception, we investigate shadow perceptions in the context of cue integration.",Robotics
47,"The usefulness of accurate sequence information is re-evaluated in this paper. A novel idea, called phonetic set hashing, of transforming phone sequences to words is then suggested. Phone sequences are mapped onto the corresponding phone sets, and the latter used as keys for indexing appropriate words. By using data-driven training strategies, the problem of word segmentation has been alleviated. The robustness of phone set hashing towards insertion, deletion, and substitution errors has also been studied. Experiments with subsets of the TIMIT database indicate that phone set hashing is a simple, fast scheme for word pre-selection.",ArtificiallIntelligence
74,Problems for strict and convex Bayesianism are discussed. A set-based Bayesianism generalizing convex Bayesianism and intervalism is proposed. This approach abandons not only the strict Bayesian requirement of a unique real-valued probability function in any decision-making context but also the requirement of convexity for a set-based representation of uncertainty. Levi's E-admissibility decision criterion is retained and is shown to be applicable in the non-convex case.,ArtificiallIntelligence
1,"Reduction is the operation of transforming a production in a Linear Context-Free Rewriting System (LCFRS) into two simpler productions by factoring out a subset of the nonterminals on the production's righthand side. Reduction lowers the rank of a production but may increase its fan-out. We show how to apply reduction in order to minimize the parsing complexity of the resulting grammar, and study the relationship between rank, fan-out, and parsing complexity. We show that it is always possible to obtain optimum parsing complexity with rank two. However, among transformed grammars of rank two, minimum parsing complexity is not always possible with minimum fan-out.",ArtificiallIntelligence
224,"One of the biggest challenges in systems neuroscience is a satisfactory model of neural signaling. From rate coding to temporal coding, models of neural signaling have been challenged by the fact that neurons fire highly irregularly. A typical interpretation of the variability is ``noise other than signal'', which not only has difficulty accounting for the speed, accuracy, efficiency and complexity of biological systems, but is also contradicted by recent studies that show both spike generation and transmission are highly reliable.  Challenged with the discrepancy between theory and data, we take a fresh view of the subject with the proposal that the randomness associated with neuronal outputs is certain to have a purpose. In particular, we model neurons as probabilistic devices that not only compute probabilities but also fire probabilistically to signal their computations. According to our model, signaling of probabilities is done by having cells with similar receptive fields fire synchronously to achieve fast communication, this is consistent with observations of neurons coding as ensembles and topographic map organization. Our proposal of probabilistic, distributed synchronous volleys as a neural signaling strategy not only accounts for variable neural responses, but also provides the advantage of robust and fast computation. Furthermore, the involvements of probabilistic firing and distributed coding explicate how synchronous firing can appear to be a rate code, accounting for the vast amount of data supporting a rate code assumption.  Any neural signaling model must support cortical computation in a biologically realistic fashion. Going beyond simply addressing the role of spikes in cortical cells' communication, we show that our distributed synchrony model can be implemented in a predictive coding framework and can be used to learn structures in the natural environment. Trained with patches from natural images, our model V1 cells develop localized and oriented receptive fields, consistent with V1 simple cell properties. Unlike most cortical computation models, our predictive coding model makes use of single spikes, instead of abstracting spikes away with analog quantities. This close resemblance to biology makes our model well suited for guiding experimental research.",Robotics
221,"Object recognition from a single view fails when the available features are not sufficient to determine the identity of a single object, either because of similarity with another object or because of feature corruption due to clutter and occlusion. Active object recognition systems have addressed this problem successfully, but they require complicated systems with adjustable viewpoints that are not always available. In this paper we investigate the performance gain available by combining the results of a single view object recognition system applied to imagery obtained from multiple fixed cameras. In particular, we address performance in cluttered scenes with varying degrees of information about relative camera pose. We argue that a property common to many recognition systems, which we term a weak target error, is responsible for two interesting limitations of multi-view performance enhancement: the lack of significant improvement in systems whose single-view performance is weak, and the plateauing of performance improvement as additional multi-view constraints are added.",Robotics
192,"Using a combination of techniques from visual representations, view synthesis, and visual-motor transfer function estimation, we present a method for animating movements of an active agent (e.g., robot), without the use of any prior models or explicit 3d information. The information needed to generate simulated images can be acquired either on or off line, by watching the agent doing an arbitrary, possibly unrelated task. We present experimental results synthesizing image sequences of the movement of a simulated PUMA 760 robot arm, using both joint space and Cartesian world coordinate control. We have created a user interface, where a user can input a robot movement program, and then upon execution, view movies of the (simulated) robot executing the program, along with the instantaneous dynamic variables from the simulated robot.",Robotics
209,"The recognition of nonrigid motion, particularly that arising from human movement (and by extension from the locomotory activity of animals) has typically made use of high-level parametric models representing the various body parts (legs, arms, trunk, head, etc.) and their connections to each other. Such model-based recognition has been successful in some cases; however, the methods are often difficult to apply to real-world scenes, and are severely limited in their generalizability. The first problem arises from the difficulty of acquiring and tracking the requisite model parts, usually specific joints such as knees, elbows or ankles. This generally requires some prior high-level understanding and segmentation of the scene, or initialization by a human operator. The second problem is due to the fact that the human model is not much good for dogs or birds; for each new type of motion, a new model must be hand-crafted. In this paper, we show that the recognition of human or animal locomotion, and, in fact, any repetitive activity, can be done using low-level, non-parametric representations. Such an approach has the advantage that the same underlying representation is used for all examples, and no individual tailoring of models or prior scene understanding is required. We show in particular that repetitive motion is such a strong cue that the moving actor can be segmented, normalized spatially and temporally, and recognized by matching against a spatio-temporal template of motion features. We have implemented a real-time system that can recognize and classify repetitive motion activities in normal gray-scale image sequences. Results on a number of real-world sequences are described.",Robotics
285,"Control and bribery are settings in which an external agent seeks to influence the outcome of an election. Faliszewski et al. [FHHR07] proved that Llull voting (which is here denoted by Copeland^1) and a variant (here denoted by Copeland^0) of Copeland voting are computationally resistant to many, yet not all, types of constructive control and that they also provide broad resistance to bribery. We study a parameterized version of Copeland voting, denoted by Copeland^alpha where the parameter alpha is a rational number between 0 and 1 that specifies how ties are valued in the pairwise comparisons of candidates in Copeland elections. We establish resistance or vulnerability results, in every previously studied control scenario, for Copeland^alpha, for each rational alpha, 0 &lt;alpha &lt; 1. In particular, we prove that Copeland^0.5, the system commonly referred to as ``Copeland voting,'' provides full resistance to constructive control. Among the systems with a polynomial-time winner problem, this is the first natural election system proven to have full resistance to constructive control. Results on bribery and fixed-parameter tractability of bounded-case control proven for Copeland^0 and Copeland^1 in [FHHR07] are extended to Copeland^alpha for each rational alpha, 0 &lt; alpha &lt; 1; we also give results in more flexible models such as microbribery and extended control.",Theory
225,"Augmented reality is the merging of synthetic sensory information into a user's perception of a real environment. Until recently, it has presented a passive interface to its human users, who were merely viewers of the scene augmented only with visual information. In contrast, practically since its inception, computer graphics--and its outgrowth into virtual reality--has presented an interactive environment. It is our thesis that the agumented reality interfce can be made interactive. We present: techniques that can free the user from restricttive requirements such as working in calibrated environments, resutls with haptic interface technology incorporated into augmented reality domains, and systems considerations that underlie the practical realization of these interactive augmented reality techinques.",Robotics
117,"In their framework for ontological analysis, Guarino and Welty provide a number of insights that are useful for guiding the design of taxonomic hierarchies. However, the formal statements of these insights as logical schemata are flawed in a number of ways, including inconsistent notation that makes the intended semantics of the logic unclear, false claims of logical consequence, and definitions that provably result in the triviality of some of their property features. This paper makes a negative contribution, by demonstrating these flaws in a rigorous way, but also makes a positive contribution wherever possible, by identifying the underlying intuitions that the faulty definitions were intended to capture, and attempting to formalize those intuitions in a more accurate way.",ArtificiallIntelligence
240,"Reuse distance is a basic metric for program locality. The distribution of reuse distances, called the reuse signature, shows the average locality or the amount of actively used data. Random access is often assumed in analytical models about program behavior. An interesting question is whether the reuse behavior of random data access has a closed-form answer. In this paper we prove that the length of reuse distances of random access is uniformly distributed from 0 to n-1 when n is the size of data. We also test random traces of different lengths to show the effect on the distribution.",Systems
228,"Fast track is a software speculation system that enables unsafe optimization of sequential code. It speculatively runs optimized code to improve performance and then checks the correctness of the speculative code by running the original program on multiple processors.  We present the interface design and system implementation for Fast Track. It lets a programmer or a proï¬ling tool mark fast-track code regions and uses a run-time system to manage the parallel execution of the speculative process and its checking processes and ensures the correct display of program outputs. The core of the run-time system is a novel concurrent algorithm that balances exploitable parallelism and available processors when the fast track is too slow or too fast. The programming interface closely affects the run-time support. Our system permits both explicit and implicit end markers for speculatively optimized code regions as well as extensions that allow the use of multiple tracks and user deï¬ned correctness checking. We discuss the possible uses of speculative optimization and demonstrate the effectiveness of our prototype system by examples of unsafe semantic optimization and a general system for fast memory-safety checking, which is able to reduce the checking time by factors between 2 and 7 for large sequential code on a 8-CPU system.",Systems
98,"The process of adding to the common ground between conversational participants (called grounding) has previously been either oversimplified or studied in an off-line manner. This dissertation presents a computational theory, in which a protocol is presented which can be used to determine, for any given state of the conversation, whether material has been grounded or what it would take to ground the material. This protocol is related to the mental states of participating agents, showing the motivations for performing particular grounding acts and what their effects will be.  We extend speech act theory to account for levels of action both above and below the sentence level, including the level of grounding acts described above. Traditional illocutionary acts are now seen to be multi-agent acts which must be grounded to have their usual effects.  A conversational agent model is provided, showing how grounding fits in naturally with the other functions that an agent must perform in engaging in conversation. These ideas are implemented within the TRAINS conversation system.  Also presented is a situation-theoretic model of plan execution relations, giving definitions of what it means for an action to begin, continue, complete, or repair the execution of a plan. This framework is then used to provide precise definitions of the grounding acts in terms of agents executing a general communication plan in which one agent must present the content and another acknowledge it.",ArtificiallIntelligence
171,"When a reinforcement learning agent's next course of action depends on information that is hidden from the sensors because of problems such as occlusion, restricted range, bounded field of view and limited attention, we say the agent suffers from the Hidden State Problem. State identification techniques use history information to uncover hidden state. Previous approaches to encoding history include: finite state machines [Chrisman 1992; McCallum 1992], recurrent neural networks [Lin and Mitchell 1992], and genetic programming with indexed memory [Teller 1994]. A chief disadvantage of all these techniques is their long training time.  This report presents Instance-Based State Identification, a new approach to reinforcement learning with state identification that learns with much fewer training steps. Noting that learning with history and learning in continuous spaces both share the property that they begin without knowing the granularity of the state space, the approach applies instance-based (or ``memory-based'') learning to history sequences---instead of recording instances in a continuous geometrical space, we record instances in action-perception-reward sequence space. The first implementation of this approach, called Nearest Sequence Memory, learns with an order of magnitude fewer steps than several previous approaches.",Robotics
114,"To describe phenomena that occur at different time scales, computational models of the brain necessarily must incorporate different levels of abstraction. We argue that at time scales of approximately one-third of a second, orienting movements of the body play a crucial role in cognition and form a useful computational level, termed the embodiment level. At this level, the constraints of the body determine the nature of cognitive operations, since the natural sequentiality of body movements can be matched to the natural computational economies of sequential decision systems. The way this is done is through a system of implicit reference termed deictic, whereby pointing movements are used to bind objects in the world to cognitive programs. We show how deictic bindings enable the solution of natural tasks and argue that one of the central features of cognition, working memory, can be related to moment-by-moment dispositions of body features such as eye movements and hand movements.",ArtificiallIntelligence
253,"We study the complexity of manipulation for a family of election systems derived from Copeland voting via introducing a parameter alpha that describes how ties in head-to-head contests are valued. We show that the problem of manipulation for unweighted Copeland^alpha elections is NP-complete even if the size of the manipulating coalition is limited to two. Our result holds for all rational values of alpha such that 0 &lt; alpha &lt; 1 except for alpha = 1/2. We contrast our result with the fact that microbribery for Copeland^alpha is currently known to be in P exactly for alpha in {0,1/2,1} (complexity results for other values of alpha are unknown). Microbribery is a problem very closely related to manipulation. Since it is well known that manipulation via a single voter is easy for Copeland, ourresult is the first one where an election system originally known to be vulnerable to manipulation via a single voter is shown to be resistant to manipulation via a coalition of a constant number of voters. We also study the complexity of manipulation for Copeland^alpha for the case of a constant number of candidates. We show that here the exact complexity of manipulation often depends closely on the winner model as well as on the parameter alpha: Depending whether we try to make our favorite candidate a winner or a unique winner and whether alpha is 0, 1 or between these values, the problem of weighted manipulation for Copeland^alpha with three candidates is either in P or is NP-complete. Our results show that ways in which ties are treated in an election system, here Copeland voting, can be crucial to establishing complexity results for this system.",Theory
118,"This paper describes how well prosodic information correlates with the topic structure of a cooperative dialogue. To investigate this correlation systematically, first we introduce the notion of utterance unit (UU) as a basic unit in conversations. We define the utterance unit by employing four principles. The grammatical principle is a syntactic criterion in which the UU boundary is set wherever the period can be placed. The pragmatic principle says that each UU corresponds to a basic speech act. In other words, if two neighboring phrases correspond to different speech acts (for instance, acknowledgment and request), they should be taken as two different UUs. The conversational principle addresses the turn-taking aspect of conversations. A UU boundary should be placed wherever the speaker changes. Finally, the prosodic principle says that whenever a medium length or longer pause (750 msec) is inserted between two phrases, they are to be taken as two different UUs. We apply these principles to a speech database containing about one and a half hours of collected dialogue to split the dialogues into a sequence of UUs. We then classify the inter-UU boundaries based on the relationship between two neighboring UUs into four semantic categories: topic shift, topic continuation, elaboration (or clarification), and speech-act continuation. The prosodic parameters measured at each boundary are the onset fundamental frequency (F0), the final F0, and the F0 maximal peak declination ratio (the ratio of the current UUUs maximal peak to that of the preceding UU). Our study shows how these prosodic parameters vary depending on the topic structure. Our results can be summarized as follows. (1) The onset F0 value tends to be higher when the topic is changed at the UU boundary. (2) The final F0 value indicates finality and is much higher (on average) at speech-act continuation boundaries than at other boundaries. (3) The maximal peak declination ratio reflects the degree of subordination to the preceding UU. That is, this ratio is lowest at elaboration boundaries and highest at topic shift boundaries. Finally, we discuss discourse structure identification via the prosodic parameters.",ArtificiallIntelligence
255,"A king in a directed graph is a node from which each node in the graph can be reached via paths of length at most two. There is a broad literature on tournaments (completely oriented digraphs), and it has been known for more than half a century that all tournaments have at least one king [Lan53]. Recently, kings have proven useful in theoretical computer science, in particular in the study of the complexity of the semifeasible sets [HNP98,HT05] and in the study of the complexity of reachability problems [Tan01,NT02].  In this paper, we study the complexity of recognizing kings. For each succinctly specified family of tournaments, the king problem is known to belong to $\Pi_2^p$ [HOZZ]. We prove that this bound is optimal: We construct a succinctly specified tournament family whose king problem is $\Pi_2^p$-complete. It follows easily from our proof approach that the problem of testing kingship in succinctly specified graphs (which need not be tournaments) is $\Pi_2^p$-complete. We also obtain $\Pi_2^p$-completeness results for k-kings in succinctly specified j-partite tournaments, $k,j \geq 2$, and we generalize our main construction to show that $\Pi_2^p$-completeness holds for testing k-kingship in succinctly specified families of tournaments for all $k \geq 2$.",Theory
45,"Supervised, neural network, learning algorithms have proven very successful at solving a variety of learning problems. However, they suffer from a common problem of requiring explicit output labels. This requirement makes such algorithms implausible as biological models. In this paper, it is shown that pattern classification can be achieved in a multi-layered, feed-forward neural network, without requiring explicit output labels, by a process of supervised self-organization. The class projection is achieved by optimizing appropriate within-class uniformity and between-class discernibility criteria. The mapping function and the class labels are developed together iteratively using the derived self-organizing back-propagation algorithm. The ability of the self-organizing network to generalize on unseen data is also experimentally evaluated on real data sets, and compares favorably with the traditional labeled supervision with neural networks. However, interesting features emerge out of the proposed self-organizing supervision that are absent in conventional approaches. The further implications of self-organizing supervision with neural networks are also discussed.",ArtificiallIntelligence
108,"Training a statistical machine translation system starts with tokenizing a parallel corpus. Some languages such as Chinese do not incorporate spacing in their writing system, which creates a challenge for tokenization. Morphologically rich languages such as Korean and Hungarian present an even bigger challenge, since optimal token boundaries for machine translation in these languages are often unclear. Both rule-based solutions and statistical solutions are currently used. In this paper, we present unsupervised methods to solve tokenization problem. Our methods incorporate information available from parallel corpus to determine a good tokenization for machine translation.",ArtificiallIntelligence
21,"In ordinary first-order logic, a valid inference in a language {\bf L} is one in which the conclusion is true in every model of the language in which the premises are true.  To accommodate inductive/uncertain/probabilistic/non-monotonic inference, we weaken that demand to the demand that the conclusion be true in a large proportion of the models in which the relevant premises are true. More generally, we say that an inference is [p,q] valid if its conclusion is true in a proportion lying between p and q of those models in which the relevant premises are true. If we include a statistical variable binding operator ``%'' in our language, there are many quite general (and useful) things we can say about uncertain validity. A surprising result is that some of these things may conflict with Bayesian conditionalization.",ArtificiallIntelligence
26,"We describe some simple domain-independent improvements to plan-refinement strategies for well-founded partial order planning that promise to bring this style of planning closer to practicality. One suggestion concerns the strategy for selecting plans for refinement among the current (incomplete) candidate plans. We propose an A* heuristic that counts only steps and open conditions, while ignoring ""unsafe conditions"" (threats). A second suggestion concerns the strategy for selecting open conditions (goals) to be established next in a selected incomplete plan. Here we propose a variant of a strategy suggested by Peot and Smith and studied by Joslin and Pollack; the variant gives top priority to unmatchable open conditions (enabling the elimination of the plan), second-highest priority to goals that can only be achieved uniquely, and otherwise uses LIFO prioritization. The preference for uniquely achievable goals is a ""zero-commitment"" strategy in the sense that the corresponding plan refinements are a matter of deductive certainty, involving no guesswork. In experiments based on modifications of UCPOP, we have obtained improvements by factors ranging from 5 to several hundred for a variety of problems that are nontrivial for the unmodified version. Crucially, the hardest problems give the greatest improvements.",ArtificiallIntelligence
61,"This study of the Fall 2002 Computer Programming (CSC 171) course provides a detailed analysis of the relationship between variables such as workshop attendance, gender, ethnicity and prior student ability and student performance. The results, detailed in the subsequent sections below, suggest the following:  * Workshop attendance has a significantly positive impact on student performance even after controlling for variations in gender and prior student ability.  * Due to the small sample size of the female and minority groups, the magnitude of the role gender and ethnicity plays in affecting student performance cannot be conclusively determined based on statistical analyses.  * Withdrawing female students performed significantly below their male counterparts even though they attended more workshops on average, while female students who completed the course did not perform significantly differently from their male counterparts.  * Prior student ability (as measured by SAT scores) is significant in affecting student performance.  * Controlling for prior student ability alters the effect of workshop attendance on performance only slightly. OLS regression results suggest an overestimation, logistic regression results suggest an underestimation of the effect prior to adding SAT scores.",ArtificiallIntelligence
40,"We describe the goals, architecture, and functioning of the TRAINS-93 system, with emphasis on the representational issues involved in putting together a complex language processing and reasoning agent. The system is intended as an experimental prototype of an intelligent, conversationally proficient planning advisor in a dynamic domain of cargo trains and factories. For this team effort, our strategy at the outset was to let the designers of the various language processing, discourse processing, plan reasoning, execution and monitoring modules choose whatever representations seemed best suited for their tasks, but with the constraint that all should strive for principled, general approaches.  Disparities between modules were bridged by careful design of the interfaces, based on regular in-depth discussion of issues encountered by the participants. Because of the goal of generality and principled representation, the multiple representations ended up with a good deal in common (for instance, the use of explicit event variables and the ability to refer to complex abstract objects such as plans); and future unifications seem quite possible. We explain some of the goals and particulars of the KRs used, evaluate the extent to which they served their purposes, and point out some of the tensions between representations that needed to be resolved. On the whole, we found that using very expressive representations minimized the tensions, since it is easier to extract what one needs from an elaborate representation retaining all semantic nuances, than to make up for lost information.",ArtificiallIntelligence
105,"A Personalized System of Instruction (PSI) is a student-paced method of teaching in which students progress by displaying mastery of written material. Cooperative Learning is a method of instruction in which students work in groups to help each other study. In the Fall of 1996, a computer literacy course in which half of the students followed a PSI curriculum and the other half followed a Cooperative Learning curriculum was offered. Data from this experiment showed several statistically significant differences between the two curricula in student satisfaction as measured by end-of-the-semester course evaluation forms. These questionnaires indicated that students felt that the PSI classes increased their knowledge at the 99\% confidence level. They also indicated that students felt that the PSI course procedures better supported course objectives, that the PSI course required more work, and that it was easier to get answers from the TAs in the PSI classes at the 95\% confidence level. The data also showed statistically significant evidence that students learned more from the PSI curriculum as measured by exams. Analysis of rosters from the programming class offered the following semester showed no statistically significant difference between the proportion of the PSI students who took the programming class and the proportion of the cooperative learning students who took the programming class.",ArtificiallIntelligence
268,"In the year 1876 the mathematician Charles Dodgson, who wrote fiction under the now more famous name of Lewis Carroll, devised a beautiful voting system that has long fascinated political scientists. However, determining the winner of a Dodgson election is known to be complete for the \Theta_2^p level of the polynomial hierarchy. This implies that unless P=NP no polynomial-time solution to this problem exists, and unless the polynomial hierarchy collapses to NP the problem is not even in NP. Nonetheless, we prove that when the number of voters is much greater than the number of candidates---although the number of voters may still be polynomial in the number of candidates---a simple greedy algorithm very frequently finds the Dodgson winners in such a way that it ``knows'' that it has found them, and furthermore the algorithm never incorrectly declares a nonwinner to be a winner.",Theory
25,"The problem of ambiguity is central to any theory of language interpretation, whether our interest is in language processing in humans or in developing a usable natural language processing system. Psycholinguistic evidence suggests that human subjects are able to choose an interpretation when necessary, and that competing factors are involved in this choice; however, no theory of language interpretation deals satisfactorily with the combinatorial explosion paradox---the fact that no matter how ambiguous natural language sentences are, they are usually interpreted without significant effort.  The main idea presented in this dissertation is that the scope preferences observed in the literature are not obtained by an independent `scope disambiguation' module, but are the result of independent interpretation processes such as definite description interpretation or the interpretation of modals. None of these interpretive procedures is especially concerned with `scope disambiguation,' but the result of these inferences is that relations of contextual dependency such as anaphoric reference or presuppositionality become part of the common ground; the scope preferences observed in the literature reflect these relations of dependency. The dissertation includes a formal proposal concerning the representation of contextual dependency and its impact on the semantics of sentence constituents.  The theory of ambiguity here presented is based on a distinction between semantic ambiguity, that can be captured implicitly, by means of underspecified representations, and perceived ambiguity, that results from the process of discourse interpretation. A new model of the common ground is introduced, that can be used to characterize both situations characterized by the presence of semantic ambiguity, and situations characterized by the existence of perceived ambiguity.  The reasoning that leads to the establishment of scoping preferences makes use, I argue, of information that is pragmatic in nature; this calls for a model of discourse interpretation in which the common ground contains such information. In the case of spoken language conversations, the common ground must be a model of the discourse situation of the conversational participants.",ArtificiallIntelligence
153,"We present an approach for building an affine representation of an unknown curved object viewed under orthographic projection from images of its occluding contour. It is based on the observation that the projection of a point on a curved, featureless surface can be computed along a special viewing direction that {\em does not} belong to the point's tangent plane. We show that by circumnavigating the object on the tangent plane of selected surface points, we can (1) compute two orthogonal projections of every point projecting to the occluding contour during this motion, and (2) compute the affine coordinates of these points. Our approach demonstrates that affine shape of curved objects can be computed {\em directly}, i.e., without Euclidean calibration or image velocity and acceleration measurements.",Robotics
